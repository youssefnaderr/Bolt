{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "afb9e2e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81b2cc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3 import SAC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b568db07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "class RoboticDogEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(RoboticDogEnv, self).__init__()\n",
    "        self.total_reward=0\n",
    "        self.counter=0\n",
    "        self.steps=0\n",
    "        self.total_steps=0\n",
    "        # Connect to PyBullet\n",
    "        self.physicsClient = p.connect(p.GUI)\n",
    "\n",
    "        # Set the simulation environment\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "\n",
    "        # Load the plane and URDF\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "        # Define the URDF path as an instance attribute\n",
    "        self.urdf_path = \"C:/Users/youss/OneDrive - aucegypt.edu/Desktop/bolt/spot.urdf\"\n",
    "        self.robot_id = p.loadURDF(self.urdf_path, [0, 0, 0.15], useFixedBase=False)\n",
    "        self.num_joints = p.getNumJoints(self.robot_id)\n",
    "\n",
    "        # Ensure the robot is correctly initialized\n",
    "        if self.robot_id < 0:\n",
    "            raise ValueError(\"Failed to initialize the robot.\")\n",
    "        \n",
    "        if self.num_joints == 0:\n",
    "            raise ValueError(\"No joints found in the robot.\")\n",
    "        \n",
    "        print(f\"Robot ID: {self.robot_id}\")\n",
    "        print(f\"Number of joints: {self.num_joints}\")\n",
    "        \n",
    "        # Set gravity\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        \n",
    "        # Get joint limits and ensure joints are valid\n",
    "        self.joint_indices = []\n",
    "        \n",
    "        joint_limits = []\n",
    "        for joint_index in range(self.num_joints):\n",
    "            joint_info = p.getJointInfo(self.robot_id, joint_index)\n",
    "            joint_name = joint_info[1].decode('utf-8')\n",
    "            joint_lower_limit = joint_info[8]\n",
    "            joint_upper_limit = joint_info[9]\n",
    "            \n",
    "            # Print joint information for debugging\n",
    "           # print(f\"Joint {joint_index}: Name: {joint_name}, Lower limit: {joint_lower_limit}, Upper limit: {joint_upper_limit}\")\n",
    "            \n",
    "            if \"coxa\" in joint_name or \"femur\" in joint_name or \"tibia\" in joint_name:\n",
    "                # Add fallback default limits if the provided limits are invalid\n",
    "                if joint_lower_limit >= joint_upper_limit:\n",
    "                    joint_lower_limit, joint_upper_limit = -1.0, 1.0\n",
    "                \n",
    "                joint_limits.append((joint_lower_limit, joint_upper_limit))\n",
    "                self.joint_indices.append(joint_index)\n",
    "\n",
    "                \n",
    "        \n",
    "                \n",
    "        # Define the action and observation space\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([limit[0] for limit in joint_limits]),\n",
    "            high=np.array([limit[1] for limit in joint_limits]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(self.joint_indices) * 2 + 6,),  # Joint positions + velocities + base orientation (3) + base angular velocity (3)\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.fall_start_time = None  # Initialize the fall start time\n",
    "        self.leg_indices = self.joint_indices   \n",
    "        self.noise_counter = 0 \n",
    "        \n",
    "        self.forward=0\n",
    "        self.stable=0\n",
    "        self.energy=0\n",
    "        self.velocity_pen=0\n",
    "        self.fall=0\n",
    "        self.smooth=0\n",
    "        self.previous_joint_velocities = None\n",
    "        self.sym=0\n",
    "        \n",
    "       \n",
    "        \n",
    "        # Store the link indices for the feet\n",
    "        self.front_left_foot = self.get_link_index('foot_FL')\n",
    "        self.front_right_foot = self.get_link_index('foot_FR')\n",
    "        self.back_left_foot = self.get_link_index('foot_BL')\n",
    "        self.back_right_foot = self.get_link_index('foot_BR')\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "    def get_link_index(self, link_name):\n",
    "        for joint_index in range(self.num_joints):\n",
    "            joint_info = p.getJointInfo(self.robot_id, joint_index)\n",
    "            joint_name = joint_info[12].decode('utf-8')  # Link name is at index 12\n",
    "            if link_name in joint_name:\n",
    "                return joint_info[0]  # Return the link index\n",
    "        raise ValueError(f\"Link {link_name} not found\")\n",
    "        \n",
    "        \n",
    "    def reset(self, **kwargs):\n",
    "        self.counter=self.counter+1\n",
    "        self.total_steps=self.steps+self.total_steps\n",
    "        if self.steps!=0: \n",
    "            average = self.total_reward/self.steps\n",
    "        if self.total_reward != 0:\n",
    "            print(f\"Episode {self.counter}  finished with cumulative reward: {self.total_reward} and \")\n",
    "            print(f\"with an average reward of: {average}\")\n",
    "            print(f\"number of steps in this episode: {self.steps}\")\n",
    "            print(f\"total steps till now: {self.total_steps}\")\n",
    "            print(f\"..........................................\")\n",
    "        self.total_reward=0\n",
    "        self.steps=0\n",
    "        # Reset the simulation environment\n",
    "        p.resetSimulation()\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")\n",
    "        self.robot_id = p.loadURDF(self.urdf_path, [0, 0, 0.15], useFixedBase=False)\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        \n",
    "        # Wait for the simulation to stabilize\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Ensure the robot is correctly initialized\n",
    "        if self.robot_id < 0:\n",
    "            print(\"Error: Robot failed to load.\")\n",
    "            return None, {}\n",
    "        \n",
    "        # Check if joints are correctly initialized\n",
    "        if p.getNumJoints(self.robot_id) == 0:\n",
    "            print(\"Error: No joints found in the robot.\")\n",
    "            return None, {}\n",
    "        \n",
    "        # Return the initial state\n",
    "        initial_state = self.get_state()\n",
    "       # print(\"Initial state:\", initial_state)  # Debugging statement\n",
    "        self.fall_start_time = None  # Reset the fall start time\n",
    "        \n",
    "        \n",
    "        print(f\"forward : {self.forward/2500} \\n velocity penalty:{self.velocity_pen/2500} \\n stability : {self.stable/2500}\")\n",
    "        print(f\"energy : {self.energy/2500} \\n  fall: {self.fall/2500} \\n smooth: {self.smooth/2500} \\n symmetry: {self.sym/2500}\")      \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "                 \n",
    "        self.forward=0\n",
    "        self.stable=0\n",
    "        self.energy=0 \n",
    "        self.velocity_pen=0\n",
    "        self.fall=0\n",
    "        self.smooth=0\n",
    "        self.sym=0\n",
    "       \n",
    "        \n",
    "        return initial_state, {}\n",
    "    \n",
    "    def step(self, action):\n",
    "        # Apply action to the robot\n",
    "        for i, joint_index in enumerate(self.joint_indices):\n",
    "            p.setJointMotorControl2(self.robot_id, joint_index, p.POSITION_CONTROL, targetPosition=action[i])\n",
    "        \n",
    "        # Step the simulation\n",
    "        p.stepSimulation()\n",
    "        time.sleep(1. / 240.)  # Adjust sleep time to control simulation speed\n",
    "        \n",
    "        # Get the next state, reward, and check if done\n",
    "        state = self.get_state()\n",
    "        reward = self.compute_reward()\n",
    "        terminated = self.is_done(state)\n",
    "        truncated = False  # This can be set to True based on some logic, e.g., max steps reached\n",
    "        self.total_reward=self.total_reward+reward\n",
    "        self.steps=self.steps+1\n",
    "        return state, reward, terminated, truncated, {}\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Debugging: Check joint indices\n",
    "       # print(\"Joint indices:\", self.joint_indices)\n",
    "        \n",
    "        joint_states = p.getJointStates(self.robot_id, self.joint_indices)\n",
    "        if joint_states is None or len(joint_states) == 0:\n",
    "           # print(\"Error: p.getJointStates returned None or an empty list\")  # Debugging statement\n",
    "            return None\n",
    "        \n",
    "        joint_positions = [state[0] for state in joint_states]\n",
    "        joint_velocities = [state[1] for state in joint_states]\n",
    "        \n",
    "        base_position, base_orientation = p.getBasePositionAndOrientation(self.robot_id)\n",
    "        base_velocity, base_angular_velocity = p.getBaseVelocity(self.robot_id)\n",
    "        \n",
    "        base_orientation_euler = p.getEulerFromQuaternion(base_orientation)\n",
    "        \n",
    "        state = np.concatenate([joint_positions, joint_velocities, base_orientation_euler, base_angular_velocity])\n",
    "        \n",
    "        # Check for nan values in the state\n",
    "        if np.isnan(state).any():\n",
    "            print(\"Error: State contains NaN values\")\n",
    "            print(\"Joint positions:\", joint_positions)\n",
    "            print(\"Joint velocities:\", joint_velocities)\n",
    "            print(\"Base orientation (Euler):\", base_orientation_euler)\n",
    "            print(\"Base angular velocity:\", base_angular_velocity)\n",
    "        \n",
    "        return state\n",
    "    \n",
    "    \n",
    "    def compute_reward(self): #under testing \n",
    "        base_position, base_orientation = p.getBasePositionAndOrientation(self.robot_id)\n",
    "        base_velocity, base_angular_velocity = p.getBaseVelocity(self.robot_id)\n",
    "        \n",
    "        \n",
    "    \n",
    "        # Calculate the forward vector from the orientation\n",
    "        forward_vector = p.getMatrixFromQuaternion(base_orientation)[0:3:2]\n",
    "        forward_vector = np.array(forward_vector)  # Forward direction in the world frame\n",
    "        forward_velocity = np.dot(forward_vector, base_velocity[:2])\n",
    "        forward_reward = forward_velocity\n",
    "        \n",
    "       \n",
    "         # Penalize high velocities\n",
    "        max_velocity_threshold = 3.0  # Set the maximum desired velocity\n",
    "        if forward_velocity > max_velocity_threshold:\n",
    "            velocity_penalty = (forward_velocity - max_velocity_threshold) ** 2\n",
    "        else:\n",
    "            velocity_penalty = -1\n",
    "            \n",
    "        \n",
    "        \n",
    "        \n",
    "        base_orientation_euler = p.getEulerFromQuaternion(base_orientation)\n",
    "        pitch, roll = base_orientation_euler[1], base_orientation_euler[0]\n",
    "        # Set acceptable thresholds for instability\n",
    "        acceptable_pitch_threshold = 0.05  # Adjust as necessary\n",
    "        acceptable_roll_threshold = 0.05  # Adjust as necessary\n",
    " \n",
    "        stability_penalty = 0.0\n",
    "        stability_penalty += abs(pitch) - acceptable_pitch_threshold\n",
    "        stability_penalty += abs(roll) - acceptable_roll_threshold\n",
    "        # Penalize instability\n",
    "        \n",
    "       \n",
    "        \"\"\"\"\n",
    "        noise = 0.0\n",
    "        self.noise_counter += 1\n",
    "        if self.noise_counter % 100 == 0:  # Add noise every 100 steps\n",
    "            noise = np.random.normal(0, 1)  # Adjust the standard deviation as needed\n",
    "        \"\"\"\n",
    "        \n",
    "        joint_states = p.getJointStates(self.robot_id, self.joint_indices)\n",
    "        joint_angles = [state[0] for state in joint_states]\n",
    "        joint_velocities = [state[1] for state in joint_states]\n",
    "        # Apply dynamic energy penalty\n",
    "        energy_penalty = 0.0\n",
    "        joint_velocity_threshold = 10  # Threshold for penalizing high joint velocities\n",
    "        for velocity in joint_velocities:\n",
    "            if abs(velocity) > joint_velocity_threshold:\n",
    "                energy_penalty += (abs(velocity) - joint_velocity_threshold) ** 2 \n",
    "            else: energy_penalty= -100\n",
    "        # Penalize high joint velocities\n",
    "        \n",
    "        \n",
    "        \n",
    "    \n",
    "        fall_penalty = 0.0\n",
    "       \n",
    "        if base_position[2] < 0.12:\n",
    "         # If the robot has fallen (height threshold)\n",
    "            fall_penalty = 50.0 \n",
    "        else: fall_penalty= -1\n",
    "        \n",
    "        \n",
    "        smoothness_penalty = self.compute_smoothness_reward(joint_velocities)\n",
    "        \n",
    "        symmetry_penalty= self.compute_symmetry_reward()\n",
    "        \n",
    "        \n",
    "        # Scaling factors\n",
    "        forward_scale = 0#700\n",
    "        stability_penalty_scale =  0#-15 #was 0.2\n",
    "        energy_penalty_scale =  0#-0.1\n",
    "        fall_penalty_scale =  -500.0\n",
    "        \n",
    "        smoothness_scale= 0# -0.05\n",
    "        symmetry_penalty_scale = 0#5.0\n",
    "        velocity_penalty_scale = 0\n",
    "        \n",
    "        \n",
    "        reward = (forward_reward * forward_scale) + \\\n",
    "                 (velocity_penalty * velocity_penalty_scale)+ \\\n",
    "                 (energy_penalty*energy_penalty_scale) +\\\n",
    "                 (stability_penalty * stability_penalty_scale) + \\\n",
    "                 (fall_penalty * fall_penalty_scale) + \\\n",
    "                 (smoothness_penalty*smoothness_scale) +\\\n",
    "                 (symmetry_penalty * symmetry_penalty_scale)\n",
    "               \n",
    "                 \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        self.forward=self.forward+(forward_reward*forward_scale)\n",
    "        self.velocity_pen=self.velocity_pen+(velocity_penalty * velocity_penalty_scale)\n",
    "        self.stable=self.stable+(stability_penalty*stability_penalty_scale)\n",
    "        self.energy=(energy_penalty*energy_penalty_scale)+self.energy\n",
    "        self.fall=self.fall+(fall_penalty * fall_penalty_scale)\n",
    "        self.smooth=self.smooth+(smoothness_penalty*smoothness_scale)\n",
    "        self.sym=self.sym+(symmetry_penalty * symmetry_penalty_scale)  \n",
    "            \n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    \n",
    "    def compute_symmetry_reward(self):\n",
    "        # Get the height of each foot joint\n",
    "        front_left_height = p.getLinkState(self.robot_id, self.front_left_foot)[0][2]\n",
    "        front_right_height = p.getLinkState(self.robot_id, self.front_right_foot)[0][2]\n",
    "        back_left_height = p.getLinkState(self.robot_id, self.back_left_foot)[0][2]\n",
    "        back_right_height = p.getLinkState(self.robot_id, self.back_right_foot)[0][2]\n",
    "        \n",
    "        # Reward for diagonal leg coordination\n",
    "        symmetry_reward = 0.0\n",
    "        if abs(front_left_height - back_right_height) < 0.015:  # Adjust threshold as needed\n",
    "            symmetry_reward += 1.0\n",
    "        if abs(front_right_height - back_left_height) < 0.015:  # Adjust threshold as needed\n",
    "            symmetry_reward += 1.0\n",
    "\n",
    "        return symmetry_reward\n",
    "\n",
    "    \n",
    "    def compute_smoothness_reward(self, joint_velocities):\n",
    "        velocity_change_threshold = 2\n",
    "        smoothness_penalty=0\n",
    "        if self.previous_joint_velocities is None:\n",
    "            smoothness_reward = 0.0\n",
    "        else:\n",
    "            velocity_differences = np.abs(np.array(joint_velocities) - np.array(self.previous_joint_velocities))\n",
    "            velocity_differences = np.clip(velocity_differences - velocity_change_threshold, 0, None)\n",
    "            smoothness_penalty = np.sum(velocity_differences)\n",
    "            \n",
    "\n",
    "        self.previous_joint_velocities = joint_velocities\n",
    "        return smoothness_penalty\n",
    "\n",
    "   \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    def is_done(self, state):\n",
    "        \"\"\"\"\n",
    "        base_position, base_orientation = p.getBasePositionAndOrientation(self.robot_id)\n",
    "        \n",
    "        if base_position[2] < 0.2:  # If the robot has fallen\n",
    "            if self.fall_start_time is None:\n",
    "                self.fall_start_time = time.time()  # Start the timer\n",
    "               # print(\"Robot has started falling\")\n",
    "            elif time.time() - self.fall_start_time > 2:  # Wait for 2 seconds before deciding to reset\n",
    "               # print(\"Robot has fallen for 2 seconds, resetting...\")\n",
    "                return True\n",
    "        else:\n",
    "            self.fall_start_time = None  # Reset the timer if the robot is not fallen\n",
    "        \"\"\"\n",
    "         \n",
    "        if self.steps == 2500:\n",
    "            return True \n",
    "        \n",
    "        else:\n",
    "            return False\n",
    "    \n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "    \n",
    "    def close(self):\n",
    "        p.disconnect()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4367d57e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Register the environment\n",
    "gym.envs.registration.register(id='RoboticDog-v0', entry_point='__main__:RoboticDogEnv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9239dfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot ID: 1\n",
      "Number of joints: 16\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Create the environment\n",
    "env = RoboticDogEnv()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb8ff3a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf31ce60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Instantiate the agent\n",
    "#model = PPO('MlpPolicy', env, verbose=1)\n",
    "# for more exploration learning rate could co down till 1e-4 and entropy could go up till 0.1\n",
    "\n",
    "model = PPO('MlpPolicy', env, verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81aae870",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=1000000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd2345c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bf5d7ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "05342695",
   "metadata": {},
   "source": [
    "# Load the previously trained model\n",
    "model = PPO.load(\"C:\\\\Users\\\\youss\\\\OneDrive - aucegypt.edu\\\\Desktop\\\\bolt\\\\ppo_spot_trial_basic.zip\", env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6ba956f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: 0.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------\n",
      "| time/              |      |\n",
      "|    fps             | 89   |\n",
      "|    iterations      | 1    |\n",
      "|    time_elapsed    | 22   |\n",
      "|    total_timesteps | 2048 |\n",
      "-----------------------------\n",
      "Episode 2  finished with cumulative reward: -3441500.0 and \n",
      "with an average reward of: -1376.0495801679328\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2501\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1376.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -3.44e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 80            |\n",
      "|    iterations           | 2             |\n",
      "|    time_elapsed         | 50            |\n",
      "|    total_timesteps      | 4096          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 0.00012139455 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 4.17e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.57e+09      |\n",
      "|    n_updates            | 10            |\n",
      "|    policy_gradient_loss | -0.00236      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.79e+09      |\n",
      "-------------------------------------------\n",
      "Episode 3  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 5002\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -4.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 76           |\n",
      "|    iterations           | 3            |\n",
      "|    time_elapsed         | 80           |\n",
      "|    total_timesteps      | 6144         |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.568558e-05 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 1.79e-07     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.46e+09     |\n",
      "|    n_updates            | 20           |\n",
      "|    policy_gradient_loss | -0.00172     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.3e+09      |\n",
      "------------------------------------------\n",
      "Episode 4  finished with cumulative reward: -11984000.0 and \n",
      "with an average reward of: -4791.683326669332\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 7503\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4793.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 4             |\n",
      "|    time_elapsed         | 109           |\n",
      "|    total_timesteps      | 8192          |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6010588e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | -2.38e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.27e+09      |\n",
      "|    n_updates            | 30            |\n",
      "|    policy_gradient_loss | -0.00124      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.24e+10      |\n",
      "-------------------------------------------\n",
      "Episode 5  finished with cumulative reward: -12927500.0 and \n",
      "with an average reward of: -5168.932427029188\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 10004\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5171.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -8.27e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 5            |\n",
      "|    time_elapsed         | 138          |\n",
      "|    total_timesteps      | 10240        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.569157e-06 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | -1.55e-06    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.68e+09     |\n",
      "|    n_updates            | 40           |\n",
      "|    policy_gradient_loss | -0.000705    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.15e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -8.27e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 76            |\n",
      "|    iterations           | 6             |\n",
      "|    time_elapsed         | 160           |\n",
      "|    total_timesteps      | 12288         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1376181e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | -7.15e-07     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.55e+10      |\n",
      "|    n_updates            | 50            |\n",
      "|    policy_gradient_loss | -0.000366     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.05e+10      |\n",
      "-------------------------------------------\n",
      "Episode 6  finished with cumulative reward: -2013500.0 and \n",
      "with an average reward of: -805.077968812475\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 12505\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -805.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.02e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 75            |\n",
      "|    iterations           | 7             |\n",
      "|    time_elapsed         | 190           |\n",
      "|    total_timesteps      | 14336         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1598138e-05 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 6.56e-07      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.1e+09       |\n",
      "|    n_updates            | 60            |\n",
      "|    policy_gradient_loss | -0.000802     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.92e+09      |\n",
      "-------------------------------------------\n",
      "Episode 7  finished with cumulative reward: -10071500.0 and \n",
      "with an average reward of: -4026.9892043182726\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 15006\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4028.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.53e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 8             |\n",
      "|    time_elapsed         | 219           |\n",
      "|    total_timesteps      | 16384         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8383726e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | -6.68e-06     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+10      |\n",
      "|    n_updates            | 70            |\n",
      "|    policy_gradient_loss | -0.00032      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.42e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 8  finished with cumulative reward: -5966000.0 and \n",
      "with an average reward of: -2385.4458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 17507\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2386.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -7.3e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 9            |\n",
      "|    time_elapsed         | 249          |\n",
      "|    total_timesteps      | 18432        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.654323e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 1.03e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.76e+10     |\n",
      "|    n_updates            | 80           |\n",
      "|    policy_gradient_loss | -0.000224    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.8e+10      |\n",
      "------------------------------------------\n",
      "Episode 9  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 20008\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.39e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 10            |\n",
      "|    time_elapsed         | 278           |\n",
      "|    total_timesteps      | 20480         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7127985e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 1.08e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.45e+09      |\n",
      "|    n_updates            | 90            |\n",
      "|    policy_gradient_loss | -0.000193     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.62e+10      |\n",
      "-------------------------------------------\n",
      "Episode 10  finished with cumulative reward: -8465000.0 and \n",
      "with an average reward of: -3384.646141543383\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 22509\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3386.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.51e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 72            |\n",
      "|    iterations           | 11            |\n",
      "|    time_elapsed         | 309           |\n",
      "|    total_timesteps      | 22528         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7739165e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 3.7e-06       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.73e+09      |\n",
      "|    n_updates            | 100           |\n",
      "|    policy_gradient_loss | -0.000186     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.53e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.51e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 12            |\n",
      "|    time_elapsed         | 331           |\n",
      "|    total_timesteps      | 24576         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8888462e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 1.23e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.17e+09      |\n",
      "|    n_updates            | 110           |\n",
      "|    policy_gradient_loss | -0.000114     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.47e+10      |\n",
      "-------------------------------------------\n",
      "Episode 11  finished with cumulative reward: -8822000.0 and \n",
      "with an average reward of: -3527.389044382247\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 25010\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3528.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -7.64e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 73          |\n",
      "|    iterations           | 13          |\n",
      "|    time_elapsed         | 363         |\n",
      "|    total_timesteps      | 26624       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 3.48693e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 6.32e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 9.34e+09    |\n",
      "|    n_updates            | 120         |\n",
      "|    policy_gradient_loss | -0.000132   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.81e+10    |\n",
      "-----------------------------------------\n",
      "Episode 12  finished with cumulative reward: -2319500.0 and \n",
      "with an average reward of: -927.4290283886445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 27511\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -927.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.16e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 72            |\n",
      "|    iterations           | 14            |\n",
      "|    time_elapsed         | 395           |\n",
      "|    total_timesteps      | 28672         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.8615115e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | -2.65e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.72e+09      |\n",
      "|    n_updates            | 130           |\n",
      "|    policy_gradient_loss | -0.00016      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.06e+10      |\n",
      "-------------------------------------------\n",
      "Episode 13  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 30012\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.22e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 71            |\n",
      "|    iterations           | 15            |\n",
      "|    time_elapsed         | 427           |\n",
      "|    total_timesteps      | 30720         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8635474e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 1.29e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.48e+09      |\n",
      "|    n_updates            | 140           |\n",
      "|    policy_gradient_loss | -0.000281     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.79e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 14  finished with cumulative reward: -1605500.0 and \n",
      "with an average reward of: -641.9432227109156\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 32513\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -642.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 71            |\n",
      "|    iterations           | 16            |\n",
      "|    time_elapsed         | 458           |\n",
      "|    total_timesteps      | 32768         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.7550067e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 1.65e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.56e+10      |\n",
      "|    n_updates            | 150           |\n",
      "|    policy_gradient_loss | -0.000174     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.96e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 72            |\n",
      "|    iterations           | 17            |\n",
      "|    time_elapsed         | 480           |\n",
      "|    total_timesteps      | 34816         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5133777e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 2.41e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.03e+08      |\n",
      "|    n_updates            | 160           |\n",
      "|    policy_gradient_loss | -0.000362     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.45e+09      |\n",
      "-------------------------------------------\n",
      "Episode 15  finished with cumulative reward: -12035000.0 and \n",
      "with an average reward of: -4812.075169932027\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 35014\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4814.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.17e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 71            |\n",
      "|    iterations           | 18            |\n",
      "|    time_elapsed         | 512           |\n",
      "|    total_timesteps      | 36864         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4339533e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 7e-05         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.98e+10      |\n",
      "|    n_updates            | 170           |\n",
      "|    policy_gradient_loss | -8.99e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.16e+10      |\n",
      "-------------------------------------------\n",
      "Episode 16  finished with cumulative reward: -6578000.0 and \n",
      "with an average reward of: -2630.1479408236705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 37515\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2631.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -7.13e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 71            |\n",
      "|    iterations           | 19            |\n",
      "|    time_elapsed         | 544           |\n",
      "|    total_timesteps      | 38912         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7196871e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 1.54e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.16e+10      |\n",
      "|    n_updates            | 180           |\n",
      "|    policy_gradient_loss | -0.000293     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.11e+10      |\n",
      "-------------------------------------------\n",
      "Episode 17  finished with cumulative reward: -3008000.0 and \n",
      "with an average reward of: -1202.718912435026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 40016\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1203.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.87e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 20           |\n",
      "|    time_elapsed         | 576          |\n",
      "|    total_timesteps      | 40960        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.195907e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | -1.43e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.93e+09     |\n",
      "|    n_updates            | 190          |\n",
      "|    policy_gradient_loss | -0.000116    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.08e+10     |\n",
      "------------------------------------------\n",
      "Episode 18  finished with cumulative reward: -4385000.0 and \n",
      "with an average reward of: -1753.298680527789\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 42517\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1754.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -6.72e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 70          |\n",
      "|    iterations           | 21          |\n",
      "|    time_elapsed         | 607         |\n",
      "|    total_timesteps      | 43008       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.35018e-07 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 8.29e-06    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.23e+09    |\n",
      "|    n_updates            | 200         |\n",
      "|    policy_gradient_loss | -0.000186   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.38e+09    |\n",
      "-----------------------------------------\n",
      "Episode 19  finished with cumulative reward: -4028000.0 and \n",
      "with an average reward of: -1610.5557776889245\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 45018\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1611.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.57e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 22           |\n",
      "|    time_elapsed         | 638          |\n",
      "|    total_timesteps      | 45056        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.625778e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 3.81e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.55e+09     |\n",
      "|    n_updates            | 210          |\n",
      "|    policy_gradient_loss | -0.000164    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.11e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.57e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 71            |\n",
      "|    iterations           | 23            |\n",
      "|    time_elapsed         | 661           |\n",
      "|    total_timesteps      | 47104         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5369442e-06 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 1.51e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.39e+09      |\n",
      "|    n_updates            | 220           |\n",
      "|    policy_gradient_loss | -0.000266     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.94e+09      |\n",
      "-------------------------------------------\n",
      "Episode 20  finished with cumulative reward: -7572500.0 and \n",
      "with an average reward of: -3027.7888844462213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 47519\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3029.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 24           |\n",
      "|    time_elapsed         | 692          |\n",
      "|    total_timesteps      | 49152        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.874325e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000125     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.16e+09     |\n",
      "|    n_updates            | 230          |\n",
      "|    policy_gradient_loss | -0.000172    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.96e+10     |\n",
      "------------------------------------------\n",
      "Episode 21  finished with cumulative reward: -5889500.0 and \n",
      "with an average reward of: -2354.858056777289\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 50020\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2355.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.59e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 25            |\n",
      "|    time_elapsed         | 724           |\n",
      "|    total_timesteps      | 51200         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2482633e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000162      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.72e+09      |\n",
      "|    n_updates            | 240           |\n",
      "|    policy_gradient_loss | -7.54e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.68e+09      |\n",
      "-------------------------------------------\n",
      "Episode 22  finished with cumulative reward: -7776500.0 and \n",
      "with an average reward of: -3109.356257497001\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 52521\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3110.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.65e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 26            |\n",
      "|    time_elapsed         | 756           |\n",
      "|    total_timesteps      | 53248         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7089845e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 9.08e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.07e+09      |\n",
      "|    n_updates            | 250           |\n",
      "|    policy_gradient_loss | -0.000105     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.04e+10      |\n",
      "-------------------------------------------\n",
      "Episode 23  finished with cumulative reward: -4206500.0 and \n",
      "with an average reward of: -1681.9272291083566\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 55022\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1682.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.54e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 27            |\n",
      "|    time_elapsed         | 787           |\n",
      "|    total_timesteps      | 55296         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7005597e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000109      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.6e+09       |\n",
      "|    n_updates            | 260           |\n",
      "|    policy_gradient_loss | -0.000139     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.55e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.54e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 28            |\n",
      "|    time_elapsed         | 811           |\n",
      "|    total_timesteps      | 57344         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8670107e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00016       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.53e+09      |\n",
      "|    n_updates            | 270           |\n",
      "|    policy_gradient_loss | -8.26e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.32e+10      |\n",
      "-------------------------------------------\n",
      "Episode 24  finished with cumulative reward: -3365000.0 and \n",
      "with an average reward of: -1345.4618152738904\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 57523\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1346.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.4e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 29            |\n",
      "|    time_elapsed         | 842           |\n",
      "|    total_timesteps      | 59392         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8390787e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000173      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.12e+08      |\n",
      "|    n_updates            | 280           |\n",
      "|    policy_gradient_loss | -0.000118     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.71e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 25  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 60024\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.33e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 30           |\n",
      "|    time_elapsed         | 873          |\n",
      "|    total_timesteps      | 61440        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.923801e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | -1.93e-05    |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.78e+09     |\n",
      "|    n_updates            | 290          |\n",
      "|    policy_gradient_loss | -5.15e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.49e+09     |\n",
      "------------------------------------------\n",
      "Episode 26  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 62525\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.19e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 31           |\n",
      "|    time_elapsed         | 904          |\n",
      "|    total_timesteps      | 63488        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.899951e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000134     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.96e+09     |\n",
      "|    n_updates            | 300          |\n",
      "|    policy_gradient_loss | -6.1e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.12e+10     |\n",
      "------------------------------------------\n",
      "Episode 27  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 65026\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.07e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 32           |\n",
      "|    time_elapsed         | 934          |\n",
      "|    total_timesteps      | 65536        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.990143e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000165     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.95e+09     |\n",
      "|    n_updates            | 310          |\n",
      "|    policy_gradient_loss | -0.000163    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.36e+09     |\n",
      "------------------------------------------\n",
      "Episode 28  finished with cumulative reward: -3135500.0 and \n",
      "with an average reward of: -1253.6985205917633\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 67527\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1254.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.96e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 33            |\n",
      "|    time_elapsed         | 964           |\n",
      "|    total_timesteps      | 67584         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5439658e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000142      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.09e+08      |\n",
      "|    n_updates            | 320           |\n",
      "|    policy_gradient_loss | -9.95e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.99e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.96e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 34            |\n",
      "|    time_elapsed         | 987           |\n",
      "|    total_timesteps      | 69632         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0256266e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000331      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.85e+09      |\n",
      "|    n_updates            | 330           |\n",
      "|    policy_gradient_loss | -9.01e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.74e+09      |\n",
      "-------------------------------------------\n",
      "Episode 29  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 70028\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------------\n",
      "| rollout/                |                |\n",
      "|    ep_len_mean          | 2.5e+03        |\n",
      "|    ep_rew_mean          | -5.97e+06      |\n",
      "| time/                   |                |\n",
      "|    fps                  | 70             |\n",
      "|    iterations           | 35             |\n",
      "|    time_elapsed         | 1018           |\n",
      "|    total_timesteps      | 71680          |\n",
      "| train/                  |                |\n",
      "|    approx_kl            | 1.08615495e-07 |\n",
      "|    clip_fraction        | 0              |\n",
      "|    clip_range           | 0.2            |\n",
      "|    entropy_loss         | -17            |\n",
      "|    explained_variance   | 6.05e-05       |\n",
      "|    learning_rate        | 0.0003         |\n",
      "|    loss                 | 8.11e+09       |\n",
      "|    n_updates            | 340            |\n",
      "|    policy_gradient_loss | -5.69e-05      |\n",
      "|    std                  | 1              |\n",
      "|    value_loss           | 1.03e+10       |\n",
      "--------------------------------------------\n",
      "Episode 30  finished with cumulative reward: -6425000.0 and \n",
      "with an average reward of: -2568.972411035586\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 72529\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2570.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.99e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 36            |\n",
      "|    time_elapsed         | 1050          |\n",
      "|    total_timesteps      | 73728         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9732397e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000217      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.86e+10      |\n",
      "|    n_updates            | 350           |\n",
      "|    policy_gradient_loss | -3.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.7e+10       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 31  finished with cumulative reward: -8694500.0 and \n",
      "with an average reward of: -3476.4094362255096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 75030\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3477.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 37           |\n",
      "|    time_elapsed         | 1081         |\n",
      "|    total_timesteps      | 75776        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.511944e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000126     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.74e+09     |\n",
      "|    n_updates            | 360          |\n",
      "|    policy_gradient_loss | -8.6e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.38e+09     |\n",
      "------------------------------------------\n",
      "Episode 32  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 77531\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.04e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 38           |\n",
      "|    time_elapsed         | 1111         |\n",
      "|    total_timesteps      | 77824        |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.162698e-07 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000137     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.45e+09     |\n",
      "|    n_updates            | 370          |\n",
      "|    policy_gradient_loss | -6.63e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.69e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.04e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 39            |\n",
      "|    time_elapsed         | 1134          |\n",
      "|    total_timesteps      | 79872         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9281364e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000111      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.22e+09      |\n",
      "|    n_updates            | 380           |\n",
      "|    policy_gradient_loss | -0.000119     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.19e+10      |\n",
      "-------------------------------------------\n",
      "Episode 33  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 80032\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.06e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 40            |\n",
      "|    time_elapsed         | 1165          |\n",
      "|    total_timesteps      | 81920         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4979742e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000195      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.13e+09      |\n",
      "|    n_updates            | 390           |\n",
      "|    policy_gradient_loss | -7.83e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.97e+10      |\n",
      "-------------------------------------------\n",
      "Episode 34  finished with cumulative reward: -11856500.0 and \n",
      "with an average reward of: -4740.703718512595\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 82533\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4742.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.24e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 41            |\n",
      "|    time_elapsed         | 1196          |\n",
      "|    total_timesteps      | 83968         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4857505e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000355      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.04e+10      |\n",
      "|    n_updates            | 400           |\n",
      "|    policy_gradient_loss | -8.15e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.87e+10      |\n",
      "-------------------------------------------\n",
      "Episode 35  finished with cumulative reward: -4844000.0 and \n",
      "with an average reward of: -1936.8252698920433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 85034\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1937.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.2e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 42            |\n",
      "|    time_elapsed         | 1228          |\n",
      "|    total_timesteps      | 86016         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0282383e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000276      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.61e+10      |\n",
      "|    n_updates            | 410           |\n",
      "|    policy_gradient_loss | -6.47e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.2e+10       |\n",
      "-------------------------------------------\n",
      "Episode 36  finished with cumulative reward: -5405000.0 and \n",
      "with an average reward of: -2161.135545781687\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 87535\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2162.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.17e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 43            |\n",
      "|    time_elapsed         | 1259          |\n",
      "|    total_timesteps      | 88064         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3172394e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000214      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.88e+08      |\n",
      "|    n_updates            | 420           |\n",
      "|    policy_gradient_loss | -8.26e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.4e+09       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 37  finished with cumulative reward: -4487000.0 and \n",
      "with an average reward of: -1794.0823670531788\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 90036\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1794.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.13e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 44            |\n",
      "|    time_elapsed         | 1289          |\n",
      "|    total_timesteps      | 90112         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2223609e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000305      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.92e+09      |\n",
      "|    n_updates            | 430           |\n",
      "|    policy_gradient_loss | -2.41e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.34e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.13e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 45            |\n",
      "|    time_elapsed         | 1311          |\n",
      "|    total_timesteps      | 92160         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.7811925e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000342      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.77e+09      |\n",
      "|    n_updates            | 440           |\n",
      "|    policy_gradient_loss | -5.33e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.99e+09      |\n",
      "-------------------------------------------\n",
      "Episode 38  finished with cumulative reward: -8439500.0 and \n",
      "with an average reward of: -3374.450219912035\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 92537\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3375.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.19e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 46            |\n",
      "|    time_elapsed         | 1343          |\n",
      "|    total_timesteps      | 94208         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.4365955e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000461      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.95e+09      |\n",
      "|    n_updates            | 450           |\n",
      "|    policy_gradient_loss | -5.29e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.51e+10      |\n",
      "-------------------------------------------\n",
      "Episode 39  finished with cumulative reward: -7929500.0 and \n",
      "with an average reward of: -3170.531787285086\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 95038\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3171.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.24e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 47            |\n",
      "|    time_elapsed         | 1373          |\n",
      "|    total_timesteps      | 96256         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0401785e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000191      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.67e+09      |\n",
      "|    n_updates            | 460           |\n",
      "|    policy_gradient_loss | -0.000108     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.35e+10      |\n",
      "-------------------------------------------\n",
      "Episode 40  finished with cumulative reward: -5226500.0 and \n",
      "with an average reward of: -2089.764094362255\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 97539\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2090.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.21e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 48            |\n",
      "|    time_elapsed         | 1404          |\n",
      "|    total_timesteps      | 98304         |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5582267e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000292      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.41e+09      |\n",
      "|    n_updates            | 470           |\n",
      "|    policy_gradient_loss | -3.31e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.79e+10      |\n",
      "-------------------------------------------\n",
      "Episode 41  finished with cumulative reward: -1784000.0 and \n",
      "with an average reward of: -713.3146741303478\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 100040\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -713.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.1e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 49            |\n",
      "|    time_elapsed         | 1435          |\n",
      "|    total_timesteps      | 100352        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1117663e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000715      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.3e+08       |\n",
      "|    n_updates            | 480           |\n",
      "|    policy_gradient_loss | -7.54e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.32e+08      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.1e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 50           |\n",
      "|    time_elapsed         | 1460         |\n",
      "|    total_timesteps      | 102400       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.745096e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000373     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.74e+09     |\n",
      "|    n_updates            | 490          |\n",
      "|    policy_gradient_loss | -5.01e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.96e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 42  finished with cumulative reward: -713000.0 and \n",
      "with an average reward of: -285.0859656137545\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 102541\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -285.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.97e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 51            |\n",
      "|    time_elapsed         | 1492          |\n",
      "|    total_timesteps      | 104448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0180672e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | -4.61e-05     |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.05e+09      |\n",
      "|    n_updates            | 500           |\n",
      "|    policy_gradient_loss | -4.06e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.27e+09      |\n",
      "-------------------------------------------\n",
      "Episode 43  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 105042\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -6.01e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 52          |\n",
      "|    time_elapsed         | 1523        |\n",
      "|    total_timesteps      | 106496      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 7.14208e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.000245    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 4.74e+09    |\n",
      "|    n_updates            | 510         |\n",
      "|    policy_gradient_loss | -5.01e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 8.56e+09    |\n",
      "-----------------------------------------\n",
      "Episode 44  finished with cumulative reward: -2243000.0 and \n",
      "with an average reward of: -896.8412634946021\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 107543\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -897.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.92e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 53           |\n",
      "|    time_elapsed         | 1554         |\n",
      "|    total_timesteps      | 108544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.902862e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000555     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.3e+09      |\n",
      "|    n_updates            | 520          |\n",
      "|    policy_gradient_loss | -7.13e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.34e+10     |\n",
      "------------------------------------------\n",
      "Episode 45  finished with cumulative reward: -1554500.0 and \n",
      "with an average reward of: -621.5513794482207\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 110044\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -621.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.82e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 54            |\n",
      "|    time_elapsed         | 1585          |\n",
      "|    total_timesteps      | 110592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4880789e-07 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000169      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.87e+09      |\n",
      "|    n_updates            | 530           |\n",
      "|    policy_gradient_loss | -7.97e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.37e+09      |\n",
      "-------------------------------------------\n",
      "Episode 46  finished with cumulative reward: -7904000.0 and \n",
      "with an average reward of: -3160.3358656537384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 112545\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3161.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.87e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 55            |\n",
      "|    time_elapsed         | 1615          |\n",
      "|    total_timesteps      | 112640        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2904715e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 8.54e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.38e+09      |\n",
      "|    n_updates            | 540           |\n",
      "|    policy_gradient_loss | -3.1e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.64e+09      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.87e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 56           |\n",
      "|    time_elapsed         | 1637         |\n",
      "|    total_timesteps      | 114688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.188646e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000139     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.59e+09     |\n",
      "|    n_updates            | 550          |\n",
      "|    policy_gradient_loss | -2.05e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.49e+10     |\n",
      "------------------------------------------\n",
      "Episode 47  finished with cumulative reward: -2090000.0 and \n",
      "with an average reward of: -835.6657337065174\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 115046\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -836.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.79e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 57           |\n",
      "|    time_elapsed         | 1668         |\n",
      "|    total_timesteps      | 116736       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.443588e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000821     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.37e+09     |\n",
      "|    n_updates            | 560          |\n",
      "|    policy_gradient_loss | -5.82e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.86e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 48  finished with cumulative reward: -126500.0 and \n",
      "with an average reward of: -50.5797680927629\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 117547\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -50.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.66e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 58           |\n",
      "|    time_elapsed         | 1700         |\n",
      "|    total_timesteps      | 118784       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.796916e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000196     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+09     |\n",
      "|    n_updates            | 570          |\n",
      "|    policy_gradient_loss | -5.92e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.93e+09     |\n",
      "------------------------------------------\n",
      "Episode 49  finished with cumulative reward: -7878500.0 and \n",
      "with an average reward of: -3150.139944022391\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 120048\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3151.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 59            |\n",
      "|    time_elapsed         | 1731          |\n",
      "|    total_timesteps      | 120832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.4068868e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 2.55e-05      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.26e+09      |\n",
      "|    n_updates            | 580           |\n",
      "|    policy_gradient_loss | -3.09e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.25e+09      |\n",
      "-------------------------------------------\n",
      "Episode 50  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 122549\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.72e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 60            |\n",
      "|    time_elapsed         | 1763          |\n",
      "|    total_timesteps      | 122880        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7439244e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000245      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.38e+10      |\n",
      "|    n_updates            | 590           |\n",
      "|    policy_gradient_loss | -1.9e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.27e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.72e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 61            |\n",
      "|    time_elapsed         | 1785          |\n",
      "|    total_timesteps      | 124928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4284312e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000218      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.22e+09      |\n",
      "|    n_updates            | 600           |\n",
      "|    policy_gradient_loss | -4.44e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.11e+10      |\n",
      "-------------------------------------------\n",
      "Episode 51  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 125050\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.73e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 62           |\n",
      "|    time_elapsed         | 1817         |\n",
      "|    total_timesteps      | 126976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.136826e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000642     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.19e+09     |\n",
      "|    n_updates            | 610          |\n",
      "|    policy_gradient_loss | -5.19e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.12e+10     |\n",
      "------------------------------------------\n",
      "Episode 52  finished with cumulative reward: -7292000.0 and \n",
      "with an average reward of: -2915.6337465013994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 127551\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2916.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.76e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 63            |\n",
      "|    time_elapsed         | 1848          |\n",
      "|    total_timesteps      | 129024        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2608008e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000516      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.22e+09      |\n",
      "|    n_updates            | 620           |\n",
      "|    policy_gradient_loss | -4.67e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.2e+10       |\n",
      "-------------------------------------------\n",
      "Episode 53  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 130052\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.76e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 64            |\n",
      "|    time_elapsed         | 1879          |\n",
      "|    total_timesteps      | 131072        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.3050614e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00041       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.38e+09      |\n",
      "|    n_updates            | 630           |\n",
      "|    policy_gradient_loss | -6.68e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.78e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 54  finished with cumulative reward: -5889500.0 and \n",
      "with an average reward of: -2354.858056777289\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 132553\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2355.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.76e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 65           |\n",
      "|    time_elapsed         | 1910         |\n",
      "|    total_timesteps      | 133120       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.504706e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000158     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.02e+09     |\n",
      "|    n_updates            | 640          |\n",
      "|    policy_gradient_loss | -5.39e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.78e+09     |\n",
      "------------------------------------------\n",
      "Episode 55  finished with cumulative reward: -10224500.0 and \n",
      "with an average reward of: -4088.1647341063576\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 135054\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4089.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.85e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 66            |\n",
      "|    time_elapsed         | 1940          |\n",
      "|    total_timesteps      | 135168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.5070116e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000418      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.01e+09      |\n",
      "|    n_updates            | 650           |\n",
      "|    policy_gradient_loss | -4.02e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.51e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.85e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 67            |\n",
      "|    time_elapsed         | 1964          |\n",
      "|    total_timesteps      | 137216        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5890691e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000711      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.22e+09      |\n",
      "|    n_updates            | 660           |\n",
      "|    policy_gradient_loss | -3.19e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.31e+10      |\n",
      "-------------------------------------------\n",
      "Episode 56  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 137555\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.85e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 68            |\n",
      "|    time_elapsed         | 1996          |\n",
      "|    total_timesteps      | 139264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7008355e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000634      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.65e+09      |\n",
      "|    n_updates            | 670           |\n",
      "|    policy_gradient_loss | -3.95e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.42e+10      |\n",
      "-------------------------------------------\n",
      "Episode 57  finished with cumulative reward: -10428500.0 and \n",
      "with an average reward of: -4169.732107157137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 140056\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4171.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.93e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 69            |\n",
      "|    time_elapsed         | 2027          |\n",
      "|    total_timesteps      | 141312        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7718564e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000306      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.12e+09      |\n",
      "|    n_updates            | 680           |\n",
      "|    policy_gradient_loss | -3.83e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.25e+10      |\n",
      "-------------------------------------------\n",
      "Episode 58  finished with cumulative reward: -9077000.0 and \n",
      "with an average reward of: -3629.348260695722\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 142557\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3630.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.98e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 70           |\n",
      "|    time_elapsed         | 2058         |\n",
      "|    total_timesteps      | 143360       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.693843e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000428     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.14e+10     |\n",
      "|    n_updates            | 690          |\n",
      "|    policy_gradient_loss | -2.66e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.75e+10     |\n",
      "------------------------------------------\n",
      "Episode 59  finished with cumulative reward: -4461500.0 and \n",
      "with an average reward of: -1783.8864454218312\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 145058\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1784.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.96e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 71           |\n",
      "|    time_elapsed         | 2088         |\n",
      "|    total_timesteps      | 145408       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.537854e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000463     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.45e+09     |\n",
      "|    n_updates            | 700          |\n",
      "|    policy_gradient_loss | -4.12e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.09e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.96e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 72            |\n",
      "|    time_elapsed         | 2110          |\n",
      "|    total_timesteps      | 147456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6379788e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000265      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.64e+09      |\n",
      "|    n_updates            | 710           |\n",
      "|    policy_gradient_loss | -3.96e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.01e+10      |\n",
      "-------------------------------------------\n",
      "Episode 60  finished with cumulative reward: -11652500.0 and \n",
      "with an average reward of: -4659.136345461815\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 147559\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4661.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -6.05e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 73          |\n",
      "|    time_elapsed         | 2141        |\n",
      "|    total_timesteps      | 149504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.78758e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.000317    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1e+10       |\n",
      "|    n_updates            | 720         |\n",
      "|    policy_gradient_loss | -4.66e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 2.58e+10    |\n",
      "-----------------------------------------\n",
      "Episode 61  finished with cumulative reward: -3008000.0 and \n",
      "with an average reward of: -1202.718912435026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 150060\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1203.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6e+06       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 74           |\n",
      "|    time_elapsed         | 2172         |\n",
      "|    total_timesteps      | 151552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.736932e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000627     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.15e+09     |\n",
      "|    n_updates            | 730          |\n",
      "|    policy_gradient_loss | -4.61e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.74e+09     |\n",
      "------------------------------------------\n",
      "Episode 62  finished with cumulative reward: -1325000.0 and \n",
      "with an average reward of: -529.7880847660936\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 152561\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -530.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.93e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 75           |\n",
      "|    time_elapsed         | 2204         |\n",
      "|    total_timesteps      | 153600       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.614129e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00156      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.38e+07     |\n",
      "|    n_updates            | 740          |\n",
      "|    policy_gradient_loss | -5.55e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.7e+08      |\n",
      "------------------------------------------\n",
      "Episode 63  finished with cumulative reward: -4895000.0 and \n",
      "with an average reward of: -1957.2171131547382\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 155062\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1958.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.91e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 76            |\n",
      "|    time_elapsed         | 2235          |\n",
      "|    total_timesteps      | 155648        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3760443e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000879      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.07e+09      |\n",
      "|    n_updates            | 750           |\n",
      "|    policy_gradient_loss | -2.01e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.34e+09      |\n",
      "-------------------------------------------\n",
      "Episode 64  finished with cumulative reward: -1427000.0 and \n",
      "with an average reward of: -570.5717712914834\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 157563\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -570.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.84e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 77            |\n",
      "|    time_elapsed         | 2266          |\n",
      "|    total_timesteps      | 157696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2386895e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00115       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.31e+08      |\n",
      "|    n_updates            | 760           |\n",
      "|    policy_gradient_loss | -2.27e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.45e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.84e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 78            |\n",
      "|    time_elapsed         | 2289          |\n",
      "|    total_timesteps      | 159744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1257514e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00146       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.95e+09      |\n",
      "|    n_updates            | 770           |\n",
      "|    policy_gradient_loss | -4.22e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.86e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 65  finished with cumulative reward: -2396000.0 and \n",
      "with an average reward of: -958.0167932826869\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 160064\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -958.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 79            |\n",
      "|    time_elapsed         | 2320          |\n",
      "|    total_timesteps      | 161792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6670826e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000861      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.2e+09       |\n",
      "|    n_updates            | 780           |\n",
      "|    policy_gradient_loss | -2.06e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.95e+09      |\n",
      "-------------------------------------------\n",
      "Episode 66  finished with cumulative reward: -5966000.0 and \n",
      "with an average reward of: -2385.4458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 162565\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2386.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 80            |\n",
      "|    time_elapsed         | 2352          |\n",
      "|    total_timesteps      | 163840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0186341e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000718      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.29e+09      |\n",
      "|    n_updates            | 790           |\n",
      "|    policy_gradient_loss | -1.5e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.17e+10      |\n",
      "-------------------------------------------\n",
      "Episode 67  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 165066\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.78e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 81            |\n",
      "|    time_elapsed         | 2383          |\n",
      "|    total_timesteps      | 165888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5465852e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000804      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.19e+09      |\n",
      "|    n_updates            | 800           |\n",
      "|    policy_gradient_loss | -3.63e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.48e+09      |\n",
      "-------------------------------------------\n",
      "Episode 68  finished with cumulative reward: -4844000.0 and \n",
      "with an average reward of: -1936.8252698920433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 167567\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1937.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.77e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 82           |\n",
      "|    time_elapsed         | 2414         |\n",
      "|    total_timesteps      | 167936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.373739e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000885     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.57e+09     |\n",
      "|    n_updates            | 810          |\n",
      "|    policy_gradient_loss | -2.14e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.34e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.77e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 83            |\n",
      "|    time_elapsed         | 2437          |\n",
      "|    total_timesteps      | 169984        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3265678e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00152       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.4e+09       |\n",
      "|    n_updates            | 820           |\n",
      "|    policy_gradient_loss | -4.69e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.99e+09      |\n",
      "-------------------------------------------\n",
      "Episode 69  finished with cumulative reward: -6782000.0 and \n",
      "with an average reward of: -2711.71531387445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 170068\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2712.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.78e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 84            |\n",
      "|    time_elapsed         | 2467          |\n",
      "|    total_timesteps      | 172032        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.0535967e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000613      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1e+10         |\n",
      "|    n_updates            | 830           |\n",
      "|    policy_gradient_loss | -1.96e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.87e+10      |\n",
      "-------------------------------------------\n",
      "Episode 70  finished with cumulative reward: -7751000.0 and \n",
      "with an average reward of: -3099.160335865654\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 172569\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3100.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.81e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 85            |\n",
      "|    time_elapsed         | 2498          |\n",
      "|    total_timesteps      | 174080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.001         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.84e+09      |\n",
      "|    n_updates            | 840           |\n",
      "|    policy_gradient_loss | -1.29e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.94e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 71  finished with cumulative reward: -7521500.0 and \n",
      "with an average reward of: -3007.3970411835267\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 175070\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3008.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.84e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 86            |\n",
      "|    time_elapsed         | 2530          |\n",
      "|    total_timesteps      | 176128        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2252713e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00126       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.8e+09       |\n",
      "|    n_updates            | 850           |\n",
      "|    policy_gradient_loss | -3.41e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.08e+10      |\n",
      "-------------------------------------------\n",
      "Episode 72  finished with cumulative reward: -4130000.0 and \n",
      "with an average reward of: -1651.3394642143144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 177571\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1652.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.81e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 87           |\n",
      "|    time_elapsed         | 2562         |\n",
      "|    total_timesteps      | 178176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.180482e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000497     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.77e+09     |\n",
      "|    n_updates            | 860          |\n",
      "|    policy_gradient_loss | -1.76e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.34e+10     |\n",
      "------------------------------------------\n",
      "Episode 73  finished with cumulative reward: -8924000.0 and \n",
      "with an average reward of: -3568.172730907637\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 180072\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3569.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.85e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 88            |\n",
      "|    time_elapsed         | 2593          |\n",
      "|    total_timesteps      | 180224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1496013e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00186       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.75e+09      |\n",
      "|    n_updates            | 870           |\n",
      "|    policy_gradient_loss | -2.98e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.22e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.85e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 89            |\n",
      "|    time_elapsed         | 2615          |\n",
      "|    total_timesteps      | 182272        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3655746e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00058       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.42e+09      |\n",
      "|    n_updates            | 880           |\n",
      "|    policy_gradient_loss | -9e-06        |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.46e+10      |\n",
      "-------------------------------------------\n",
      "Episode 74  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 182573\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.85e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 90           |\n",
      "|    time_elapsed         | 2647         |\n",
      "|    total_timesteps      | 184320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.003553e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000992     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.16e+09     |\n",
      "|    n_updates            | 890          |\n",
      "|    policy_gradient_loss | -2.35e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.12e+09     |\n",
      "------------------------------------------\n",
      "Episode 75  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 185074\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.83e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 91            |\n",
      "|    time_elapsed         | 2677          |\n",
      "|    total_timesteps      | 186368        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1350494e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000288      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.09e+09      |\n",
      "|    n_updates            | 900           |\n",
      "|    policy_gradient_loss | -2.54e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.73e+10      |\n",
      "-------------------------------------------\n",
      "Episode 76  finished with cumulative reward: -1809500.0 and \n",
      "with an average reward of: -723.5105957616953\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 187575\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -723.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.78e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 92            |\n",
      "|    time_elapsed         | 2708          |\n",
      "|    total_timesteps      | 188416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3271347e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000102      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.15e+09      |\n",
      "|    n_updates            | 910           |\n",
      "|    policy_gradient_loss | -2.38e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.87e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 77  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 190076\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.76e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 93           |\n",
      "|    time_elapsed         | 2738         |\n",
      "|    total_timesteps      | 190464       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.688627e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000204     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.9e+09      |\n",
      "|    n_updates            | 920          |\n",
      "|    policy_gradient_loss | -2.66e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.18e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.76e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 94           |\n",
      "|    time_elapsed         | 2762         |\n",
      "|    total_timesteps      | 192512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.773028e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000387     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6e+09        |\n",
      "|    n_updates            | 930          |\n",
      "|    policy_gradient_loss | -2.21e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.59e+09     |\n",
      "------------------------------------------\n",
      "Episode 78  finished with cumulative reward: -4436000.0 and \n",
      "with an average reward of: -1773.690523790484\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 192577\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1774.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.74e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 95            |\n",
      "|    time_elapsed         | 2794          |\n",
      "|    total_timesteps      | 194560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8102583e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00222       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.36e+08      |\n",
      "|    n_updates            | 940           |\n",
      "|    policy_gradient_loss | -3.22e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.44e+09      |\n",
      "-------------------------------------------\n",
      "Episode 79  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 195078\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.76e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 96            |\n",
      "|    time_elapsed         | 2824          |\n",
      "|    total_timesteps      | 196608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9976945e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000627      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.28e+09      |\n",
      "|    n_updates            | 950           |\n",
      "|    policy_gradient_loss | -1.75e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.57e+10      |\n",
      "-------------------------------------------\n",
      "Episode 80  finished with cumulative reward: -4130000.0 and \n",
      "with an average reward of: -1651.3394642143144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 197579\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1652.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.74e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 97           |\n",
      "|    time_elapsed         | 2854         |\n",
      "|    total_timesteps      | 198656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.717126e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00105      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+09     |\n",
      "|    n_updates            | 960          |\n",
      "|    policy_gradient_loss | -1.61e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.1e+09      |\n",
      "------------------------------------------\n",
      "Episode 81  finished with cumulative reward: -1554500.0 and \n",
      "with an average reward of: -621.5513794482207\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 200080\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -621.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.69e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 98            |\n",
      "|    time_elapsed         | 2885          |\n",
      "|    total_timesteps      | 200704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0506483e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00108       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.48e+09      |\n",
      "|    n_updates            | 970           |\n",
      "|    policy_gradient_loss | -2.75e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.67e+09      |\n",
      "-------------------------------------------\n",
      "Episode 82  finished with cumulative reward: -9995000.0 and \n",
      "with an average reward of: -3996.4014394242304\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 202581\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3998.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.74e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 99           |\n",
      "|    time_elapsed         | 2916         |\n",
      "|    total_timesteps      | 202752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.516953e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00135      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.65e+08     |\n",
      "|    n_updates            | 980          |\n",
      "|    policy_gradient_loss | -2.7e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.93e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.74e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 100           |\n",
      "|    time_elapsed         | 2939          |\n",
      "|    total_timesteps      | 204800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.2759576e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00187       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+10      |\n",
      "|    n_updates            | 990           |\n",
      "|    policy_gradient_loss | -1.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.27e+10      |\n",
      "-------------------------------------------\n",
      "Episode 83  finished with cumulative reward: -6297500.0 and \n",
      "with an average reward of: -2517.9928028788486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 205082\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2519.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.75e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 101           |\n",
      "|    time_elapsed         | 2970          |\n",
      "|    total_timesteps      | 206848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.5582267e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000699      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.81e+09      |\n",
      "|    n_updates            | 1000          |\n",
      "|    policy_gradient_loss | -3.63e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.63e+10      |\n",
      "-------------------------------------------\n",
      "Episode 84  finished with cumulative reward: -9000500.0 and \n",
      "with an average reward of: -3598.7604958016796\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 207583\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3600.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 102           |\n",
      "|    time_elapsed         | 3001          |\n",
      "|    total_timesteps      | 208896        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000607      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.43e+10      |\n",
      "|    n_updates            | 1010          |\n",
      "|    policy_gradient_loss | -7.9e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.36e+10      |\n",
      "-------------------------------------------\n",
      "Episode 85  finished with cumulative reward: -4206500.0 and \n",
      "with an average reward of: -1681.9272291083566\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 210084\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1682.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.77e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 103           |\n",
      "|    time_elapsed         | 3032          |\n",
      "|    total_timesteps      | 210944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3009412e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00166       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.53e+09      |\n",
      "|    n_updates            | 1020          |\n",
      "|    policy_gradient_loss | -3.21e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.86e+09      |\n",
      "-------------------------------------------\n",
      "Episode 86  finished with cumulative reward: -9128000.0 and \n",
      "with an average reward of: -3649.7401039584165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 212585\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3651.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.81e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 104          |\n",
      "|    time_elapsed         | 3062         |\n",
      "|    total_timesteps      | 212992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000904     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.37e+10     |\n",
      "|    n_updates            | 1030         |\n",
      "|    policy_gradient_loss | -1.02e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.07e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.81e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 105           |\n",
      "|    time_elapsed         | 3085          |\n",
      "|    total_timesteps      | 215040        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1088559e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00102       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.25e+09      |\n",
      "|    n_updates            | 1040          |\n",
      "|    policy_gradient_loss | -2.47e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.77e+10      |\n",
      "-------------------------------------------\n",
      "Episode 87  finished with cumulative reward: -9918500.0 and \n",
      "with an average reward of: -3965.813674530188\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 215086\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3967.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.85e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 106           |\n",
      "|    time_elapsed         | 3116          |\n",
      "|    total_timesteps      | 217088        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.1723175e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000943      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.08e+10      |\n",
      "|    n_updates            | 1050          |\n",
      "|    policy_gradient_loss | -2.06e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.47e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 88  finished with cumulative reward: -3594500.0 and \n",
      "with an average reward of: -1437.2251099560176\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 217587\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1437.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.83e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 107           |\n",
      "|    time_elapsed         | 3148          |\n",
      "|    total_timesteps      | 219136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6065695e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00183       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.11e+09      |\n",
      "|    n_updates            | 1060          |\n",
      "|    policy_gradient_loss | -2.26e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.89e+09      |\n",
      "-------------------------------------------\n",
      "Episode 89  finished with cumulative reward: -4691000.0 and \n",
      "with an average reward of: -1875.6497401039585\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 220088\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1876.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.82e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 108           |\n",
      "|    time_elapsed         | 3179          |\n",
      "|    total_timesteps      | 221184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1490725e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000665      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.82e+09      |\n",
      "|    n_updates            | 1070          |\n",
      "|    policy_gradient_loss | -1.4e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.28e+10      |\n",
      "-------------------------------------------\n",
      "Episode 90  finished with cumulative reward: -11729000.0 and \n",
      "with an average reward of: -4689.724110355858\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 222589\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4691.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.88e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 109           |\n",
      "|    time_elapsed         | 3211          |\n",
      "|    total_timesteps      | 223232        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000449      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.62e+09      |\n",
      "|    n_updates            | 1080          |\n",
      "|    policy_gradient_loss | -8.92e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.48e+10      |\n",
      "-------------------------------------------\n",
      "Episode 91  finished with cumulative reward: -2498000.0 and \n",
      "with an average reward of: -998.8004798080767\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 225090\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -999.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "----------------------------------------\n",
      "| rollout/                |            |\n",
      "|    ep_len_mean          | 2.5e+03    |\n",
      "|    ep_rew_mean          | -5.84e+06  |\n",
      "| time/                   |            |\n",
      "|    fps                  | 69         |\n",
      "|    iterations           | 110        |\n",
      "|    time_elapsed         | 3241       |\n",
      "|    total_timesteps      | 225280     |\n",
      "| train/                  |            |\n",
      "|    approx_kl            | 6.8394e-09 |\n",
      "|    clip_fraction        | 0          |\n",
      "|    clip_range           | 0.2        |\n",
      "|    entropy_loss         | -17        |\n",
      "|    explained_variance   | 0.00157    |\n",
      "|    learning_rate        | 0.0003     |\n",
      "|    loss                 | 8.18e+09   |\n",
      "|    n_updates            | 1090       |\n",
      "|    policy_gradient_loss | -2.36e-05  |\n",
      "|    std                  | 1          |\n",
      "|    value_loss           | 1.5e+10    |\n",
      "----------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.84e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 111          |\n",
      "|    time_elapsed         | 3264         |\n",
      "|    total_timesteps      | 227328       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.905772e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0012       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.35e+09     |\n",
      "|    n_updates            | 1100         |\n",
      "|    policy_gradient_loss | -2.53e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.6e+09      |\n",
      "------------------------------------------\n",
      "Episode 92  finished with cumulative reward: -5099000.0 and \n",
      "with an average reward of: -2038.7844862055179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 227591\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2039.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.84e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 112          |\n",
      "|    time_elapsed         | 3296         |\n",
      "|    total_timesteps      | 229376       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.085006e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000912     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.65e+09     |\n",
      "|    n_updates            | 1110         |\n",
      "|    policy_gradient_loss | -1.65e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.04e+10     |\n",
      "------------------------------------------\n",
      "Episode 93  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 230092\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.82e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 113           |\n",
      "|    time_elapsed         | 3326          |\n",
      "|    total_timesteps      | 231424        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9453076e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000858      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.33e+09      |\n",
      "|    n_updates            | 1120          |\n",
      "|    policy_gradient_loss | -3.75e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.49e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 94  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 232593\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.83e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 114           |\n",
      "|    time_elapsed         | 3358          |\n",
      "|    total_timesteps      | 233472        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2700988e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00102       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.56e+09      |\n",
      "|    n_updates            | 1130          |\n",
      "|    policy_gradient_loss | -1.69e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.07e+10      |\n",
      "-------------------------------------------\n",
      "Episode 95  finished with cumulative reward: -5405000.0 and \n",
      "with an average reward of: -2161.135545781687\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 235094\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2162.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.82e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 115          |\n",
      "|    time_elapsed         | 3389         |\n",
      "|    total_timesteps      | 235520       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.770723e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00142      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.15e+09     |\n",
      "|    n_updates            | 1140         |\n",
      "|    policy_gradient_loss | -2.55e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.42e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.82e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 116           |\n",
      "|    time_elapsed         | 3411          |\n",
      "|    total_timesteps      | 237568        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9499566e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00118       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.92e+09      |\n",
      "|    n_updates            | 1150          |\n",
      "|    policy_gradient_loss | -1.66e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.16e+10      |\n",
      "-------------------------------------------\n",
      "Episode 96  finished with cumulative reward: -2880500.0 and \n",
      "with an average reward of: -1151.7393042782887\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 237595\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1152.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 117           |\n",
      "|    time_elapsed         | 3442          |\n",
      "|    total_timesteps      | 239616        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0081643e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00307       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.8e+09       |\n",
      "|    n_updates            | 1160          |\n",
      "|    policy_gradient_loss | -1.69e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.99e+09      |\n",
      "-------------------------------------------\n",
      "Episode 97  finished with cumulative reward: -3212000.0 and \n",
      "with an average reward of: -1284.2862854858056\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 240096\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1284.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.76e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 118          |\n",
      "|    time_elapsed         | 3474         |\n",
      "|    total_timesteps      | 241664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.014023e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 6.44e-05     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.31e+09     |\n",
      "|    n_updates            | 1170         |\n",
      "|    policy_gradient_loss | -2.37e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.69e+09     |\n",
      "------------------------------------------\n",
      "Episode 98  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 242597\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.73e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 119          |\n",
      "|    time_elapsed         | 3505         |\n",
      "|    total_timesteps      | 243712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.717126e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000184     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.55e+09     |\n",
      "|    n_updates            | 1180         |\n",
      "|    policy_gradient_loss | -1.43e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.25e+09     |\n",
      "------------------------------------------\n",
      "Episode 99  finished with cumulative reward: -11397500.0 and \n",
      "with an average reward of: -4557.177129148341\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 245098\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4559.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.79e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 120          |\n",
      "|    time_elapsed         | 3536         |\n",
      "|    total_timesteps      | 245760       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.500624e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0017       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.74e+09     |\n",
      "|    n_updates            | 1190         |\n",
      "|    policy_gradient_loss | -1.79e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.43e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 100  finished with cumulative reward: -6246500.0 and \n",
      "with an average reward of: -2497.6009596161534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 247599\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2498.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 121           |\n",
      "|    time_elapsed         | 3566          |\n",
      "|    total_timesteps      | 247808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7148205e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00103       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.19e+10      |\n",
      "|    n_updates            | 1200          |\n",
      "|    policy_gradient_loss | -2.21e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.36e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 122           |\n",
      "|    time_elapsed         | 3588          |\n",
      "|    total_timesteps      | 249856        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1059456e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00131       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.16e+09      |\n",
      "|    n_updates            | 1210          |\n",
      "|    policy_gradient_loss | -1.36e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.25e+10      |\n",
      "-------------------------------------------\n",
      "Episode 101  finished with cumulative reward: -3875000.0 and \n",
      "with an average reward of: -1549.3802479008398\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 250100\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1550.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.78e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 123          |\n",
      "|    time_elapsed         | 3620         |\n",
      "|    total_timesteps      | 251904       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.415618e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00134      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.71e+09     |\n",
      "|    n_updates            | 1220         |\n",
      "|    policy_gradient_loss | -2.92e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.67e+09     |\n",
      "------------------------------------------\n",
      "Episode 102  finished with cumulative reward: -6501500.0 and \n",
      "with an average reward of: -2599.5601759296283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 252601\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2600.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.81e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 124           |\n",
      "|    time_elapsed         | 3651          |\n",
      "|    total_timesteps      | 253952        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000505      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.43e+09      |\n",
      "|    n_updates            | 1230          |\n",
      "|    policy_gradient_loss | -1.19e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.29e+10      |\n",
      "-------------------------------------------\n",
      "Episode 103  finished with cumulative reward: -3441500.0 and \n",
      "with an average reward of: -1376.0495801679328\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 255102\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1376.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.79e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 125           |\n",
      "|    time_elapsed         | 3682          |\n",
      "|    total_timesteps      | 256000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00105       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.56e+09      |\n",
      "|    n_updates            | 1240          |\n",
      "|    policy_gradient_loss | -1.76e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.46e+09      |\n",
      "-------------------------------------------\n",
      "Episode 104  finished with cumulative reward: -3645500.0 and \n",
      "with an average reward of: -1457.6169532187125\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 257603\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1458.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 126           |\n",
      "|    time_elapsed         | 3713          |\n",
      "|    total_timesteps      | 258048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4260877e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000153      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.29e+09      |\n",
      "|    n_updates            | 1250          |\n",
      "|    policy_gradient_loss | -1.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.37e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.71e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 127          |\n",
      "|    time_elapsed         | 3736         |\n",
      "|    total_timesteps      | 260096       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.837095e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00162      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.25e+08     |\n",
      "|    n_updates            | 1260         |\n",
      "|    policy_gradient_loss | -2.7e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.47e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 105  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 260104\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 128           |\n",
      "|    time_elapsed         | 3767          |\n",
      "|    total_timesteps      | 262144        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8044375e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000886      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.09e+09      |\n",
      "|    n_updates            | 1270          |\n",
      "|    policy_gradient_loss | -1.73e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.32e+10      |\n",
      "-------------------------------------------\n",
      "Episode 106  finished with cumulative reward: -4181000.0 and \n",
      "with an average reward of: -1671.7313074770093\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 262605\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1672.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.65e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 129           |\n",
      "|    time_elapsed         | 3798          |\n",
      "|    total_timesteps      | 264192        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0022        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+10      |\n",
      "|    n_updates            | 1280          |\n",
      "|    policy_gradient_loss | -1.03e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.16e+09      |\n",
      "-------------------------------------------\n",
      "Episode 107  finished with cumulative reward: -6782000.0 and \n",
      "with an average reward of: -2711.71531387445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 265106\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2712.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.61e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 130           |\n",
      "|    time_elapsed         | 3828          |\n",
      "|    total_timesteps      | 266240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0855729e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00139       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1e+10         |\n",
      "|    n_updates            | 1290          |\n",
      "|    policy_gradient_loss | -2.45e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.07e+10      |\n",
      "-------------------------------------------\n",
      "Episode 108  finished with cumulative reward: -9026000.0 and \n",
      "with an average reward of: -3608.9564174330267\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 267607\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3610.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 131          |\n",
      "|    time_elapsed         | 3860         |\n",
      "|    total_timesteps      | 268288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.240995e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00144      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.16e+10     |\n",
      "|    n_updates            | 1300         |\n",
      "|    policy_gradient_loss | -1.7e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.21e+10     |\n",
      "------------------------------------------\n",
      "Episode 109  finished with cumulative reward: -11117000.0 and \n",
      "with an average reward of: -4445.021991203518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 270108\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4446.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.68e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 132           |\n",
      "|    time_elapsed         | 3890          |\n",
      "|    total_timesteps      | 270336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0640665e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0013        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.93e+09      |\n",
      "|    n_updates            | 1310          |\n",
      "|    policy_gradient_loss | -2.01e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.1e+09       |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 133       |\n",
      "|    time_elapsed         | 3912      |\n",
      "|    total_timesteps      | 272384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00151   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.69e+09  |\n",
      "|    n_updates            | 1320      |\n",
      "|    policy_gradient_loss | -6.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 110  finished with cumulative reward: -7725500.0 and \n",
      "with an average reward of: -3088.9644142343063\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 272609\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3090.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 134       |\n",
      "|    time_elapsed         | 3944      |\n",
      "|    total_timesteps      | 274432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00126   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.81e+09  |\n",
      "|    n_updates            | 1330      |\n",
      "|    policy_gradient_loss | -7.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.69e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 111  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 275110\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.64e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 135           |\n",
      "|    time_elapsed         | 3975          |\n",
      "|    total_timesteps      | 276480        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9354047e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0014        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.83e+09      |\n",
      "|    n_updates            | 1340          |\n",
      "|    policy_gradient_loss | -3.18e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.45e+10      |\n",
      "-------------------------------------------\n",
      "Episode 112  finished with cumulative reward: -7011500.0 and \n",
      "with an average reward of: -2803.4786085565775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 277611\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2804.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.69e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 136           |\n",
      "|    time_elapsed         | 4005          |\n",
      "|    total_timesteps      | 278528        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0360964e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00122       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.82e+09      |\n",
      "|    n_updates            | 1350          |\n",
      "|    policy_gradient_loss | -2.84e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.4e+10       |\n",
      "-------------------------------------------\n",
      "Episode 113  finished with cumulative reward: -3212000.0 and \n",
      "with an average reward of: -1284.2862854858056\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 280112\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1284.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 137          |\n",
      "|    time_elapsed         | 4037         |\n",
      "|    total_timesteps      | 280576       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0025       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.64e+09     |\n",
      "|    n_updates            | 1360         |\n",
      "|    policy_gradient_loss | -9.86e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.36e+09     |\n",
      "------------------------------------------\n",
      "Episode 114  finished with cumulative reward: -6017000.0 and \n",
      "with an average reward of: -2405.837664934026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 282613\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2406.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 138          |\n",
      "|    time_elapsed         | 4066         |\n",
      "|    total_timesteps      | 282624       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00114      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+09     |\n",
      "|    n_updates            | 1370         |\n",
      "|    policy_gradient_loss | -2.31e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.91e+09     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 139       |\n",
      "|    time_elapsed         | 4089      |\n",
      "|    total_timesteps      | 284672    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0012    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.22e+10  |\n",
      "|    n_updates            | 1380      |\n",
      "|    policy_gradient_loss | -7.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.4e+10   |\n",
      "---------------------------------------\n",
      "Episode 115  finished with cumulative reward: -7241000.0 and \n",
      "with an average reward of: -2895.2419032387047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 285114\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2896.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.63e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 140           |\n",
      "|    time_elapsed         | 4120          |\n",
      "|    total_timesteps      | 286720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0011        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.93e+09      |\n",
      "|    n_updates            | 1390          |\n",
      "|    policy_gradient_loss | -1.05e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.37e+09      |\n",
      "-------------------------------------------\n",
      "Episode 116  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 287615\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 141          |\n",
      "|    time_elapsed         | 4151         |\n",
      "|    total_timesteps      | 288768       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.240995e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00188      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.06e+10     |\n",
      "|    n_updates            | 1400         |\n",
      "|    policy_gradient_loss | -1.68e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.82e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 117  finished with cumulative reward: -4563500.0 and \n",
      "with an average reward of: -1824.670131947221\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 290116\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1825.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 142          |\n",
      "|    time_elapsed         | 4183         |\n",
      "|    total_timesteps      | 290816       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000206     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.76e+09     |\n",
      "|    n_updates            | 1410         |\n",
      "|    policy_gradient_loss | -9.26e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.43e+10     |\n",
      "------------------------------------------\n",
      "Episode 118  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 292617\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.66e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 143           |\n",
      "|    time_elapsed         | 4213          |\n",
      "|    total_timesteps      | 292864        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.6379788e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00216       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.31e+09      |\n",
      "|    n_updates            | 1420          |\n",
      "|    policy_gradient_loss | -2.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.72e+09      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 144       |\n",
      "|    time_elapsed         | 4236      |\n",
      "|    total_timesteps      | 294912    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.000142  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.04e+09  |\n",
      "|    n_updates            | 1430      |\n",
      "|    policy_gradient_loss | -6.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 119  finished with cumulative reward: -8363000.0 and \n",
      "with an average reward of: -3343.862455017993\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 295118\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3345.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.7e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 145          |\n",
      "|    time_elapsed         | 4266         |\n",
      "|    total_timesteps      | 296960       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000892     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.39e+09     |\n",
      "|    n_updates            | 1440         |\n",
      "|    policy_gradient_loss | -8.46e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.74e+10     |\n",
      "------------------------------------------\n",
      "Episode 120  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 297619\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 146          |\n",
      "|    time_elapsed         | 4297         |\n",
      "|    total_timesteps      | 299008       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.561137e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000788     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+10     |\n",
      "|    n_updates            | 1450         |\n",
      "|    policy_gradient_loss | -2e-05       |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.7e+10      |\n",
      "------------------------------------------\n",
      "Episode 121  finished with cumulative reward: -4512500.0 and \n",
      "with an average reward of: -1804.2782886845262\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 300120\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1805.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 147          |\n",
      "|    time_elapsed         | 4327         |\n",
      "|    total_timesteps      | 301056       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00208      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.13e+09     |\n",
      "|    n_updates            | 1460         |\n",
      "|    policy_gradient_loss | -1.33e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.92e+09     |\n",
      "------------------------------------------\n",
      "Episode 122  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 302621\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.66e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 148           |\n",
      "|    time_elapsed         | 4359          |\n",
      "|    total_timesteps      | 303104        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5395926e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00111       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.43e+09      |\n",
      "|    n_updates            | 1470          |\n",
      "|    policy_gradient_loss | -2.51e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.93e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 123  finished with cumulative reward: -4589000.0 and \n",
      "with an average reward of: -1834.8660535785687\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 305122\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1835.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 149           |\n",
      "|    time_elapsed         | 4388          |\n",
      "|    total_timesteps      | 305152        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5588316e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00247       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.23e+09      |\n",
      "|    n_updates            | 1480          |\n",
      "|    policy_gradient_loss | -2.2e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.54e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 150           |\n",
      "|    time_elapsed         | 4411          |\n",
      "|    total_timesteps      | 307200        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8044375e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0017        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.48e+09      |\n",
      "|    n_updates            | 1490          |\n",
      "|    policy_gradient_loss | -1.81e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.41e+10      |\n",
      "-------------------------------------------\n",
      "Episode 124  finished with cumulative reward: -1401500.0 and \n",
      "with an average reward of: -560.3758496601359\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 307623\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -560.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.65e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 151           |\n",
      "|    time_elapsed         | 4442          |\n",
      "|    total_timesteps      | 309248        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0954758e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00381       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.05e+09      |\n",
      "|    n_updates            | 1500          |\n",
      "|    policy_gradient_loss | -2.85e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.99e+09      |\n",
      "-------------------------------------------\n",
      "Episode 125  finished with cumulative reward: -1452500.0 and \n",
      "with an average reward of: -580.7676929228309\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 310124\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -581.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.61e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 152          |\n",
      "|    time_elapsed         | 4473         |\n",
      "|    total_timesteps      | 311296       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00113      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.88e+09     |\n",
      "|    n_updates            | 1510         |\n",
      "|    policy_gradient_loss | -7.91e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.07e+09     |\n",
      "------------------------------------------\n",
      "Episode 126  finished with cumulative reward: -5838500.0 and \n",
      "with an average reward of: -2334.466213514594\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 312625\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2335.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.64e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 153           |\n",
      "|    time_elapsed         | 4503          |\n",
      "|    total_timesteps      | 313344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8230716e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00108       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.42e+08      |\n",
      "|    n_updates            | 1520          |\n",
      "|    policy_gradient_loss | -1.82e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.48e+09      |\n",
      "-------------------------------------------\n",
      "Episode 127  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 315126\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.67e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 154          |\n",
      "|    time_elapsed         | 4533         |\n",
      "|    total_timesteps      | 315392       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.783498e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000823     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.98e+09     |\n",
      "|    n_updates            | 1530         |\n",
      "|    policy_gradient_loss | -1.83e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.24e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 155           |\n",
      "|    time_elapsed         | 4556          |\n",
      "|    total_timesteps      | 317440        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.2759576e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00142       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.4e+09       |\n",
      "|    n_updates            | 1540          |\n",
      "|    policy_gradient_loss | -1.53e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.13e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 128  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 317627\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.68e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 156           |\n",
      "|    time_elapsed         | 4587          |\n",
      "|    total_timesteps      | 319488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00245       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.74e+09      |\n",
      "|    n_updates            | 1550          |\n",
      "|    policy_gradient_loss | -1.27e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.55e+09      |\n",
      "-------------------------------------------\n",
      "Episode 129  finished with cumulative reward: -9485000.0 and \n",
      "with an average reward of: -3792.483006797281\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 320128\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3794.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.71e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 157          |\n",
      "|    time_elapsed         | 4618         |\n",
      "|    total_timesteps      | 321536       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.170012e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00182      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.31e+10     |\n",
      "|    n_updates            | 1560         |\n",
      "|    policy_gradient_loss | -1.7e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.1e+10      |\n",
      "------------------------------------------\n",
      "Episode 130  finished with cumulative reward: -2829500.0 and \n",
      "with an average reward of: -1131.3474610155938\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 322629\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1131.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 158          |\n",
      "|    time_elapsed         | 4649         |\n",
      "|    total_timesteps      | 323584       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00277      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.73e+09     |\n",
      "|    n_updates            | 1570         |\n",
      "|    policy_gradient_loss | -8.9e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.11e+09     |\n",
      "------------------------------------------\n",
      "Episode 131  finished with cumulative reward: -1376000.0 and \n",
      "with an average reward of: -550.1799280287885\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 325130\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -550.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.6e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 159           |\n",
      "|    time_elapsed         | 4680          |\n",
      "|    total_timesteps      | 325632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3655746e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0006        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.78e+09      |\n",
      "|    n_updates            | 1580          |\n",
      "|    policy_gradient_loss | -1.82e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.46e+09      |\n",
      "-------------------------------------------\n",
      "Episode 132  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 327631\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.6e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 160           |\n",
      "|    time_elapsed         | 4710          |\n",
      "|    total_timesteps      | 327680        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00256       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.79e+09      |\n",
      "|    n_updates            | 1590          |\n",
      "|    policy_gradient_loss | -1.34e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.12e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.6e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 161          |\n",
      "|    time_elapsed         | 4733         |\n",
      "|    total_timesteps      | 329728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000177     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.95e+09     |\n",
      "|    n_updates            | 1600         |\n",
      "|    policy_gradient_loss | -1.31e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.51e+09     |\n",
      "------------------------------------------\n",
      "Episode 133  finished with cumulative reward: -2804000.0 and \n",
      "with an average reward of: -1121.1515393842462\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 330132\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1121.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.56e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 162          |\n",
      "|    time_elapsed         | 4764         |\n",
      "|    total_timesteps      | 331776       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00128      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.16e+09     |\n",
      "|    n_updates            | 1610         |\n",
      "|    policy_gradient_loss | -1.18e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.65e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 134  finished with cumulative reward: -9587000.0 and \n",
      "with an average reward of: -3833.266693322671\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 332633\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3834.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 163       |\n",
      "|    time_elapsed         | 4795      |\n",
      "|    total_timesteps      | 333824    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00104   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e+10  |\n",
      "|    n_updates            | 1620      |\n",
      "|    policy_gradient_loss | -5.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 135  finished with cumulative reward: -4334000.0 and \n",
      "with an average reward of: -1732.906837265094\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 335134\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1733.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.53e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 164           |\n",
      "|    time_elapsed         | 4826          |\n",
      "|    total_timesteps      | 335872        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0015        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.28e+09      |\n",
      "|    n_updates            | 1630          |\n",
      "|    policy_gradient_loss | -1.28e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.15e+10      |\n",
      "-------------------------------------------\n",
      "Episode 136  finished with cumulative reward: -101000.0 and \n",
      "with an average reward of: -40.38384646141543\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 337635\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -40.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.48e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 165           |\n",
      "|    time_elapsed         | 4856          |\n",
      "|    total_timesteps      | 337920        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0081643e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00327       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.8e+09       |\n",
      "|    n_updates            | 1640          |\n",
      "|    policy_gradient_loss | -1.79e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.61e+09      |\n",
      "-------------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -5.48e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 166         |\n",
      "|    time_elapsed         | 4879        |\n",
      "|    total_timesteps      | 339968      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 2.73576e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.00155     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.32e+09    |\n",
      "|    n_updates            | 1650        |\n",
      "|    policy_gradient_loss | -1.59e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 7.62e+08    |\n",
      "-----------------------------------------\n",
      "Episode 137  finished with cumulative reward: -7904000.0 and \n",
      "with an average reward of: -3160.3358656537384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 340136\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3161.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 167       |\n",
      "|    time_elapsed         | 4910      |\n",
      "|    total_timesteps      | 342016    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00111   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.33e+09  |\n",
      "|    n_updates            | 1660      |\n",
      "|    policy_gradient_loss | -4.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.64e+10  |\n",
      "---------------------------------------\n",
      "Episode 138  finished with cumulative reward: -2039000.0 and \n",
      "with an average reward of: -815.2738904438224\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 342637\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -815.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.45e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 168          |\n",
      "|    time_elapsed         | 4942         |\n",
      "|    total_timesteps      | 344064       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.167707e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0027       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.56e+08     |\n",
      "|    n_updates            | 1670         |\n",
      "|    policy_gradient_loss | -2.55e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.3e+09      |\n",
      "------------------------------------------\n",
      "Episode 139  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 345138\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.4e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 169           |\n",
      "|    time_elapsed         | 4973          |\n",
      "|    total_timesteps      | 346112        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00139       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.9e+09       |\n",
      "|    n_updates            | 1680          |\n",
      "|    policy_gradient_loss | -8.8e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.25e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 140  finished with cumulative reward: -8082500.0 and \n",
      "with an average reward of: -3231.7073170731705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 347639\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3233.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.43e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 170           |\n",
      "|    time_elapsed         | 5003          |\n",
      "|    total_timesteps      | 348160        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4260877e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000758      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.65e+08      |\n",
      "|    n_updates            | 1690          |\n",
      "|    policy_gradient_loss | -1.67e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.12e+09      |\n",
      "-------------------------------------------\n",
      "Episode 141  finished with cumulative reward: -13412000.0 and \n",
      "with an average reward of: -5362.65493802479\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 350140\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5364.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.54e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 171           |\n",
      "|    time_elapsed         | 5034          |\n",
      "|    total_timesteps      | 350208        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00189       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.37e+09      |\n",
      "|    n_updates            | 1700          |\n",
      "|    policy_gradient_loss | -1.1e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.43e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.54e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 172       |\n",
      "|    time_elapsed         | 5056      |\n",
      "|    total_timesteps      | 352256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00188   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.98e+10  |\n",
      "|    n_updates            | 1710      |\n",
      "|    policy_gradient_loss | -5.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 142  finished with cumulative reward: -3722000.0 and \n",
      "with an average reward of: -1488.2047181127548\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 352641\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1488.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.57e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 173          |\n",
      "|    time_elapsed         | 5087         |\n",
      "|    total_timesteps      | 354304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.812602e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00201      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.09e+09     |\n",
      "|    n_updates            | 1720         |\n",
      "|    policy_gradient_loss | -2.26e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.53e+09     |\n",
      "------------------------------------------\n",
      "Episode 143  finished with cumulative reward: -8286500.0 and \n",
      "with an average reward of: -3313.27469012395\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 355142\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3314.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.58e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 174           |\n",
      "|    time_elapsed         | 5118          |\n",
      "|    total_timesteps      | 356352        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00171       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.6e+09       |\n",
      "|    n_updates            | 1730          |\n",
      "|    policy_gradient_loss | -1e-05        |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.72e+09      |\n",
      "-------------------------------------------\n",
      "Episode 144  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 357643\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 175           |\n",
      "|    time_elapsed         | 5149          |\n",
      "|    total_timesteps      | 358400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4842954e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0015        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.31e+10      |\n",
      "|    n_updates            | 1740          |\n",
      "|    policy_gradient_loss | -1.74e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.19e+10      |\n",
      "-------------------------------------------\n",
      "Episode 145  finished with cumulative reward: -1962500.0 and \n",
      "with an average reward of: -784.6861255497801\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 360144\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -785.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 176           |\n",
      "|    time_elapsed         | 5180          |\n",
      "|    total_timesteps      | 360448        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0157237e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00134       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.08e+09      |\n",
      "|    n_updates            | 1750          |\n",
      "|    policy_gradient_loss | -2.53e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.2e+10       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 177          |\n",
      "|    time_elapsed         | 5203         |\n",
      "|    total_timesteps      | 362496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.240995e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00523      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+09     |\n",
      "|    n_updates            | 1760         |\n",
      "|    policy_gradient_loss | -1.45e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.43e+09     |\n",
      "------------------------------------------\n",
      "Episode 146  finished with cumulative reward: -6221000.0 and \n",
      "with an average reward of: -2487.405037984806\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 362645\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2488.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 178       |\n",
      "|    time_elapsed         | 5235      |\n",
      "|    total_timesteps      | 364544    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00153   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.85e+09  |\n",
      "|    n_updates            | 1770      |\n",
      "|    policy_gradient_loss | -6.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 147  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 365146\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.65e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 179           |\n",
      "|    time_elapsed         | 5266          |\n",
      "|    total_timesteps      | 366592        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1350494e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00213       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.19e+09      |\n",
      "|    n_updates            | 1780          |\n",
      "|    policy_gradient_loss | -1.48e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.52e+10      |\n",
      "-------------------------------------------\n",
      "Episode 148  finished with cumulative reward: -6476000.0 and \n",
      "with an average reward of: -2589.3642542982807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 367647\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2590.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.72e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 180          |\n",
      "|    time_elapsed         | 5296         |\n",
      "|    total_timesteps      | 368640       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.294592e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000932     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+10     |\n",
      "|    n_updates            | 1790         |\n",
      "|    policy_gradient_loss | -2.01e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.05e+10     |\n",
      "------------------------------------------\n",
      "Episode 149  finished with cumulative reward: -3722000.0 and \n",
      "with an average reward of: -1488.2047181127548\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 370148\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1488.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 181           |\n",
      "|    time_elapsed         | 5327          |\n",
      "|    total_timesteps      | 370688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00361       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.66e+09      |\n",
      "|    n_updates            | 1800          |\n",
      "|    policy_gradient_loss | -9.76e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.58e+09      |\n",
      "-------------------------------------------\n",
      "Episode 150  finished with cumulative reward: -5940500.0 and \n",
      "with an average reward of: -2375.249900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 372649\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2376.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 182           |\n",
      "|    time_elapsed         | 5357          |\n",
      "|    total_timesteps      | 372736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3178367e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0029        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.84e+09      |\n",
      "|    n_updates            | 1810          |\n",
      "|    policy_gradient_loss | -2.08e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.63e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 183           |\n",
      "|    time_elapsed         | 5379          |\n",
      "|    total_timesteps      | 374784        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4028427e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00262       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.4e+09       |\n",
      "|    n_updates            | 1820          |\n",
      "|    policy_gradient_loss | -1.25e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.04e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 151  finished with cumulative reward: -2013500.0 and \n",
      "with an average reward of: -805.077968812475\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 375150\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -805.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.63e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 184           |\n",
      "|    time_elapsed         | 5411          |\n",
      "|    total_timesteps      | 376832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00178       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.15e+08      |\n",
      "|    n_updates            | 1830          |\n",
      "|    policy_gradient_loss | -1.27e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.87e+09      |\n",
      "-------------------------------------------\n",
      "Episode 152  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 377651\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 185          |\n",
      "|    time_elapsed         | 5443         |\n",
      "|    total_timesteps      | 378880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.440111e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00209      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.02e+09     |\n",
      "|    n_updates            | 1840         |\n",
      "|    policy_gradient_loss | -1.37e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.6e+09      |\n",
      "------------------------------------------\n",
      "Episode 153  finished with cumulative reward: -7368500.0 and \n",
      "with an average reward of: -2946.2215113954417\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 380152\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2947.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.63e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 186           |\n",
      "|    time_elapsed         | 5475          |\n",
      "|    total_timesteps      | 380928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0646333e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00118       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.13e+09      |\n",
      "|    n_updates            | 1850          |\n",
      "|    policy_gradient_loss | -3.53e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.2e+10       |\n",
      "-------------------------------------------\n",
      "Episode 154  finished with cumulative reward: -4920500.0 and \n",
      "with an average reward of: -1967.4130347860855\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 382653\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1968.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 187           |\n",
      "|    time_elapsed         | 5506          |\n",
      "|    total_timesteps      | 382976        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00309       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.84e+09      |\n",
      "|    n_updates            | 1860          |\n",
      "|    policy_gradient_loss | -1.08e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.32e+09      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 188          |\n",
      "|    time_elapsed         | 5529         |\n",
      "|    total_timesteps      | 385024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.783498e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00266      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.57e+09     |\n",
      "|    n_updates            | 1870         |\n",
      "|    policy_gradient_loss | -9.22e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.26e+10     |\n",
      "------------------------------------------\n",
      "Episode 155  finished with cumulative reward: -7445000.0 and \n",
      "with an average reward of: -2976.8092762894844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 385154\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2978.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.6e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 69       |\n",
      "|    iterations           | 189      |\n",
      "|    time_elapsed         | 5560     |\n",
      "|    total_timesteps      | 387072   |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.00236  |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 8.06e+09 |\n",
      "|    n_updates            | 1880     |\n",
      "|    policy_gradient_loss | -9.6e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.61e+10 |\n",
      "--------------------------------------\n",
      "Episode 156  finished with cumulative reward: -10250000.0 and \n",
      "with an average reward of: -4098.360655737705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 387655\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4100.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.64e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 190           |\n",
      "|    time_elapsed         | 5591          |\n",
      "|    total_timesteps      | 389120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0745363e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00325       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.24e+10      |\n",
      "|    n_updates            | 1890          |\n",
      "|    policy_gradient_loss | -1.21e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.69e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 157  finished with cumulative reward: -8516000.0 and \n",
      "with an average reward of: -3405.0379848060775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 390156\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3406.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 191          |\n",
      "|    time_elapsed         | 5623         |\n",
      "|    total_timesteps      | 391168       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.085006e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00124      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+10     |\n",
      "|    n_updates            | 1900         |\n",
      "|    policy_gradient_loss | -1.74e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.22e+10     |\n",
      "------------------------------------------\n",
      "Episode 158  finished with cumulative reward: -10862000.0 and \n",
      "with an average reward of: -4343.062774890044\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 392657\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4344.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 192          |\n",
      "|    time_elapsed         | 5655         |\n",
      "|    total_timesteps      | 393216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.683411e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00274      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.8e+09      |\n",
      "|    n_updates            | 1910         |\n",
      "|    policy_gradient_loss | -2.39e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.61e+09     |\n",
      "------------------------------------------\n",
      "Episode 159  finished with cumulative reward: -5073500.0 and \n",
      "with an average reward of: -2028.5885645741703\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 395158\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2029.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 193          |\n",
      "|    time_elapsed         | 5685         |\n",
      "|    total_timesteps      | 395264       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.783498e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00191      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.46e+10     |\n",
      "|    n_updates            | 1920         |\n",
      "|    policy_gradient_loss | -9.95e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.64e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 194          |\n",
      "|    time_elapsed         | 5708         |\n",
      "|    total_timesteps      | 397312       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00278      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.12e+09     |\n",
      "|    n_updates            | 1930         |\n",
      "|    policy_gradient_loss | -7.45e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.2e+10      |\n",
      "------------------------------------------\n",
      "Episode 160  finished with cumulative reward: -4538000.0 and \n",
      "with an average reward of: -1814.4742103158737\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 397659\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1815.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.57e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 195           |\n",
      "|    time_elapsed         | 5739          |\n",
      "|    total_timesteps      | 399360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0768417e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000384      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.83e+09      |\n",
      "|    n_updates            | 1940          |\n",
      "|    policy_gradient_loss | -1.2e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.81e+09      |\n",
      "-------------------------------------------\n",
      "Episode 161  finished with cumulative reward: -3645500.0 and \n",
      "with an average reward of: -1457.6169532187125\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 400160\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1458.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.58e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 196           |\n",
      "|    time_elapsed         | 5771          |\n",
      "|    total_timesteps      | 401408        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.0267984e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000326      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.5e+09       |\n",
      "|    n_updates            | 1950          |\n",
      "|    policy_gradient_loss | -1.85e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4e+09         |\n",
      "-------------------------------------------\n",
      "Episode 162  finished with cumulative reward: -3798500.0 and \n",
      "with an average reward of: -1518.7924830067973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 402661\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1519.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.61e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 197           |\n",
      "|    time_elapsed         | 5801          |\n",
      "|    total_timesteps      | 403456        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00137       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.56e+09      |\n",
      "|    n_updates            | 1960          |\n",
      "|    policy_gradient_loss | -1.09e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.11e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 163  finished with cumulative reward: -11984000.0 and \n",
      "with an average reward of: -4791.683326669332\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 405162\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4793.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -5.68e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 198         |\n",
      "|    time_elapsed         | 5832        |\n",
      "|    total_timesteps      | 405504      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.36788e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.00187     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 8.18e+09    |\n",
      "|    n_updates            | 1970        |\n",
      "|    policy_gradient_loss | -1.51e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 1.44e+10    |\n",
      "-----------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 199       |\n",
      "|    time_elapsed         | 5854      |\n",
      "|    total_timesteps      | 407552    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00134   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.23e+10  |\n",
      "|    n_updates            | 1980      |\n",
      "|    policy_gradient_loss | -4.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 164  finished with cumulative reward: -4920500.0 and \n",
      "with an average reward of: -1967.4130347860855\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 407663\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1968.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 200           |\n",
      "|    time_elapsed         | 5885          |\n",
      "|    total_timesteps      | 409600        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.1536835e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00168       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.08e+09      |\n",
      "|    n_updates            | 1990          |\n",
      "|    policy_gradient_loss | -1.51e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.54e+09      |\n",
      "-------------------------------------------\n",
      "Episode 165  finished with cumulative reward: -2906000.0 and \n",
      "with an average reward of: -1161.935225909636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 410164\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1162.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -5.72e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 201         |\n",
      "|    time_elapsed         | 5916        |\n",
      "|    total_timesteps      | 411648      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 4.10364e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | -0.00037    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.77e+09    |\n",
      "|    n_updates            | 2000        |\n",
      "|    policy_gradient_loss | -1.87e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 4.23e+09    |\n",
      "-----------------------------------------\n",
      "Episode 166  finished with cumulative reward: -6680000.0 and \n",
      "with an average reward of: -2670.9316273490604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 412665\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2672.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.72e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 202           |\n",
      "|    time_elapsed         | 5947          |\n",
      "|    total_timesteps      | 413696        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00113       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.16e+09      |\n",
      "|    n_updates            | 2010          |\n",
      "|    policy_gradient_loss | -8.33e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.19e+10      |\n",
      "-------------------------------------------\n",
      "Episode 167  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 415166\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.74e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 203          |\n",
      "|    time_elapsed         | 5978         |\n",
      "|    total_timesteps      | 415744       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.812602e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00296      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.67e+09     |\n",
      "|    n_updates            | 2020         |\n",
      "|    policy_gradient_loss | -2.07e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.87e+09     |\n",
      "------------------------------------------\n",
      "Episode 168  finished with cumulative reward: -7215500.0 and \n",
      "with an average reward of: -2885.045981607357\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 417667\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2886.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.76e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 204       |\n",
      "|    time_elapsed         | 6008      |\n",
      "|    total_timesteps      | 417792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00197   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28e+10  |\n",
      "|    n_updates            | 2030      |\n",
      "|    policy_gradient_loss | -3.58e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.68e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.76e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 205           |\n",
      "|    time_elapsed         | 6030          |\n",
      "|    total_timesteps      | 419840        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.7148205e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00313       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.69e+09      |\n",
      "|    n_updates            | 2040          |\n",
      "|    policy_gradient_loss | -2.12e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.23e+09      |\n",
      "-------------------------------------------\n",
      "Episode 169  finished with cumulative reward: -6374000.0 and \n",
      "with an average reward of: -2548.580567772891\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 420168\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2549.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.76e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 206           |\n",
      "|    time_elapsed         | 6061          |\n",
      "|    total_timesteps      | 421888        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00316       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.73e+09      |\n",
      "|    n_updates            | 2050          |\n",
      "|    policy_gradient_loss | -1.18e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.25e+10      |\n",
      "-------------------------------------------\n",
      "Episode 170  finished with cumulative reward: -2345000.0 and \n",
      "with an average reward of: -937.624950019992\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 422669\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -938.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.7e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 207          |\n",
      "|    time_elapsed         | 6092         |\n",
      "|    total_timesteps      | 423936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000901     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.91e+08     |\n",
      "|    n_updates            | 2060         |\n",
      "|    policy_gradient_loss | -9.6e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.31e+09     |\n",
      "------------------------------------------\n",
      "Episode 171  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 425170\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 208          |\n",
      "|    time_elapsed         | 6124         |\n",
      "|    total_timesteps      | 425984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000781     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.15e+09     |\n",
      "|    n_updates            | 2070         |\n",
      "|    policy_gradient_loss | -5.64e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.66e+09     |\n",
      "------------------------------------------\n",
      "Episode 172  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 427671\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 209       |\n",
      "|    time_elapsed         | 6155      |\n",
      "|    total_timesteps      | 428032    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00211   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+09  |\n",
      "|    n_updates            | 2080      |\n",
      "|    policy_gradient_loss | -6.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.69e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 210           |\n",
      "|    time_elapsed         | 6178          |\n",
      "|    total_timesteps      | 430080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.2118911e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00272       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.7e+09       |\n",
      "|    n_updates            | 2090          |\n",
      "|    policy_gradient_loss | -1.72e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.98e+09      |\n",
      "-------------------------------------------\n",
      "Episode 173  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 430172\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 211          |\n",
      "|    time_elapsed         | 6209         |\n",
      "|    total_timesteps      | 432128       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00158      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49e+10     |\n",
      "|    n_updates            | 2100         |\n",
      "|    policy_gradient_loss | -6.59e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.94e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 174  finished with cumulative reward: 358000.0 and \n",
      "with an average reward of: 143.14274290283888\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 432673\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: 143.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 212           |\n",
      "|    time_elapsed         | 6240          |\n",
      "|    total_timesteps      | 434176        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7625584e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | -0.617        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.33e+07      |\n",
      "|    n_updates            | 2110          |\n",
      "|    policy_gradient_loss | -2.32e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.87e+07      |\n",
      "-------------------------------------------\n",
      "Episode 175  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 435174\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 213          |\n",
      "|    time_elapsed         | 6272         |\n",
      "|    total_timesteps      | 436224       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00113      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.12e+09     |\n",
      "|    n_updates            | 2120         |\n",
      "|    policy_gradient_loss | -7.66e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.12e+09     |\n",
      "------------------------------------------\n",
      "Episode 176  finished with cumulative reward: -8388500.0 and \n",
      "with an average reward of: -3354.05837664934\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 437675\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3355.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 214       |\n",
      "|    time_elapsed         | 6303      |\n",
      "|    total_timesteps      | 438272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.000685  |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.37e+09  |\n",
      "|    n_updates            | 2130      |\n",
      "|    policy_gradient_loss | -4.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 177  finished with cumulative reward: -8082500.0 and \n",
      "with an average reward of: -3231.7073170731705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 440176\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3233.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.75e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 215           |\n",
      "|    time_elapsed         | 6333          |\n",
      "|    total_timesteps      | 440320        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2282197e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00307       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.02e+09      |\n",
      "|    n_updates            | 2140          |\n",
      "|    policy_gradient_loss | -2.11e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.03e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.75e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 216          |\n",
      "|    time_elapsed         | 6356         |\n",
      "|    total_timesteps      | 442368       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00132      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.84e+09     |\n",
      "|    n_updates            | 2150         |\n",
      "|    policy_gradient_loss | -1.05e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.86e+10     |\n",
      "------------------------------------------\n",
      "Episode 178  finished with cumulative reward: -9434000.0 and \n",
      "with an average reward of: -3772.091163534586\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 442677\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3773.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.8e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 217           |\n",
      "|    time_elapsed         | 6387          |\n",
      "|    total_timesteps      | 444416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00107       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.52e+10      |\n",
      "|    n_updates            | 2160          |\n",
      "|    policy_gradient_loss | -1.21e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.84e+10      |\n",
      "-------------------------------------------\n",
      "Episode 179  finished with cumulative reward: -9663500.0 and \n",
      "with an average reward of: -3863.8544582167133\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 445178\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3865.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.82e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 218           |\n",
      "|    time_elapsed         | 6418          |\n",
      "|    total_timesteps      | 446464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3096724e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00289       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.52e+09      |\n",
      "|    n_updates            | 2170          |\n",
      "|    policy_gradient_loss | -1.41e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.96e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 180  finished with cumulative reward: -4461500.0 and \n",
      "with an average reward of: -1783.8864454218312\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 447679\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1784.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.82e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 219          |\n",
      "|    time_elapsed         | 6450         |\n",
      "|    total_timesteps      | 448512       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.625204e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00432      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.26e+09     |\n",
      "|    n_updates            | 2180         |\n",
      "|    policy_gradient_loss | -2.53e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.13e+09     |\n",
      "------------------------------------------\n",
      "Episode 181  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 450180\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.88e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 220           |\n",
      "|    time_elapsed         | 6481          |\n",
      "|    total_timesteps      | 450560        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0768417e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000912      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.01e+09      |\n",
      "|    n_updates            | 2190          |\n",
      "|    policy_gradient_loss | -1.11e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.23e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.88e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 221           |\n",
      "|    time_elapsed         | 6503          |\n",
      "|    total_timesteps      | 452608        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.6385457e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.000924      |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.42e+09      |\n",
      "|    n_updates            | 2200          |\n",
      "|    policy_gradient_loss | -2.33e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.22e+09      |\n",
      "-------------------------------------------\n",
      "Episode 182  finished with cumulative reward: -9306500.0 and \n",
      "with an average reward of: -3721.111555377849\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 452681\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3722.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.87e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 222          |\n",
      "|    time_elapsed         | 6534         |\n",
      "|    total_timesteps      | 454656       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000771     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.84e+10     |\n",
      "|    n_updates            | 2210         |\n",
      "|    policy_gradient_loss | -7.4e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.56e+10     |\n",
      "------------------------------------------\n",
      "Episode 183  finished with cumulative reward: -1682000.0 and \n",
      "with an average reward of: -672.530987604958\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 455182\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -672.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.82e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 223           |\n",
      "|    time_elapsed         | 6565          |\n",
      "|    total_timesteps      | 456704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4842954e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00182       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.45e+09      |\n",
      "|    n_updates            | 2220          |\n",
      "|    policy_gradient_loss | -1.46e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.91e+09      |\n",
      "-------------------------------------------\n",
      "Episode 184  finished with cumulative reward: -6501500.0 and \n",
      "with an average reward of: -2599.5601759296283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 457683\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2600.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.8e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 224          |\n",
      "|    time_elapsed         | 6597         |\n",
      "|    total_timesteps      | 458752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.440111e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00151      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.7e+09      |\n",
      "|    n_updates            | 2230         |\n",
      "|    policy_gradient_loss | -1.27e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.38e+10     |\n",
      "------------------------------------------\n",
      "Episode 185  finished with cumulative reward: -8108000.0 and \n",
      "with an average reward of: -3241.903238704518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 460184\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3243.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.84e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 225          |\n",
      "|    time_elapsed         | 6628         |\n",
      "|    total_timesteps      | 460800       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.891749e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00249      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.37e+09     |\n",
      "|    n_updates            | 2240         |\n",
      "|    policy_gradient_loss | -1.75e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.56e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 186  finished with cumulative reward: -5048000.0 and \n",
      "with an average reward of: -2018.392642942823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 462685\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2019.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.8e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 226           |\n",
      "|    time_elapsed         | 6658          |\n",
      "|    total_timesteps      | 462848        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00221       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.5e+09       |\n",
      "|    n_updates            | 2250          |\n",
      "|    policy_gradient_loss | -8.1e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.55e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.8e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 227          |\n",
      "|    time_elapsed         | 6681         |\n",
      "|    total_timesteps      | 464896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00161      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.01e+09     |\n",
      "|    n_updates            | 2260         |\n",
      "|    policy_gradient_loss | -1.32e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.98e+09     |\n",
      "------------------------------------------\n",
      "Episode 187  finished with cumulative reward: -2906000.0 and \n",
      "with an average reward of: -1161.935225909636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 465186\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1162.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.73e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 228           |\n",
      "|    time_elapsed         | 6712          |\n",
      "|    total_timesteps      | 466944        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00142       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.08e+09      |\n",
      "|    n_updates            | 2270          |\n",
      "|    policy_gradient_loss | -1.22e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.38e+09      |\n",
      "-------------------------------------------\n",
      "Episode 188  finished with cumulative reward: -5430500.0 and \n",
      "with an average reward of: -2171.3314674130347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 467687\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2172.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.74e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 229           |\n",
      "|    time_elapsed         | 6744          |\n",
      "|    total_timesteps      | 468992        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3096724e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00334       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.38e+09      |\n",
      "|    n_updates            | 2280          |\n",
      "|    policy_gradient_loss | -1.57e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.69e+09      |\n",
      "-------------------------------------------\n",
      "Episode 189  finished with cumulative reward: -7827500.0 and \n",
      "with an average reward of: -3129.748100759696\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 470188\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3131.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 230       |\n",
      "|    time_elapsed         | 6777      |\n",
      "|    total_timesteps      | 471040    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00248   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.91e+09  |\n",
      "|    n_updates            | 2290      |\n",
      "|    policy_gradient_loss | -6.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 190  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 472689\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.7e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 231          |\n",
      "|    time_elapsed         | 6810         |\n",
      "|    total_timesteps      | 473088       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.259629e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00119      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.24e+09     |\n",
      "|    n_updates            | 2300         |\n",
      "|    policy_gradient_loss | -1.71e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.53e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.7e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 232          |\n",
      "|    time_elapsed         | 6836         |\n",
      "|    total_timesteps      | 475136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.529728e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00272      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.33e+09     |\n",
      "|    n_updates            | 2310         |\n",
      "|    policy_gradient_loss | -1.21e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.21e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 191  finished with cumulative reward: -7496000.0 and \n",
      "with an average reward of: -2997.201119552179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 475190\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2998.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.75e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 233           |\n",
      "|    time_elapsed         | 6867          |\n",
      "|    total_timesteps      | 477184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00207       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.64e+09      |\n",
      "|    n_updates            | 2320          |\n",
      "|    policy_gradient_loss | -7.66e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.99e+10      |\n",
      "-------------------------------------------\n",
      "Episode 192  finished with cumulative reward: -16497500.0 and \n",
      "with an average reward of: -6596.361455417833\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 477691\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -6599.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.86e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 234          |\n",
      "|    time_elapsed         | 6899         |\n",
      "|    total_timesteps      | 479232       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00299      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.53e+10     |\n",
      "|    n_updates            | 2330         |\n",
      "|    policy_gradient_loss | -6.7e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.54e+10     |\n",
      "------------------------------------------\n",
      "Episode 193  finished with cumulative reward: -5966000.0 and \n",
      "with an average reward of: -2385.4458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 480192\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2386.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 235          |\n",
      "|    time_elapsed         | 6931         |\n",
      "|    total_timesteps      | 481280       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.905772e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00216      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.24e+10     |\n",
      "|    n_updates            | 2340         |\n",
      "|    policy_gradient_loss | -2.09e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.08e+10     |\n",
      "------------------------------------------\n",
      "Episode 194  finished with cumulative reward: -4436000.0 and \n",
      "with an average reward of: -1773.690523790484\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 482693\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1774.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -5.86e+06   |\n",
      "| time/                   |             |\n",
      "|    fps                  | 69          |\n",
      "|    iterations           | 236         |\n",
      "|    time_elapsed         | 6962        |\n",
      "|    total_timesteps      | 483328      |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 8.58563e-09 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.00311     |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 6.86e+09    |\n",
      "|    n_updates            | 2350        |\n",
      "|    policy_gradient_loss | -2.48e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 9.11e+09    |\n",
      "-----------------------------------------\n",
      "Episode 195  finished with cumulative reward: -7292000.0 and \n",
      "with an average reward of: -2915.6337465013994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 485194\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2916.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.88e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 237           |\n",
      "|    time_elapsed         | 6994          |\n",
      "|    total_timesteps      | 485376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7753337e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00337       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.77e+09      |\n",
      "|    n_updates            | 2360          |\n",
      "|    policy_gradient_loss | -1.6e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.76e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 238          |\n",
      "|    time_elapsed         | 7018         |\n",
      "|    total_timesteps      | 487424       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.783498e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0023       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.82e+09     |\n",
      "|    n_updates            | 2370         |\n",
      "|    policy_gradient_loss | -9.53e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.46e+10     |\n",
      "------------------------------------------\n",
      "Episode 196  finished with cumulative reward: -2447000.0 and \n",
      "with an average reward of: -978.4086365453818\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 487695\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -978.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.87e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 239          |\n",
      "|    time_elapsed         | 7048         |\n",
      "|    total_timesteps      | 489472       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.240995e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.005        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.51e+09     |\n",
      "|    n_updates            | 2380         |\n",
      "|    policy_gradient_loss | -1.69e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.56e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 197  finished with cumulative reward: -7929500.0 and \n",
      "with an average reward of: -3170.531787285086\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 490196\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3171.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.92e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 240           |\n",
      "|    time_elapsed         | 7080          |\n",
      "|    total_timesteps      | 491520        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00187       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.54e+09      |\n",
      "|    n_updates            | 2390          |\n",
      "|    policy_gradient_loss | -1.01e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.23e+09      |\n",
      "-------------------------------------------\n",
      "Episode 198  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 492697\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 241       |\n",
      "|    time_elapsed         | 7112      |\n",
      "|    total_timesteps      | 493568    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0012    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.13e+09  |\n",
      "|    n_updates            | 2400      |\n",
      "|    policy_gradient_loss | -4.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 199  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 495198\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.91e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 242           |\n",
      "|    time_elapsed         | 7144          |\n",
      "|    total_timesteps      | 495616        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0745363e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00289       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.81e+09      |\n",
      "|    n_updates            | 2410          |\n",
      "|    policy_gradient_loss | -1.26e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.08e+09      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.91e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 243          |\n",
      "|    time_elapsed         | 7169         |\n",
      "|    total_timesteps      | 497664       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00221      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.8e+09      |\n",
      "|    n_updates            | 2420         |\n",
      "|    policy_gradient_loss | -8.43e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.94e+10     |\n",
      "------------------------------------------\n",
      "Episode 200  finished with cumulative reward: -10275500.0 and \n",
      "with an average reward of: -4108.556577369052\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 497699\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4110.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.95e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 244          |\n",
      "|    time_elapsed         | 7200         |\n",
      "|    total_timesteps      | 499712       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.313226e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00193      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.3e+10      |\n",
      "|    n_updates            | 2430         |\n",
      "|    policy_gradient_loss | -1.14e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.04e+10     |\n",
      "------------------------------------------\n",
      "Episode 201  finished with cumulative reward: -4461500.0 and \n",
      "with an average reward of: -1783.8864454218312\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 500200\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1784.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.96e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 245           |\n",
      "|    time_elapsed         | 7230          |\n",
      "|    total_timesteps      | 501760        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5716068e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00209       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.69e+09      |\n",
      "|    n_updates            | 2440          |\n",
      "|    policy_gradient_loss | -1.25e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.14e+10      |\n",
      "-------------------------------------------\n",
      "Episode 202  finished with cumulative reward: -7853000.0 and \n",
      "with an average reward of: -3139.9440223910437\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 502701\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3141.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.97e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 246           |\n",
      "|    time_elapsed         | 7261          |\n",
      "|    total_timesteps      | 503808        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.2282197e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00261       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.99e+09      |\n",
      "|    n_updates            | 2450          |\n",
      "|    policy_gradient_loss | -2.16e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.62e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 203  finished with cumulative reward: -11525000.0 and \n",
      "with an average reward of: -4608.156737305078\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 505202\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4610.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.05e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 247           |\n",
      "|    time_elapsed         | 7292          |\n",
      "|    total_timesteps      | 505856        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00548       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.71e+09      |\n",
      "|    n_updates            | 2460          |\n",
      "|    policy_gradient_loss | -1.23e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.1e+10       |\n",
      "-------------------------------------------\n",
      "Episode 204  finished with cumulative reward: -1070000.0 and \n",
      "with an average reward of: -427.82886845261896\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 507703\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -428.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.02e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 248           |\n",
      "|    time_elapsed         | 7323          |\n",
      "|    total_timesteps      | 507904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00278       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.6e+10       |\n",
      "|    n_updates            | 2470          |\n",
      "|    policy_gradient_loss | -9.83e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.54e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.02e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 249          |\n",
      "|    time_elapsed         | 7348         |\n",
      "|    total_timesteps      | 509952       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00142      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+09     |\n",
      "|    n_updates            | 2480         |\n",
      "|    policy_gradient_loss | -9.69e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.27e+09     |\n",
      "------------------------------------------\n",
      "Episode 205  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 510204\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.01e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 250           |\n",
      "|    time_elapsed         | 7380          |\n",
      "|    total_timesteps      | 512000        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3655746e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00169       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+10      |\n",
      "|    n_updates            | 2490          |\n",
      "|    policy_gradient_loss | -1.21e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.98e+09      |\n",
      "-------------------------------------------\n",
      "Episode 206  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 512705\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.03e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 251           |\n",
      "|    time_elapsed         | 7411          |\n",
      "|    total_timesteps      | 514048        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00253       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.63e+09      |\n",
      "|    n_updates            | 2500          |\n",
      "|    policy_gradient_loss | -9.49e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.32e+10      |\n",
      "-------------------------------------------\n",
      "Episode 207  finished with cumulative reward: -11423000.0 and \n",
      "with an average reward of: -4567.373050779688\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 515206\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4569.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.08e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 252           |\n",
      "|    time_elapsed         | 7442          |\n",
      "|    total_timesteps      | 516096        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00168       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.3e+09       |\n",
      "|    n_updates            | 2510          |\n",
      "|    policy_gradient_loss | -1.01e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.26e+10      |\n",
      "-------------------------------------------\n",
      "Episode 208  finished with cumulative reward: -5991500.0 and \n",
      "with an average reward of: -2395.641743302679\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 517707\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2396.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 253       |\n",
      "|    time_elapsed         | 7475      |\n",
      "|    total_timesteps      | 518144    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00188   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.81e+09  |\n",
      "|    n_updates            | 2520      |\n",
      "|    policy_gradient_loss | -5.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.25e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.05e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 254           |\n",
      "|    time_elapsed         | 7498          |\n",
      "|    total_timesteps      | 520192        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.8504368e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00267       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.45e+09      |\n",
      "|    n_updates            | 2530          |\n",
      "|    policy_gradient_loss | -3.64e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.52e+10      |\n",
      "-------------------------------------------\n",
      "Episode 209  finished with cumulative reward: -2753000.0 and \n",
      "with an average reward of: -1100.7596961215513\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 520208\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1101.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.97e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 255           |\n",
      "|    time_elapsed         | 7529          |\n",
      "|    total_timesteps      | 522240        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6484486e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00355       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.31e+09      |\n",
      "|    n_updates            | 2540          |\n",
      "|    policy_gradient_loss | -1.58e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.81e+09      |\n",
      "-------------------------------------------\n",
      "Episode 210  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 522709\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.92e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 256          |\n",
      "|    time_elapsed         | 7561         |\n",
      "|    total_timesteps      | 524288       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.764864e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0032       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.14e+09     |\n",
      "|    n_updates            | 2550         |\n",
      "|    policy_gradient_loss | -1.58e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.34e+09     |\n",
      "------------------------------------------\n",
      "Episode 211  finished with cumulative reward: -12315500.0 and \n",
      "with an average reward of: -4924.230307876849\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 525210\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4926.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.99e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 257           |\n",
      "|    time_elapsed         | 7592          |\n",
      "|    total_timesteps      | 526336        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00384       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.95e+10      |\n",
      "|    n_updates            | 2560          |\n",
      "|    policy_gradient_loss | -1.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.78e+10      |\n",
      "-------------------------------------------\n",
      "Episode 212  finished with cumulative reward: -8873000.0 and \n",
      "with an average reward of: -3547.780887644942\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 527711\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3549.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 258       |\n",
      "|    time_elapsed         | 7622      |\n",
      "|    total_timesteps      | 528384    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00225   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.71e+09  |\n",
      "|    n_updates            | 2570      |\n",
      "|    policy_gradient_loss | -6.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.4e+10   |\n",
      "---------------------------------------\n",
      "Episode 213  finished with cumulative reward: -10760000.0 and \n",
      "with an average reward of: -4302.279088364654\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 530212\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4304.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 259          |\n",
      "|    time_elapsed         | 7653         |\n",
      "|    total_timesteps      | 530432       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.000628     |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.18e+09     |\n",
      "|    n_updates            | 2580         |\n",
      "|    policy_gradient_loss | -6.92e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.09e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 260          |\n",
      "|    time_elapsed         | 7676         |\n",
      "|    total_timesteps      | 532480       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00218      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.66e+10     |\n",
      "|    n_updates            | 2590         |\n",
      "|    policy_gradient_loss | -1.11e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.65e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 214  finished with cumulative reward: -10224500.0 and \n",
      "with an average reward of: -4088.1647341063576\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 532713\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4089.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.12e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 261          |\n",
      "|    time_elapsed         | 7708         |\n",
      "|    total_timesteps      | 534528       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00333      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.84e+09     |\n",
      "|    n_updates            | 2600         |\n",
      "|    policy_gradient_loss | -9.81e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.47e+10     |\n",
      "------------------------------------------\n",
      "Episode 215  finished with cumulative reward: -13641500.0 and \n",
      "with an average reward of: -5454.4182327069175\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 535214\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5456.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.19e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 262           |\n",
      "|    time_elapsed         | 7739          |\n",
      "|    total_timesteps      | 536576        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00151       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.84e+10      |\n",
      "|    n_updates            | 2610          |\n",
      "|    policy_gradient_loss | -9.42e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.79e+10      |\n",
      "-------------------------------------------\n",
      "Episode 216  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 537715\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.19e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 263           |\n",
      "|    time_elapsed         | 7770          |\n",
      "|    total_timesteps      | 538624        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00407       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+10      |\n",
      "|    n_updates            | 2620          |\n",
      "|    policy_gradient_loss | -1.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.83e+10      |\n",
      "-------------------------------------------\n",
      "Episode 217  finished with cumulative reward: -7547000.0 and \n",
      "with an average reward of: -3017.592962814874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 540216\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3018.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.22e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 264           |\n",
      "|    time_elapsed         | 7801          |\n",
      "|    total_timesteps      | 540672        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.9394869e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00301       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.92e+09      |\n",
      "|    n_updates            | 2630          |\n",
      "|    policy_gradient_loss | -1.59e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.73e+09      |\n",
      "-------------------------------------------\n",
      "Episode 218  finished with cumulative reward: -9306500.0 and \n",
      "with an average reward of: -3721.111555377849\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 542717\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3722.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.25e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 265           |\n",
      "|    time_elapsed         | 7831          |\n",
      "|    total_timesteps      | 542720        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00268       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.74e+09      |\n",
      "|    n_updates            | 2640          |\n",
      "|    policy_gradient_loss | -7.84e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.61e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.25e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 266           |\n",
      "|    time_elapsed         | 7853          |\n",
      "|    total_timesteps      | 544768        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00318       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.09e+09      |\n",
      "|    n_updates            | 2650          |\n",
      "|    policy_gradient_loss | -1.07e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.93e+10      |\n",
      "-------------------------------------------\n",
      "Episode 219  finished with cumulative reward: -4359500.0 and \n",
      "with an average reward of: -1743.1027588964414\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 545218\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1743.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.21e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 267           |\n",
      "|    time_elapsed         | 7884          |\n",
      "|    total_timesteps      | 546816        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00172       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.82e+09      |\n",
      "|    n_updates            | 2660          |\n",
      "|    policy_gradient_loss | -1.05e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.32e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 220  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 547719\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 268       |\n",
      "|    time_elapsed         | 7915      |\n",
      "|    total_timesteps      | 548864    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00358   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.27e+09  |\n",
      "|    n_updates            | 2670      |\n",
      "|    policy_gradient_loss | -9.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.81e+10  |\n",
      "---------------------------------------\n",
      "Episode 221  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 550220\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.23e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 269           |\n",
      "|    time_elapsed         | 7947          |\n",
      "|    total_timesteps      | 550912        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00382       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.91e+09      |\n",
      "|    n_updates            | 2680          |\n",
      "|    policy_gradient_loss | -1.05e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.81e+09      |\n",
      "-------------------------------------------\n",
      "Episode 222  finished with cumulative reward: -1427000.0 and \n",
      "with an average reward of: -570.5717712914834\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 552721\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -570.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.18e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 270           |\n",
      "|    time_elapsed         | 7977          |\n",
      "|    total_timesteps      | 552960        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1350494e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00378       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.55e+09      |\n",
      "|    n_updates            | 2690          |\n",
      "|    policy_gradient_loss | -1.25e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.59e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.18e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 271           |\n",
      "|    time_elapsed         | 8000          |\n",
      "|    total_timesteps      | 555008        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0044        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.37e+09      |\n",
      "|    n_updates            | 2700          |\n",
      "|    policy_gradient_loss | -9.38e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.73e+09      |\n",
      "-------------------------------------------\n",
      "Episode 223  finished with cumulative reward: -6960500.0 and \n",
      "with an average reward of: -2783.0867652938823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 555222\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2784.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.2e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 272           |\n",
      "|    time_elapsed         | 8032          |\n",
      "|    total_timesteps      | 557056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00151       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.19e+10      |\n",
      "|    n_updates            | 2710          |\n",
      "|    policy_gradient_loss | -8.75e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.93e+10      |\n",
      "-------------------------------------------\n",
      "Episode 224  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 557723\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.23e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 273           |\n",
      "|    time_elapsed         | 8063          |\n",
      "|    total_timesteps      | 559104        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00203       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.32e+09      |\n",
      "|    n_updates            | 2720          |\n",
      "|    policy_gradient_loss | -9.66e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.98e+09      |\n",
      "-------------------------------------------\n",
      "Episode 225  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 560224\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.28e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 274          |\n",
      "|    time_elapsed         | 8094         |\n",
      "|    total_timesteps      | 561152       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.891749e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00171      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.18e+09     |\n",
      "|    n_updates            | 2730         |\n",
      "|    policy_gradient_loss | -1.37e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.79e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 226  finished with cumulative reward: -11474000.0 and \n",
      "with an average reward of: -4587.764894042383\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 562725\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4589.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 275       |\n",
      "|    time_elapsed         | 8126      |\n",
      "|    total_timesteps      | 563200    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00179   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.03e+09  |\n",
      "|    n_updates            | 2740      |\n",
      "|    policy_gradient_loss | -5.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n",
      "Episode 227  finished with cumulative reward: -13029500.0 and \n",
      "with an average reward of: -5209.716113554578\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 565226\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5211.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 276       |\n",
      "|    time_elapsed         | 8156      |\n",
      "|    total_timesteps      | 565248    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00174   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.8e+10   |\n",
      "|    n_updates            | 2750      |\n",
      "|    policy_gradient_loss | -5.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.4e+10   |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.41e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 277           |\n",
      "|    time_elapsed         | 8179          |\n",
      "|    total_timesteps      | 567296        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00488       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.25e+10      |\n",
      "|    n_updates            | 2760          |\n",
      "|    policy_gradient_loss | -1.17e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.33e+10      |\n",
      "-------------------------------------------\n",
      "Episode 228  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 567727\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.43e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 278           |\n",
      "|    time_elapsed         | 8211          |\n",
      "|    total_timesteps      | 569344        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00383       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.71e+09      |\n",
      "|    n_updates            | 2770          |\n",
      "|    policy_gradient_loss | -1.13e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.39e+10      |\n",
      "-------------------------------------------\n",
      "Episode 229  finished with cumulative reward: -2319500.0 and \n",
      "with an average reward of: -927.4290283886445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 570228\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -927.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.36e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 279           |\n",
      "|    time_elapsed         | 8242          |\n",
      "|    total_timesteps      | 571392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00218       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.27e+08      |\n",
      "|    n_updates            | 2780          |\n",
      "|    policy_gradient_loss | -8.52e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.38e+09      |\n",
      "-------------------------------------------\n",
      "Episode 230  finished with cumulative reward: -2294000.0 and \n",
      "with an average reward of: -917.233106757297\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 572729\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -917.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.35e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 280          |\n",
      "|    time_elapsed         | 8273         |\n",
      "|    total_timesteps      | 573440       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.132744e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00525      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.9e+08      |\n",
      "|    n_updates            | 2790         |\n",
      "|    policy_gradient_loss | -2.08e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.16e+09     |\n",
      "------------------------------------------\n",
      "Episode 231  finished with cumulative reward: -12111500.0 and \n",
      "with an average reward of: -4842.662934826069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 575230\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4844.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.46e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 281           |\n",
      "|    time_elapsed         | 8303          |\n",
      "|    total_timesteps      | 575488        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8521754e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00196       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.09e+09      |\n",
      "|    n_updates            | 2800          |\n",
      "|    policy_gradient_loss | -1.68e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.01e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 282       |\n",
      "|    time_elapsed         | 8326      |\n",
      "|    total_timesteps      | 577536    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00253   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.6e+10   |\n",
      "|    n_updates            | 2810      |\n",
      "|    policy_gradient_loss | -4.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 232  finished with cumulative reward: -8363000.0 and \n",
      "with an average reward of: -3343.862455017993\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 577731\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3345.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.5e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 283           |\n",
      "|    time_elapsed         | 8357          |\n",
      "|    total_timesteps      | 579584        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00367       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.38e+09      |\n",
      "|    n_updates            | 2820          |\n",
      "|    policy_gradient_loss | -9.78e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.89e+10      |\n",
      "-------------------------------------------\n",
      "Episode 233  finished with cumulative reward: -4895000.0 and \n",
      "with an average reward of: -1957.2171131547382\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 580232\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1958.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.52e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 284           |\n",
      "|    time_elapsed         | 8388          |\n",
      "|    total_timesteps      | 581632        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9208528e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00322       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.79e+09      |\n",
      "|    n_updates            | 2830          |\n",
      "|    policy_gradient_loss | -1.52e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.76e+09      |\n",
      "-------------------------------------------\n",
      "Episode 234  finished with cumulative reward: -1784000.0 and \n",
      "with an average reward of: -713.3146741303478\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 582733\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -713.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 285       |\n",
      "|    time_elapsed         | 8420      |\n",
      "|    total_timesteps      | 583680    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0033    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.37e+09  |\n",
      "|    n_updates            | 2840      |\n",
      "|    policy_gradient_loss | -4.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 235  finished with cumulative reward: -7215500.0 and \n",
      "with an average reward of: -2885.045981607357\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 585234\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2886.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.47e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 286           |\n",
      "|    time_elapsed         | 8452          |\n",
      "|    total_timesteps      | 585728        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00171       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.13e+08      |\n",
      "|    n_updates            | 2850          |\n",
      "|    policy_gradient_loss | -8.08e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.84e+09      |\n",
      "-------------------------------------------\n",
      "Episode 236  finished with cumulative reward: -12927500.0 and \n",
      "with an average reward of: -5168.932427029188\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 587735\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5171.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 287       |\n",
      "|    time_elapsed         | 8483      |\n",
      "|    total_timesteps      | 587776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00258   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.3e+10   |\n",
      "|    n_updates            | 2860      |\n",
      "|    policy_gradient_loss | -4.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.92e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.6e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 288           |\n",
      "|    time_elapsed         | 8506          |\n",
      "|    total_timesteps      | 589824        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00393       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.15e+10      |\n",
      "|    n_updates            | 2870          |\n",
      "|    policy_gradient_loss | -1.15e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.27e+10      |\n",
      "-------------------------------------------\n",
      "Episode 237  finished with cumulative reward: -10862000.0 and \n",
      "with an average reward of: -4343.062774890044\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 590236\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4344.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 289       |\n",
      "|    time_elapsed         | 8538      |\n",
      "|    total_timesteps      | 591872    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00321   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 2880      |\n",
      "|    policy_gradient_loss | -4.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.07e+10  |\n",
      "---------------------------------------\n",
      "Episode 238  finished with cumulative reward: -4155500.0 and \n",
      "with an average reward of: -1661.5353858456617\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 592737\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1662.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 290          |\n",
      "|    time_elapsed         | 8568         |\n",
      "|    total_timesteps      | 593920       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0036       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.65e+09     |\n",
      "|    n_updates            | 2890         |\n",
      "|    policy_gradient_loss | -9.66e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.22e+10     |\n",
      "------------------------------------------\n",
      "Episode 239  finished with cumulative reward: -3824000.0 and \n",
      "with an average reward of: -1528.9884046381446\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 595238\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1529.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.66e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 291           |\n",
      "|    time_elapsed         | 8599          |\n",
      "|    total_timesteps      | 595968        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3655746e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00379       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.06e+09      |\n",
      "|    n_updates            | 2900          |\n",
      "|    policy_gradient_loss | -1.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.95e+09      |\n",
      "-------------------------------------------\n",
      "Episode 240  finished with cumulative reward: -10301000.0 and \n",
      "with an average reward of: -4118.752499000399\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 597739\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4120.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 292          |\n",
      "|    time_elapsed         | 8630         |\n",
      "|    total_timesteps      | 598016       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.947651e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00328      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.52e+09     |\n",
      "|    n_updates            | 2910         |\n",
      "|    policy_gradient_loss | -1.27e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.15e+10     |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.68e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 293           |\n",
      "|    time_elapsed         | 8653          |\n",
      "|    total_timesteps      | 600064        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00109       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.65e+09      |\n",
      "|    n_updates            | 2920          |\n",
      "|    policy_gradient_loss | -9.28e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.46e+10      |\n",
      "-------------------------------------------\n",
      "Episode 241  finished with cumulative reward: -2396000.0 and \n",
      "with an average reward of: -958.0167932826869\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 600240\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -958.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.57e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 294           |\n",
      "|    time_elapsed         | 8685          |\n",
      "|    total_timesteps      | 602112        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3271347e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00357       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.85e+09      |\n",
      "|    n_updates            | 2930          |\n",
      "|    policy_gradient_loss | -2.92e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.79e+09      |\n",
      "-------------------------------------------\n",
      "Episode 242  finished with cumulative reward: -8439500.0 and \n",
      "with an average reward of: -3374.450219912035\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 602741\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3375.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.61e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 295          |\n",
      "|    time_elapsed         | 8716         |\n",
      "|    total_timesteps      | 604160       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.693881e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00342      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.99e+09     |\n",
      "|    n_updates            | 2940         |\n",
      "|    policy_gradient_loss | -1.19e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.94e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 243  finished with cumulative reward: -5252000.0 and \n",
      "with an average reward of: -2099.9600159936026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 605242\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2100.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.58e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 296           |\n",
      "|    time_elapsed         | 8748          |\n",
      "|    total_timesteps      | 606208        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00225       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.09e+09      |\n",
      "|    n_updates            | 2950          |\n",
      "|    policy_gradient_loss | -1.11e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.45e+10      |\n",
      "-------------------------------------------\n",
      "Episode 244  finished with cumulative reward: -7088000.0 and \n",
      "with an average reward of: -2834.0663734506197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 607743\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2835.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.59e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 297           |\n",
      "|    time_elapsed         | 8779          |\n",
      "|    total_timesteps      | 608256        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00114       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.57e+09      |\n",
      "|    n_updates            | 2960          |\n",
      "|    policy_gradient_loss | -6.68e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.54e+09      |\n",
      "-------------------------------------------\n",
      "Episode 245  finished with cumulative reward: -7955000.0 and \n",
      "with an average reward of: -3180.7277089164336\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 610244\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3182.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 298          |\n",
      "|    time_elapsed         | 8809         |\n",
      "|    total_timesteps      | 610304       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0021       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.01e+09     |\n",
      "|    n_updates            | 2970         |\n",
      "|    policy_gradient_loss | -7.54e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.37e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 299          |\n",
      "|    time_elapsed         | 8831         |\n",
      "|    total_timesteps      | 612352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 6.693881e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00202      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.34e+10     |\n",
      "|    n_updates            | 2980         |\n",
      "|    policy_gradient_loss | -1.22e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.72e+10     |\n",
      "------------------------------------------\n",
      "Episode 246  finished with cumulative reward: -8465000.0 and \n",
      "with an average reward of: -3384.646141543383\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 612745\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3386.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.68e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 300           |\n",
      "|    time_elapsed         | 8862          |\n",
      "|    total_timesteps      | 614400        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00414       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.37e+09      |\n",
      "|    n_updates            | 2990          |\n",
      "|    policy_gradient_loss | -8.53e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.77e+10      |\n",
      "-------------------------------------------\n",
      "Episode 247  finished with cumulative reward: -11117000.0 and \n",
      "with an average reward of: -4445.021991203518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 615246\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4446.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.72e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 301          |\n",
      "|    time_elapsed         | 8893         |\n",
      "|    total_timesteps      | 616448       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00376      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.86e+09     |\n",
      "|    n_updates            | 3000         |\n",
      "|    policy_gradient_loss | -8.94e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.23e+10     |\n",
      "------------------------------------------\n",
      "Episode 248  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 617747\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.72e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 302          |\n",
      "|    time_elapsed         | 8925         |\n",
      "|    total_timesteps      | 618496       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00581      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.89e+10     |\n",
      "|    n_updates            | 3010         |\n",
      "|    policy_gradient_loss | -1.45e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.38e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 249  finished with cumulative reward: -3390500.0 and \n",
      "with an average reward of: -1355.657736905238\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 620248\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1356.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.71e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 303          |\n",
      "|    time_elapsed         | 8956         |\n",
      "|    total_timesteps      | 620544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.313226e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00274      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.14e+09     |\n",
      "|    n_updates            | 3020         |\n",
      "|    policy_gradient_loss | -1.39e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.09e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 304       |\n",
      "|    time_elapsed         | 8980      |\n",
      "|    total_timesteps      | 622592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0028    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.78e+09  |\n",
      "|    n_updates            | 3030      |\n",
      "|    policy_gradient_loss | -7.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 250  finished with cumulative reward: -3314000.0 and \n",
      "with an average reward of: -1325.0699720111954\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 622749\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1325.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 305       |\n",
      "|    time_elapsed         | 9011      |\n",
      "|    total_timesteps      | 624640    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00418   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.04e+09  |\n",
      "|    n_updates            | 3040      |\n",
      "|    policy_gradient_loss | -4.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.49e+09  |\n",
      "---------------------------------------\n",
      "Episode 251  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 625250\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.73e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 306          |\n",
      "|    time_elapsed         | 9042         |\n",
      "|    total_timesteps      | 626688       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0026       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.34e+09     |\n",
      "|    n_updates            | 3050         |\n",
      "|    policy_gradient_loss | -7.56e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.54e+10     |\n",
      "------------------------------------------\n",
      "Episode 252  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 627751\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.73e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 307           |\n",
      "|    time_elapsed         | 9073          |\n",
      "|    total_timesteps      | 628736        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4842954e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00257       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.16e+09      |\n",
      "|    n_updates            | 3060          |\n",
      "|    policy_gradient_loss | -1.61e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.19e+10      |\n",
      "-------------------------------------------\n",
      "Episode 253  finished with cumulative reward: -6578000.0 and \n",
      "with an average reward of: -2630.1479408236705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 630252\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2631.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 308       |\n",
      "|    time_elapsed         | 9103      |\n",
      "|    total_timesteps      | 630784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00391   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.45e+09  |\n",
      "|    n_updates            | 3070      |\n",
      "|    policy_gradient_loss | -5.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.47e+10  |\n",
      "---------------------------------------\n",
      "Episode 254  finished with cumulative reward: -3467000.0 and \n",
      "with an average reward of: -1386.2455017992802\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 632753\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1386.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 309           |\n",
      "|    time_elapsed         | 9133          |\n",
      "|    total_timesteps      | 632832        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4028427e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00569       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.4e+09       |\n",
      "|    n_updates            | 3080          |\n",
      "|    policy_gradient_loss | -1.19e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.24e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.71e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 310          |\n",
      "|    time_elapsed         | 9156         |\n",
      "|    total_timesteps      | 634880       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00502      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.96e+09     |\n",
      "|    n_updates            | 3090         |\n",
      "|    policy_gradient_loss | -1.31e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.03e+09     |\n",
      "------------------------------------------\n",
      "Episode 255  finished with cumulative reward: -5430500.0 and \n",
      "with an average reward of: -2171.3314674130347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 635254\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2172.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 311       |\n",
      "|    time_elapsed         | 9187      |\n",
      "|    total_timesteps      | 636928    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00226   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.81e+09  |\n",
      "|    n_updates            | 3100      |\n",
      "|    policy_gradient_loss | -5.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.28e+09  |\n",
      "---------------------------------------\n",
      "Episode 256  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 637755\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 312          |\n",
      "|    time_elapsed         | 9217         |\n",
      "|    total_timesteps      | 638976       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00411      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.11e+10     |\n",
      "|    n_updates            | 3110         |\n",
      "|    policy_gradient_loss | -7.21e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.09e+10     |\n",
      "------------------------------------------\n",
      "Episode 257  finished with cumulative reward: -4818500.0 and \n",
      "with an average reward of: -1926.6293482606957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 640256\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1927.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 313          |\n",
      "|    time_elapsed         | 9248         |\n",
      "|    total_timesteps      | 641024       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.313226e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00612      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.3e+09      |\n",
      "|    n_updates            | 3120         |\n",
      "|    policy_gradient_loss | -1.33e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.97e+09     |\n",
      "------------------------------------------\n",
      "Episode 258  finished with cumulative reward: -1503500.0 and \n",
      "with an average reward of: -601.1595361855258\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 642757\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -601.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 314       |\n",
      "|    time_elapsed         | 9279      |\n",
      "|    total_timesteps      | 643072    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00487   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.52e+09  |\n",
      "|    n_updates            | 3130      |\n",
      "|    policy_gradient_loss | -4.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.98e+09  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.52e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 315           |\n",
      "|    time_elapsed         | 9302          |\n",
      "|    total_timesteps      | 645120        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00488       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.29e+08      |\n",
      "|    n_updates            | 3140          |\n",
      "|    policy_gradient_loss | -1.12e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.63e+09      |\n",
      "-------------------------------------------\n",
      "Episode 259  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 645258\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.55e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 316           |\n",
      "|    time_elapsed         | 9332          |\n",
      "|    total_timesteps      | 647168        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3969839e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00277       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.79e+09      |\n",
      "|    n_updates            | 3150          |\n",
      "|    policy_gradient_loss | -1.24e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.74e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 260  finished with cumulative reward: -3467000.0 and \n",
      "with an average reward of: -1386.2455017992802\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 647759\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1386.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.54e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 317          |\n",
      "|    time_elapsed         | 9362         |\n",
      "|    total_timesteps      | 649216       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00285      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+09     |\n",
      "|    n_updates            | 3160         |\n",
      "|    policy_gradient_loss | -7.81e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.35e+09     |\n",
      "------------------------------------------\n",
      "Episode 261  finished with cumulative reward: -10836500.0 and \n",
      "with an average reward of: -4332.866853258696\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 650260\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4334.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.61e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 318           |\n",
      "|    time_elapsed         | 9393          |\n",
      "|    total_timesteps      | 651264        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4028427e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00298       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.04e+10      |\n",
      "|    n_updates            | 3170          |\n",
      "|    policy_gradient_loss | -1.13e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.88e+10      |\n",
      "-------------------------------------------\n",
      "Episode 262  finished with cumulative reward: 77500.0 and \n",
      "with an average reward of: 30.987604958016792\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 652761\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: 31.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.57e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 319           |\n",
      "|    time_elapsed         | 9425          |\n",
      "|    total_timesteps      | 653312        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00519       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.28e+09      |\n",
      "|    n_updates            | 3180          |\n",
      "|    policy_gradient_loss | -8.68e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.7e+10       |\n",
      "-------------------------------------------\n",
      "Episode 263  finished with cumulative reward: -7547000.0 and \n",
      "with an average reward of: -3017.592962814874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 655262\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3018.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.53e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 320           |\n",
      "|    time_elapsed         | 9455          |\n",
      "|    total_timesteps      | 655360        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0179        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.13e+07      |\n",
      "|    n_updates            | 3190          |\n",
      "|    policy_gradient_loss | -1.01e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.66e+08      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 321       |\n",
      "|    time_elapsed         | 9477      |\n",
      "|    total_timesteps      | 657408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00426   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.6e+09   |\n",
      "|    n_updates            | 3200      |\n",
      "|    policy_gradient_loss | -2.98e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.72e+10  |\n",
      "---------------------------------------\n",
      "Episode 264  finished with cumulative reward: -2319500.0 and \n",
      "with an average reward of: -927.4290283886445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 657763\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -927.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.5e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 322          |\n",
      "|    time_elapsed         | 9507         |\n",
      "|    total_timesteps      | 659456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.905772e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00574      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.17e+09     |\n",
      "|    n_updates            | 3210         |\n",
      "|    policy_gradient_loss | -2e-05       |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.75e+09     |\n",
      "------------------------------------------\n",
      "Episode 265  finished with cumulative reward: -9230000.0 and \n",
      "with an average reward of: -3690.5237904838064\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 660264\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3692.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 323       |\n",
      "|    time_elapsed         | 9538      |\n",
      "|    total_timesteps      | 661504    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00287   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.3e+09   |\n",
      "|    n_updates            | 3220      |\n",
      "|    policy_gradient_loss | -2.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 266  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 662765\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 324       |\n",
      "|    time_elapsed         | 9569      |\n",
      "|    total_timesteps      | 663552    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00359   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.79e+09  |\n",
      "|    n_updates            | 3230      |\n",
      "|    policy_gradient_loss | -5.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 267  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 665266\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 325       |\n",
      "|    time_elapsed         | 9600      |\n",
      "|    total_timesteps      | 665600    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00646   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.98e+09  |\n",
      "|    n_updates            | 3240      |\n",
      "|    policy_gradient_loss | -8.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.11e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 326       |\n",
      "|    time_elapsed         | 9623      |\n",
      "|    total_timesteps      | 667648    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00353   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.72e+09  |\n",
      "|    n_updates            | 3250      |\n",
      "|    policy_gradient_loss | -5.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n",
      "Episode 268  finished with cumulative reward: -8592500.0 and \n",
      "with an average reward of: -3435.6257497001197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 667767\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3437.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.54e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 327       |\n",
      "|    time_elapsed         | 9654      |\n",
      "|    total_timesteps      | 669696    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00414   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.15e+10  |\n",
      "|    n_updates            | 3260      |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 269  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 670268\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.54e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 328           |\n",
      "|    time_elapsed         | 9685          |\n",
      "|    total_timesteps      | 671744        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00476       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+10      |\n",
      "|    n_updates            | 3270          |\n",
      "|    policy_gradient_loss | -9.16e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.68e+10      |\n",
      "-------------------------------------------\n",
      "Episode 270  finished with cumulative reward: -8363000.0 and \n",
      "with an average reward of: -3343.862455017993\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 672769\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3345.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.6e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 329           |\n",
      "|    time_elapsed         | 9715          |\n",
      "|    total_timesteps      | 673792        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8521754e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0024        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.61e+09      |\n",
      "|    n_updates            | 3280          |\n",
      "|    policy_gradient_loss | -1.61e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.49e+10      |\n",
      "-------------------------------------------\n",
      "Episode 271  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 675270\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.61e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 330          |\n",
      "|    time_elapsed         | 9746         |\n",
      "|    total_timesteps      | 675840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00658      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.38e+09     |\n",
      "|    n_updates            | 3290         |\n",
      "|    policy_gradient_loss | -6.76e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.13e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 272  finished with cumulative reward: -5838500.0 and \n",
      "with an average reward of: -2334.466213514594\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 677771\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2335.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 331          |\n",
      "|    time_elapsed         | 9776         |\n",
      "|    total_timesteps      | 677888       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00497      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.04e+09     |\n",
      "|    n_updates            | 3300         |\n",
      "|    policy_gradient_loss | -7.07e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.28e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 332       |\n",
      "|    time_elapsed         | 9798      |\n",
      "|    total_timesteps      | 679936    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00391   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.06e+09  |\n",
      "|    n_updates            | 3310      |\n",
      "|    policy_gradient_loss | -8.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.25e+10  |\n",
      "---------------------------------------\n",
      "Episode 273  finished with cumulative reward: -6884000.0 and \n",
      "with an average reward of: -2752.49900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 680272\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2753.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 333       |\n",
      "|    time_elapsed         | 9829      |\n",
      "|    total_timesteps      | 681984    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00316   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.13e+10  |\n",
      "|    n_updates            | 3320      |\n",
      "|    policy_gradient_loss | -9.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.88e+10  |\n",
      "---------------------------------------\n",
      "Episode 274  finished with cumulative reward: -1580000.0 and \n",
      "with an average reward of: -631.7473010795682\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 682773\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -632.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 334          |\n",
      "|    time_elapsed         | 9859         |\n",
      "|    total_timesteps      | 684032       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00203      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.41e+09     |\n",
      "|    n_updates            | 3330         |\n",
      "|    policy_gradient_loss | -6.38e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.17e+09     |\n",
      "------------------------------------------\n",
      "Episode 275  finished with cumulative reward: -6348500.0 and \n",
      "with an average reward of: -2538.3846461415433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 685274\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2539.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 335           |\n",
      "|    time_elapsed         | 9890          |\n",
      "|    total_timesteps      | 686080        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00683       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.17e+09      |\n",
      "|    n_updates            | 3340          |\n",
      "|    policy_gradient_loss | -1.03e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.3e+09       |\n",
      "-------------------------------------------\n",
      "Episode 276  finished with cumulative reward: -2115500.0 and \n",
      "with an average reward of: -845.8616553378648\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 687775\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -846.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 336       |\n",
      "|    time_elapsed         | 9921      |\n",
      "|    total_timesteps      | 688128    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00185   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.11e+09  |\n",
      "|    n_updates            | 3350      |\n",
      "|    policy_gradient_loss | -7.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.51e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.56e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 337          |\n",
      "|    time_elapsed         | 9944         |\n",
      "|    total_timesteps      | 690176       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00412      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.65e+09     |\n",
      "|    n_updates            | 3360         |\n",
      "|    policy_gradient_loss | -6.85e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.08e+09     |\n",
      "------------------------------------------\n",
      "Episode 277  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 690276\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.51e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 338           |\n",
      "|    time_elapsed         | 9975          |\n",
      "|    total_timesteps      | 692224        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0037        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.34e+09      |\n",
      "|    n_updates            | 3370          |\n",
      "|    policy_gradient_loss | -9.75e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.81e+09      |\n",
      "-------------------------------------------\n",
      "Episode 278  finished with cumulative reward: -1860500.0 and \n",
      "with an average reward of: -743.9024390243902\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 692777\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -744.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.43e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 339          |\n",
      "|    time_elapsed         | 10005        |\n",
      "|    total_timesteps      | 694272       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00477      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.57e+08     |\n",
      "|    n_updates            | 3380         |\n",
      "|    policy_gradient_loss | -1.29e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.18e+09     |\n",
      "------------------------------------------\n",
      "Episode 279  finished with cumulative reward: -8439500.0 and \n",
      "with an average reward of: -3374.450219912035\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 695278\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3375.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.42e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 340          |\n",
      "|    time_elapsed         | 10036        |\n",
      "|    total_timesteps      | 696320       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00516      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.57e+09     |\n",
      "|    n_updates            | 3390         |\n",
      "|    policy_gradient_loss | -9.45e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.01e+09     |\n",
      "------------------------------------------\n",
      "Episode 280  finished with cumulative reward: -7062500.0 and \n",
      "with an average reward of: -2823.870451819272\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 697779\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2825.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.45e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 341       |\n",
      "|    time_elapsed         | 10065     |\n",
      "|    total_timesteps      | 698368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0062    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.39e+09  |\n",
      "|    n_updates            | 3400      |\n",
      "|    policy_gradient_loss | -6.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.97e+10  |\n",
      "---------------------------------------\n",
      "Episode 281  finished with cumulative reward: -3696500.0 and \n",
      "with an average reward of: -1478.0087964814074\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 700280\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1478.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.42e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 342           |\n",
      "|    time_elapsed         | 10096         |\n",
      "|    total_timesteps      | 700416        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00279       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.41e+09      |\n",
      "|    n_updates            | 3410          |\n",
      "|    policy_gradient_loss | -8.81e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.46e+10      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.42e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 343           |\n",
      "|    time_elapsed         | 10118         |\n",
      "|    total_timesteps      | 702464        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00579       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.8e+09       |\n",
      "|    n_updates            | 3420          |\n",
      "|    policy_gradient_loss | -1.08e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.15e+10      |\n",
      "-------------------------------------------\n",
      "Episode 282  finished with cumulative reward: -6374000.0 and \n",
      "with an average reward of: -2548.580567772891\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 702781\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2549.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 344       |\n",
      "|    time_elapsed         | 10149     |\n",
      "|    total_timesteps      | 704512    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00307   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.3e+09   |\n",
      "|    n_updates            | 3430      |\n",
      "|    policy_gradient_loss | -2.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 283  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 705282\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 345       |\n",
      "|    time_elapsed         | 10180     |\n",
      "|    total_timesteps      | 706560    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00278   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.75e+09  |\n",
      "|    n_updates            | 3440      |\n",
      "|    policy_gradient_loss | -4.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.63e+10  |\n",
      "---------------------------------------\n",
      "Episode 284  finished with cumulative reward: -8618000.0 and \n",
      "with an average reward of: -3445.8216713314673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 707783\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3447.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 346       |\n",
      "|    time_elapsed         | 10211     |\n",
      "|    total_timesteps      | 708608    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00413   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.54e+10  |\n",
      "|    n_updates            | 3450      |\n",
      "|    policy_gradient_loss | -3.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 285  finished with cumulative reward: -9230000.0 and \n",
      "with an average reward of: -3690.5237904838064\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 710284\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3692.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 347       |\n",
      "|    time_elapsed         | 10241     |\n",
      "|    total_timesteps      | 710656    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00478   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.03e+09  |\n",
      "|    n_updates            | 3460      |\n",
      "|    policy_gradient_loss | -4.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.46e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.47e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 348           |\n",
      "|    time_elapsed         | 10264         |\n",
      "|    total_timesteps      | 712704        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9790605e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00521       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.77e+09      |\n",
      "|    n_updates            | 3470          |\n",
      "|    policy_gradient_loss | -1.59e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6e+09         |\n",
      "-------------------------------------------\n",
      "Episode 286  finished with cumulative reward: -2549000.0 and \n",
      "with an average reward of: -1019.1923230707716\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 712785\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1019.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.44e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 349          |\n",
      "|    time_elapsed         | 10295        |\n",
      "|    total_timesteps      | 714752       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00378      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.9e+08      |\n",
      "|    n_updates            | 3480         |\n",
      "|    policy_gradient_loss | -6.95e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.68e+09     |\n",
      "------------------------------------------\n",
      "Episode 287  finished with cumulative reward: -6705500.0 and \n",
      "with an average reward of: -2681.127548980408\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 715286\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2682.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.48e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 350           |\n",
      "|    time_elapsed         | 10326         |\n",
      "|    total_timesteps      | 716800        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00723       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.12e+09      |\n",
      "|    n_updates            | 3490          |\n",
      "|    policy_gradient_loss | -8.95e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.45e+10      |\n",
      "-------------------------------------------\n",
      "Episode 288  finished with cumulative reward: -9153500.0 and \n",
      "with an average reward of: -3659.936025589764\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 717787\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3661.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 351       |\n",
      "|    time_elapsed         | 10357     |\n",
      "|    total_timesteps      | 718848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00272   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.64e+09  |\n",
      "|    n_updates            | 3500      |\n",
      "|    policy_gradient_loss | -2.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 289  finished with cumulative reward: -5532500.0 and \n",
      "with an average reward of: -2212.1151539384246\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 720288\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2213.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.5e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 352          |\n",
      "|    time_elapsed         | 10388        |\n",
      "|    total_timesteps      | 720896       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 3.783498e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00217      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.78e+09     |\n",
      "|    n_updates            | 3510         |\n",
      "|    policy_gradient_loss | -1.22e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.79e+10     |\n",
      "------------------------------------------\n",
      "Episode 290  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 722789\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.5e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 353       |\n",
      "|    time_elapsed         | 10418     |\n",
      "|    total_timesteps      | 722944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00307   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.29e+09  |\n",
      "|    n_updates            | 3520      |\n",
      "|    policy_gradient_loss | -6.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.82e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.5e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 354          |\n",
      "|    time_elapsed         | 10441        |\n",
      "|    total_timesteps      | 724992       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00481      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.08e+09     |\n",
      "|    n_updates            | 3530         |\n",
      "|    policy_gradient_loss | -6.63e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.47e+09     |\n",
      "------------------------------------------\n",
      "Episode 291  finished with cumulative reward: -2013500.0 and \n",
      "with an average reward of: -805.077968812475\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 725290\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -805.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 355       |\n",
      "|    time_elapsed         | 10472     |\n",
      "|    total_timesteps      | 727040    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00534   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.56e+09  |\n",
      "|    n_updates            | 3540      |\n",
      "|    policy_gradient_loss | -6.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 292  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 727791\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 356       |\n",
      "|    time_elapsed         | 10502     |\n",
      "|    total_timesteps      | 729088    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00684   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 3550      |\n",
      "|    policy_gradient_loss | -6.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 293  finished with cumulative reward: -8592500.0 and \n",
      "with an average reward of: -3435.6257497001197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 730292\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3437.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.37e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 357          |\n",
      "|    time_elapsed         | 10532        |\n",
      "|    total_timesteps      | 731136       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00636      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.48e+09     |\n",
      "|    n_updates            | 3560         |\n",
      "|    policy_gradient_loss | -7.75e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.08e+10     |\n",
      "------------------------------------------\n",
      "Episode 294  finished with cumulative reward: -6450500.0 and \n",
      "with an average reward of: -2579.168332666933\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 732793\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2580.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.39e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 358          |\n",
      "|    time_elapsed         | 10563        |\n",
      "|    total_timesteps      | 733184       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00716      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.27e+09     |\n",
      "|    n_updates            | 3570         |\n",
      "|    policy_gradient_loss | -7.69e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.08e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 359       |\n",
      "|    time_elapsed         | 10585     |\n",
      "|    total_timesteps      | 735232    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00354   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.94e+09  |\n",
      "|    n_updates            | 3580      |\n",
      "|    policy_gradient_loss | -5.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.5e+10   |\n",
      "---------------------------------------\n",
      "Episode 295  finished with cumulative reward: -2727500.0 and \n",
      "with an average reward of: -1090.563774490204\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 735294\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1091.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 360       |\n",
      "|    time_elapsed         | 10616     |\n",
      "|    total_timesteps      | 737280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00675   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.5e+09   |\n",
      "|    n_updates            | 3590      |\n",
      "|    policy_gradient_loss | -9.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.41e+09  |\n",
      "---------------------------------------\n",
      "Episode 296  finished with cumulative reward: -3543500.0 and \n",
      "with an average reward of: -1416.8332666933227\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 737795\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1417.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 361       |\n",
      "|    time_elapsed         | 10647     |\n",
      "|    total_timesteps      | 739328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0061    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.93e+09  |\n",
      "|    n_updates            | 3600      |\n",
      "|    policy_gradient_loss | -5.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.74e+09  |\n",
      "---------------------------------------\n",
      "Episode 297  finished with cumulative reward: -1197500.0 and \n",
      "with an average reward of: -478.80847660935626\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 740296\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -479.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.29e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 362           |\n",
      "|    time_elapsed         | 10678         |\n",
      "|    total_timesteps      | 741376        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1059456e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00901       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.89e+09      |\n",
      "|    n_updates            | 3610          |\n",
      "|    policy_gradient_loss | -1.42e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.49e+09      |\n",
      "-------------------------------------------\n",
      "Episode 298  finished with cumulative reward: -12035000.0 and \n",
      "with an average reward of: -4812.075169932027\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 742797\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4814.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 363       |\n",
      "|    time_elapsed         | 10709     |\n",
      "|    total_timesteps      | 743424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00697   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.03e+10  |\n",
      "|    n_updates            | 3620      |\n",
      "|    policy_gradient_loss | -6.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 299  finished with cumulative reward: -6884000.0 and \n",
      "with an average reward of: -2752.49900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 745298\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2753.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 364       |\n",
      "|    time_elapsed         | 10740     |\n",
      "|    total_timesteps      | 745472    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00605   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.12e+10  |\n",
      "|    n_updates            | 3630      |\n",
      "|    policy_gradient_loss | -2.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.31e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 365       |\n",
      "|    time_elapsed         | 10762     |\n",
      "|    total_timesteps      | 747520    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00427   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.83e+09  |\n",
      "|    n_updates            | 3640      |\n",
      "|    policy_gradient_loss | -3.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 300  finished with cumulative reward: -6884000.0 and \n",
      "with an average reward of: -2752.49900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 747799\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2753.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.32e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 366          |\n",
      "|    time_elapsed         | 10793        |\n",
      "|    total_timesteps      | 749568       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00562      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.07e+09     |\n",
      "|    n_updates            | 3650         |\n",
      "|    policy_gradient_loss | -7.66e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.47e+09     |\n",
      "------------------------------------------\n",
      "Episode 301  finished with cumulative reward: -1962500.0 and \n",
      "with an average reward of: -784.6861255497801\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 750300\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -785.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 367       |\n",
      "|    time_elapsed         | 10824     |\n",
      "|    total_timesteps      | 751616    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0067    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.67e+09  |\n",
      "|    n_updates            | 3660      |\n",
      "|    policy_gradient_loss | -4.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n",
      "Episode 302  finished with cumulative reward: -1427000.0 and \n",
      "with an average reward of: -570.5717712914834\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 752801\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -570.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 368       |\n",
      "|    time_elapsed         | 10856     |\n",
      "|    total_timesteps      | 753664    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00801   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.59e+08  |\n",
      "|    n_updates            | 3670      |\n",
      "|    policy_gradient_loss | -6.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 303  finished with cumulative reward: -8006000.0 and \n",
      "with an average reward of: -3201.1195521791283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 755302\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3202.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 369       |\n",
      "|    time_elapsed         | 10887     |\n",
      "|    total_timesteps      | 755712    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00767   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.71e+09  |\n",
      "|    n_updates            | 3680      |\n",
      "|    policy_gradient_loss | -8.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.64e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 370       |\n",
      "|    time_elapsed         | 10909     |\n",
      "|    total_timesteps      | 757760    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00675   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.59e+09  |\n",
      "|    n_updates            | 3690      |\n",
      "|    policy_gradient_loss | -3.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 304  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 757803\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 371       |\n",
      "|    time_elapsed         | 10939     |\n",
      "|    total_timesteps      | 759808    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00438   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.04e+09  |\n",
      "|    n_updates            | 3700      |\n",
      "|    policy_gradient_loss | -5.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 305  finished with cumulative reward: -4155500.0 and \n",
      "with an average reward of: -1661.5353858456617\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 760304\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1662.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 372       |\n",
      "|    time_elapsed         | 10970     |\n",
      "|    total_timesteps      | 761856    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00644   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.18e+09  |\n",
      "|    n_updates            | 3710      |\n",
      "|    policy_gradient_loss | -5.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 306  finished with cumulative reward: -611000.0 and \n",
      "with an average reward of: -244.30227908836466\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 762805\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -244.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.19e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 373           |\n",
      "|    time_elapsed         | 11001         |\n",
      "|    total_timesteps      | 763904        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2677933e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00853       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.11e+08      |\n",
      "|    n_updates            | 3720          |\n",
      "|    policy_gradient_loss | -1.36e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.5e+09       |\n",
      "-------------------------------------------\n",
      "Episode 307  finished with cumulative reward: -4946000.0 and \n",
      "with an average reward of: -1977.608956417433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 765306\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1978.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 374       |\n",
      "|    time_elapsed         | 11032     |\n",
      "|    total_timesteps      | 765952    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00537   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.38e+09  |\n",
      "|    n_updates            | 3730      |\n",
      "|    policy_gradient_loss | -3.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 308  finished with cumulative reward: -2498000.0 and \n",
      "with an average reward of: -998.8004798080767\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 767807\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -999.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 375       |\n",
      "|    time_elapsed         | 11062     |\n",
      "|    total_timesteps      | 768000    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00504   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.05e+08  |\n",
      "|    n_updates            | 3740      |\n",
      "|    policy_gradient_loss | -4.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.56e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 376       |\n",
      "|    time_elapsed         | 11085     |\n",
      "|    total_timesteps      | 770048    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0081    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28e+09  |\n",
      "|    n_updates            | 3750      |\n",
      "|    policy_gradient_loss | -7.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.68e+09  |\n",
      "---------------------------------------\n",
      "Episode 309  finished with cumulative reward: -10148000.0 and \n",
      "with an average reward of: -4057.576969212315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 770308\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4059.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 377       |\n",
      "|    time_elapsed         | 11116     |\n",
      "|    total_timesteps      | 772096    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00444   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+10  |\n",
      "|    n_updates            | 3760      |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 310  finished with cumulative reward: -9689000.0 and \n",
      "with an average reward of: -3874.050379848061\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 772809\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3875.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 378       |\n",
      "|    time_elapsed         | 11147     |\n",
      "|    total_timesteps      | 774144    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00767   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.09e+09  |\n",
      "|    n_updates            | 3770      |\n",
      "|    policy_gradient_loss | -5e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.95e+10  |\n",
      "---------------------------------------\n",
      "Episode 311  finished with cumulative reward: -6221000.0 and \n",
      "with an average reward of: -2487.405037984806\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 775310\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2488.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 379       |\n",
      "|    time_elapsed         | 11178     |\n",
      "|    total_timesteps      | 776192    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00892   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e+10  |\n",
      "|    n_updates            | 3780      |\n",
      "|    policy_gradient_loss | -6.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.67e+10  |\n",
      "---------------------------------------\n",
      "Episode 312  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 777811\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.15e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 380          |\n",
      "|    time_elapsed         | 11208        |\n",
      "|    total_timesteps      | 778240       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0084       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.87e+09     |\n",
      "|    n_updates            | 3790         |\n",
      "|    policy_gradient_loss | -7.87e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.66e+09     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.15e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 381       |\n",
      "|    time_elapsed         | 11231     |\n",
      "|    total_timesteps      | 780288    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00565   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.88e+09  |\n",
      "|    n_updates            | 3800      |\n",
      "|    policy_gradient_loss | -6.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 313  finished with cumulative reward: -4232000.0 and \n",
      "with an average reward of: -1692.1231507397042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 780312\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1692.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 382       |\n",
      "|    time_elapsed         | 11261     |\n",
      "|    total_timesteps      | 782336    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0042    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.73e+09  |\n",
      "|    n_updates            | 3810      |\n",
      "|    policy_gradient_loss | -4.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.14e+09  |\n",
      "---------------------------------------\n",
      "Episode 314  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 782813\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.03e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 383           |\n",
      "|    time_elapsed         | 11292         |\n",
      "|    total_timesteps      | 784384        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00686       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.45e+09      |\n",
      "|    n_updates            | 3820          |\n",
      "|    policy_gradient_loss | -1.08e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.39e+10      |\n",
      "-------------------------------------------\n",
      "Episode 315  finished with cumulative reward: -3441500.0 and \n",
      "with an average reward of: -1376.0495801679328\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 785314\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1376.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 384       |\n",
      "|    time_elapsed         | 11323     |\n",
      "|    total_timesteps      | 786432    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00888   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.49e+09  |\n",
      "|    n_updates            | 3830      |\n",
      "|    policy_gradient_loss | -3.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.02e+09  |\n",
      "---------------------------------------\n",
      "Episode 316  finished with cumulative reward: -4691000.0 and \n",
      "with an average reward of: -1875.6497401039585\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 787815\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1876.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 385       |\n",
      "|    time_elapsed         | 11355     |\n",
      "|    total_timesteps      | 788480    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0119    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.62e+08  |\n",
      "|    n_updates            | 3840      |\n",
      "|    policy_gradient_loss | -3.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.55e+09  |\n",
      "---------------------------------------\n",
      "Episode 317  finished with cumulative reward: -3161000.0 and \n",
      "with an average reward of: -1263.8944422231107\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 790316\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1264.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 386       |\n",
      "|    time_elapsed         | 11385     |\n",
      "|    total_timesteps      | 790528    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00639   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.16e+09  |\n",
      "|    n_updates            | 3850      |\n",
      "|    policy_gradient_loss | -2.68e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.87e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 387           |\n",
      "|    time_elapsed         | 11407         |\n",
      "|    total_timesteps      | 792576        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0745363e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0107        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.25e+09      |\n",
      "|    n_updates            | 3860          |\n",
      "|    policy_gradient_loss | -9.21e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.22e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 318  finished with cumulative reward: -10275500.0 and \n",
      "with an average reward of: -4108.556577369052\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 792817\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4110.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 388       |\n",
      "|    time_elapsed         | 11438     |\n",
      "|    total_timesteps      | 794624    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00461   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.25e+10  |\n",
      "|    n_updates            | 3870      |\n",
      "|    policy_gradient_loss | -1.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.21e+10  |\n",
      "---------------------------------------\n",
      "Episode 319  finished with cumulative reward: -3492500.0 and \n",
      "with an average reward of: -1396.4414234306278\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 795318\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1397.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 389       |\n",
      "|    time_elapsed         | 11469     |\n",
      "|    total_timesteps      | 796672    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00646   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.27e+09  |\n",
      "|    n_updates            | 3880      |\n",
      "|    policy_gradient_loss | -5.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 320  finished with cumulative reward: -7037000.0 and \n",
      "with an average reward of: -2813.674530187925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 797819\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2814.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 390       |\n",
      "|    time_elapsed         | 11500     |\n",
      "|    total_timesteps      | 798720    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00746   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.83e+09  |\n",
      "|    n_updates            | 3890      |\n",
      "|    policy_gradient_loss | -3.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.38e+09  |\n",
      "---------------------------------------\n",
      "Episode 321  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 800320\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 391       |\n",
      "|    time_elapsed         | 11531     |\n",
      "|    total_timesteps      | 800768    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00632   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.57e+09  |\n",
      "|    n_updates            | 3900      |\n",
      "|    policy_gradient_loss | -2.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.46e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 392       |\n",
      "|    time_elapsed         | 11554     |\n",
      "|    total_timesteps      | 802816    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00618   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.95e+09  |\n",
      "|    n_updates            | 3910      |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 322  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 802821\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.91e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 393          |\n",
      "|    time_elapsed         | 11585        |\n",
      "|    total_timesteps      | 804864       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00697      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.54e+09     |\n",
      "|    n_updates            | 3920         |\n",
      "|    policy_gradient_loss | -8.99e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.31e+10     |\n",
      "------------------------------------------\n",
      "Episode 323  finished with cumulative reward: -9969500.0 and \n",
      "with an average reward of: -3986.205517792883\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 805322\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3987.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.94e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 394          |\n",
      "|    time_elapsed         | 11616        |\n",
      "|    total_timesteps      | 806912       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0067       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.77e+09     |\n",
      "|    n_updates            | 3930         |\n",
      "|    policy_gradient_loss | -8.86e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.86e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 324  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 807823\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 395       |\n",
      "|    time_elapsed         | 11647     |\n",
      "|    total_timesteps      | 808960    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00546   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.12e+09  |\n",
      "|    n_updates            | 3940      |\n",
      "|    policy_gradient_loss | -4.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.93e+10  |\n",
      "---------------------------------------\n",
      "Episode 325  finished with cumulative reward: -7547000.0 and \n",
      "with an average reward of: -3017.592962814874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 810324\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3018.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 396       |\n",
      "|    time_elapsed         | 11678     |\n",
      "|    total_timesteps      | 811008    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00574   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.25e+09  |\n",
      "|    n_updates            | 3950      |\n",
      "|    policy_gradient_loss | -4.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+10   |\n",
      "---------------------------------------\n",
      "Episode 326  finished with cumulative reward: -5022500.0 and \n",
      "with an average reward of: -2008.1967213114754\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 812825\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2009.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.9e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 397           |\n",
      "|    time_elapsed         | 11708         |\n",
      "|    total_timesteps      | 813056        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00907       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.63e+09      |\n",
      "|    n_updates            | 3960          |\n",
      "|    policy_gradient_loss | -1.03e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.1e+09       |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 398       |\n",
      "|    time_elapsed         | 11731     |\n",
      "|    total_timesteps      | 815104    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0082    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.01e+09  |\n",
      "|    n_updates            | 3970      |\n",
      "|    policy_gradient_loss | -6.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.22e+09  |\n",
      "---------------------------------------\n",
      "Episode 327  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 815326\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 399       |\n",
      "|    time_elapsed         | 11763     |\n",
      "|    total_timesteps      | 817152    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00706   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.74e+09  |\n",
      "|    n_updates            | 3980      |\n",
      "|    policy_gradient_loss | -2.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 328  finished with cumulative reward: -4002500.0 and \n",
      "with an average reward of: -1600.359856057577\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 817827\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1601.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.82e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 400          |\n",
      "|    time_elapsed         | 11794        |\n",
      "|    total_timesteps      | 819200       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0114       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.8e+09      |\n",
      "|    n_updates            | 3990         |\n",
      "|    policy_gradient_loss | -7.24e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.6e+09      |\n",
      "------------------------------------------\n",
      "Episode 329  finished with cumulative reward: -5124500.0 and \n",
      "with an average reward of: -2048.980407836865\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 820328\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2049.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 401       |\n",
      "|    time_elapsed         | 11825     |\n",
      "|    total_timesteps      | 821248    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0101    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.63e+09  |\n",
      "|    n_updates            | 4000      |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.86e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 330  finished with cumulative reward: -9102500.0 and \n",
      "with an average reward of: -3639.5441823270694\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 822829\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3641.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 402       |\n",
      "|    time_elapsed         | 11855     |\n",
      "|    total_timesteps      | 823296    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0117    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.09e+09  |\n",
      "|    n_updates            | 4010      |\n",
      "|    policy_gradient_loss | -5.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n",
      "Episode 331  finished with cumulative reward: -5201000.0 and \n",
      "with an average reward of: -2079.5681727309075\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 825330\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2080.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 403       |\n",
      "|    time_elapsed         | 11885     |\n",
      "|    total_timesteps      | 825344    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0104    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.43e+10  |\n",
      "|    n_updates            | 4020      |\n",
      "|    policy_gradient_loss | -4.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.09e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.85e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 404           |\n",
      "|    time_elapsed         | 11908         |\n",
      "|    total_timesteps      | 827392        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00784       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.31e+09      |\n",
      "|    n_updates            | 4030          |\n",
      "|    policy_gradient_loss | -1.29e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.31e+10      |\n",
      "-------------------------------------------\n",
      "Episode 332  finished with cumulative reward: -4844000.0 and \n",
      "with an average reward of: -1936.8252698920433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 827831\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1937.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.81e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 405       |\n",
      "|    time_elapsed         | 11938     |\n",
      "|    total_timesteps      | 829440    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00906   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.63e+09  |\n",
      "|    n_updates            | 4040      |\n",
      "|    policy_gradient_loss | -4.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.92e+09  |\n",
      "---------------------------------------\n",
      "Episode 333  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 830332\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 406       |\n",
      "|    time_elapsed         | 11969     |\n",
      "|    total_timesteps      | 831488    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00605   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.17e+10  |\n",
      "|    n_updates            | 4050      |\n",
      "|    policy_gradient_loss | -5.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 334  finished with cumulative reward: -7164500.0 and \n",
      "with an average reward of: -2864.654138344662\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 832833\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2865.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 407       |\n",
      "|    time_elapsed         | 11999     |\n",
      "|    total_timesteps      | 833536    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0152    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.44e+09  |\n",
      "|    n_updates            | 4060      |\n",
      "|    policy_gradient_loss | -4.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 335  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 835334\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 408       |\n",
      "|    time_elapsed         | 12029     |\n",
      "|    total_timesteps      | 835584    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00646   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28e+10  |\n",
      "|    n_updates            | 4070      |\n",
      "|    policy_gradient_loss | -5.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.04e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 409       |\n",
      "|    time_elapsed         | 12051     |\n",
      "|    total_timesteps      | 837632    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00922   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.71e+09  |\n",
      "|    n_updates            | 4080      |\n",
      "|    policy_gradient_loss | -5.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.59e+10  |\n",
      "---------------------------------------\n",
      "Episode 336  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 837835\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.82e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 410       |\n",
      "|    time_elapsed         | 12082     |\n",
      "|    total_timesteps      | 839680    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00941   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.15e+09  |\n",
      "|    n_updates            | 4090      |\n",
      "|    policy_gradient_loss | -3.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.47e+10  |\n",
      "---------------------------------------\n",
      "Episode 337  finished with cumulative reward: -6629000.0 and \n",
      "with an average reward of: -2650.5397840863657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 840336\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2651.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.77e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 411          |\n",
      "|    time_elapsed         | 12113        |\n",
      "|    total_timesteps      | 841728       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00716      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.39e+10     |\n",
      "|    n_updates            | 4100         |\n",
      "|    policy_gradient_loss | -6.58e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.7e+10      |\n",
      "------------------------------------------\n",
      "Episode 338  finished with cumulative reward: -2523500.0 and \n",
      "with an average reward of: -1008.9964014394242\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 842837\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1009.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.76e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 412       |\n",
      "|    time_elapsed         | 12144     |\n",
      "|    total_timesteps      | 843776    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00552   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.47e+08  |\n",
      "|    n_updates            | 4110      |\n",
      "|    policy_gradient_loss | -2.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.8e+09   |\n",
      "---------------------------------------\n",
      "Episode 339  finished with cumulative reward: -4155500.0 and \n",
      "with an average reward of: -1661.5353858456617\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 845338\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1662.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.76e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 413          |\n",
      "|    time_elapsed         | 12176        |\n",
      "|    total_timesteps      | 845824       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00864      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.98e+09     |\n",
      "|    n_updates            | 4120         |\n",
      "|    policy_gradient_loss | -6.89e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.74e+09     |\n",
      "------------------------------------------\n",
      "Episode 340  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 847839\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 414       |\n",
      "|    time_elapsed         | 12207     |\n",
      "|    total_timesteps      | 847872    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00809   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.64e+09  |\n",
      "|    n_updates            | 4130      |\n",
      "|    policy_gradient_loss | -4.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 415       |\n",
      "|    time_elapsed         | 12231     |\n",
      "|    total_timesteps      | 849920    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00888   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.37e+10  |\n",
      "|    n_updates            | 4140      |\n",
      "|    policy_gradient_loss | -3.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 341  finished with cumulative reward: -2268500.0 and \n",
      "with an average reward of: -907.0371851259496\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 850340\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -907.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 416       |\n",
      "|    time_elapsed         | 12261     |\n",
      "|    total_timesteps      | 851968    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0103    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.69e+08  |\n",
      "|    n_updates            | 4150      |\n",
      "|    policy_gradient_loss | -3.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.08e+09  |\n",
      "---------------------------------------\n",
      "Episode 342  finished with cumulative reward: -10071500.0 and \n",
      "with an average reward of: -4026.9892043182726\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 852841\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4028.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.74e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 417          |\n",
      "|    time_elapsed         | 12291        |\n",
      "|    total_timesteps      | 854016       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00978      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.24e+09     |\n",
      "|    n_updates            | 4160         |\n",
      "|    policy_gradient_loss | -6.67e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.05e+10     |\n",
      "------------------------------------------\n",
      "Episode 343  finished with cumulative reward: -2294000.0 and \n",
      "with an average reward of: -917.233106757297\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 855342\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -917.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 418       |\n",
      "|    time_elapsed         | 12322     |\n",
      "|    total_timesteps      | 856064    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00929   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.29e+09  |\n",
      "|    n_updates            | 4170      |\n",
      "|    policy_gradient_loss | -3.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 344  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 857843\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.72e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 419          |\n",
      "|    time_elapsed         | 12352        |\n",
      "|    total_timesteps      | 858112       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00732      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.82e+09     |\n",
      "|    n_updates            | 4180         |\n",
      "|    policy_gradient_loss | -3.89e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.66e+09     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 420       |\n",
      "|    time_elapsed         | 12374     |\n",
      "|    total_timesteps      | 860160    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00935   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.75e+09  |\n",
      "|    n_updates            | 4190      |\n",
      "|    policy_gradient_loss | -2.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 345  finished with cumulative reward: -2498000.0 and \n",
      "with an average reward of: -998.8004798080767\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 860344\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -999.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 421       |\n",
      "|    time_elapsed         | 12402     |\n",
      "|    total_timesteps      | 862208    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00591   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.05e+09  |\n",
      "|    n_updates            | 4200      |\n",
      "|    policy_gradient_loss | -3.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.61e+09  |\n",
      "---------------------------------------\n",
      "Episode 346  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 862845\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 422       |\n",
      "|    time_elapsed         | 12429     |\n",
      "|    total_timesteps      | 864256    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0114    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.68e+09  |\n",
      "|    n_updates            | 4210      |\n",
      "|    policy_gradient_loss | -5e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.82e+09  |\n",
      "---------------------------------------\n",
      "Episode 347  finished with cumulative reward: -3416000.0 and \n",
      "with an average reward of: -1365.8536585365853\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 865346\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1366.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 423       |\n",
      "|    time_elapsed         | 12457     |\n",
      "|    total_timesteps      | 866304    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.011     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.87e+09  |\n",
      "|    n_updates            | 4220      |\n",
      "|    policy_gradient_loss | -4.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 348  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 867847\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.54e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 424          |\n",
      "|    time_elapsed         | 12484        |\n",
      "|    total_timesteps      | 868352       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0128       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.19e+09     |\n",
      "|    n_updates            | 4230         |\n",
      "|    policy_gradient_loss | -7.49e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.96e+09     |\n",
      "------------------------------------------\n",
      "Episode 349  finished with cumulative reward: -7725500.0 and \n",
      "with an average reward of: -3088.9644142343063\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 870348\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3090.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 425       |\n",
      "|    time_elapsed         | 12511     |\n",
      "|    total_timesteps      | 870400    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0103    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.13e+09  |\n",
      "|    n_updates            | 4240      |\n",
      "|    policy_gradient_loss | -3.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.33e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 426       |\n",
      "|    time_elapsed         | 12532     |\n",
      "|    total_timesteps      | 872448    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00961   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+10  |\n",
      "|    n_updates            | 4250      |\n",
      "|    policy_gradient_loss | -3.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.98e+10  |\n",
      "---------------------------------------\n",
      "Episode 350  finished with cumulative reward: -7419500.0 and \n",
      "with an average reward of: -2966.613354658137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 872849\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2967.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 427       |\n",
      "|    time_elapsed         | 12559     |\n",
      "|    total_timesteps      | 874496    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0129    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.31e+09  |\n",
      "|    n_updates            | 4260      |\n",
      "|    policy_gradient_loss | -3.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 351  finished with cumulative reward: -3696500.0 and \n",
      "with an average reward of: -1478.0087964814074\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 875350\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1478.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.6e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 428          |\n",
      "|    time_elapsed         | 12587        |\n",
      "|    total_timesteps      | 876544       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0109       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.53e+09     |\n",
      "|    n_updates            | 4270         |\n",
      "|    policy_gradient_loss | -7.04e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.56e+09     |\n",
      "------------------------------------------\n",
      "Episode 352  finished with cumulative reward: -6221000.0 and \n",
      "with an average reward of: -2487.405037984806\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 877851\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2488.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 429       |\n",
      "|    time_elapsed         | 12614     |\n",
      "|    total_timesteps      | 878592    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0104    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.44e+10  |\n",
      "|    n_updates            | 4280      |\n",
      "|    policy_gradient_loss | -2.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 353  finished with cumulative reward: -3645500.0 and \n",
      "with an average reward of: -1457.6169532187125\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 880352\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1458.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 430       |\n",
      "|    time_elapsed         | 12642     |\n",
      "|    total_timesteps      | 880640    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0108    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.48e+09  |\n",
      "|    n_updates            | 4290      |\n",
      "|    policy_gradient_loss | -3.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.57e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 431           |\n",
      "|    time_elapsed         | 12663         |\n",
      "|    total_timesteps      | 882688        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0745363e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0143        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.84e+09      |\n",
      "|    n_updates            | 4300          |\n",
      "|    policy_gradient_loss | -9.57e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.16e+09      |\n",
      "-------------------------------------------\n",
      "Episode 354  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 882853\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 432       |\n",
      "|    time_elapsed         | 12690     |\n",
      "|    total_timesteps      | 884736    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0112    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.76e+09  |\n",
      "|    n_updates            | 4310      |\n",
      "|    policy_gradient_loss | -3.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.11e+09  |\n",
      "---------------------------------------\n",
      "Episode 355  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 885354\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 433       |\n",
      "|    time_elapsed         | 12718     |\n",
      "|    total_timesteps      | 886784    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0092    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.01e+09  |\n",
      "|    n_updates            | 4320      |\n",
      "|    policy_gradient_loss | -3.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.85e+09  |\n",
      "---------------------------------------\n",
      "Episode 356  finished with cumulative reward: -1809500.0 and \n",
      "with an average reward of: -723.5105957616953\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 887855\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -723.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.5e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 434       |\n",
      "|    time_elapsed         | 12746     |\n",
      "|    total_timesteps      | 888832    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.018     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28e+09  |\n",
      "|    n_updates            | 4330      |\n",
      "|    policy_gradient_loss | -3.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.17e+09  |\n",
      "---------------------------------------\n",
      "Episode 357  finished with cumulative reward: -9332000.0 and \n",
      "with an average reward of: -3731.307477009196\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 890356\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3732.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 435       |\n",
      "|    time_elapsed         | 12774     |\n",
      "|    total_timesteps      | 890880    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0133    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.89e+09  |\n",
      "|    n_updates            | 4340      |\n",
      "|    policy_gradient_loss | -6.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.45e+09  |\n",
      "---------------------------------------\n",
      "Episode 358  finished with cumulative reward: -2931500.0 and \n",
      "with an average reward of: -1172.1311475409836\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 892857\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1172.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.56e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 69            |\n",
      "|    iterations           | 436           |\n",
      "|    time_elapsed         | 12802         |\n",
      "|    total_timesteps      | 892928        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.00762       |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.47e+10      |\n",
      "|    n_updates            | 4350          |\n",
      "|    policy_gradient_loss | -7.16e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.62e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 437       |\n",
      "|    time_elapsed         | 12822     |\n",
      "|    total_timesteps      | 894976    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0107    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.22e+08  |\n",
      "|    n_updates            | 4360      |\n",
      "|    policy_gradient_loss | -4.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.74e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 359  finished with cumulative reward: -5099000.0 and \n",
      "with an average reward of: -2038.7844862055179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 895358\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2039.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.54e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 438       |\n",
      "|    time_elapsed         | 12850     |\n",
      "|    total_timesteps      | 897024    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0117    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.91e+09  |\n",
      "|    n_updates            | 4370      |\n",
      "|    policy_gradient_loss | -2.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.4e+10   |\n",
      "---------------------------------------\n",
      "Episode 360  finished with cumulative reward: -7215500.0 and \n",
      "with an average reward of: -2885.045981607357\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 897859\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2886.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 439       |\n",
      "|    time_elapsed         | 12878     |\n",
      "|    total_timesteps      | 899072    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0147    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.02e+09  |\n",
      "|    n_updates            | 4380      |\n",
      "|    policy_gradient_loss | -3.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 361  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 900360\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 440       |\n",
      "|    time_elapsed         | 12905     |\n",
      "|    total_timesteps      | 901120    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00772   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.89e+09  |\n",
      "|    n_updates            | 4390      |\n",
      "|    policy_gradient_loss | -3.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 362  finished with cumulative reward: -6960500.0 and \n",
      "with an average reward of: -2783.0867652938823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 902861\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2784.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 441       |\n",
      "|    time_elapsed         | 12932     |\n",
      "|    total_timesteps      | 903168    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00772   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.22e+09  |\n",
      "|    n_updates            | 4400      |\n",
      "|    policy_gradient_loss | -2.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.83e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 442       |\n",
      "|    time_elapsed         | 12953     |\n",
      "|    total_timesteps      | 905216    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00745   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.41e+09  |\n",
      "|    n_updates            | 4410      |\n",
      "|    policy_gradient_loss | -3.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 363  finished with cumulative reward: -4563500.0 and \n",
      "with an average reward of: -1824.670131947221\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 905362\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1825.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 443       |\n",
      "|    time_elapsed         | 12981     |\n",
      "|    total_timesteps      | 907264    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0103    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.57e+09  |\n",
      "|    n_updates            | 4420      |\n",
      "|    policy_gradient_loss | -2.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.22e+09  |\n",
      "---------------------------------------\n",
      "Episode 364  finished with cumulative reward: -8669000.0 and \n",
      "with an average reward of: -3466.2135145941625\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 907863\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3467.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 444       |\n",
      "|    time_elapsed         | 13008     |\n",
      "|    total_timesteps      | 909312    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0147    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.59e+09  |\n",
      "|    n_updates            | 4430      |\n",
      "|    policy_gradient_loss | -4.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 365  finished with cumulative reward: -713000.0 and \n",
      "with an average reward of: -285.0859656137545\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 910364\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -285.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 445       |\n",
      "|    time_elapsed         | 13035     |\n",
      "|    total_timesteps      | 911360    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.012     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.67e+09  |\n",
      "|    n_updates            | 4440      |\n",
      "|    policy_gradient_loss | -5.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.07e+10  |\n",
      "---------------------------------------\n",
      "Episode 366  finished with cumulative reward: -19557500.0 and \n",
      "with an average reward of: -7819.872051179528\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 912865\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -7823.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 446       |\n",
      "|    time_elapsed         | 13063     |\n",
      "|    total_timesteps      | 913408    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00896   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.54e+09  |\n",
      "|    n_updates            | 4450      |\n",
      "|    policy_gradient_loss | -3.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.7e+10   |\n",
      "---------------------------------------\n",
      "Episode 367  finished with cumulative reward: -4538000.0 and \n",
      "with an average reward of: -1814.4742103158737\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 915366\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1815.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 447          |\n",
      "|    time_elapsed         | 13090        |\n",
      "|    total_timesteps      | 915456       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00953      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.47e+09     |\n",
      "|    n_updates            | 4460         |\n",
      "|    policy_gradient_loss | -7.98e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.88e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 448       |\n",
      "|    time_elapsed         | 13111     |\n",
      "|    total_timesteps      | 917504    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0102    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.19e+09  |\n",
      "|    n_updates            | 4470      |\n",
      "|    policy_gradient_loss | -2.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "Episode 368  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 917867\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 69           |\n",
      "|    iterations           | 449          |\n",
      "|    time_elapsed         | 13138        |\n",
      "|    total_timesteps      | 919552       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0165       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.81e+09     |\n",
      "|    n_updates            | 4480         |\n",
      "|    policy_gradient_loss | -6.81e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.15e+09     |\n",
      "------------------------------------------\n",
      "Episode 369  finished with cumulative reward: -4461500.0 and \n",
      "with an average reward of: -1783.8864454218312\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 920368\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1784.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 69        |\n",
      "|    iterations           | 450       |\n",
      "|    time_elapsed         | 13165     |\n",
      "|    total_timesteps      | 921600    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0113    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.04e+09  |\n",
      "|    n_updates            | 4490      |\n",
      "|    policy_gradient_loss | -2.58e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 370  finished with cumulative reward: -3696500.0 and \n",
      "with an average reward of: -1478.0087964814074\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 922869\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1478.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.58e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 451          |\n",
      "|    time_elapsed         | 13193        |\n",
      "|    total_timesteps      | 923648       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0121       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.48e+09     |\n",
      "|    n_updates            | 4500         |\n",
      "|    policy_gradient_loss | -7.07e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.22e+09     |\n",
      "------------------------------------------\n",
      "Episode 371  finished with cumulative reward: -9408500.0 and \n",
      "with an average reward of: -3761.895241903239\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 925370\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3763.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 452       |\n",
      "|    time_elapsed         | 13220     |\n",
      "|    total_timesteps      | 925696    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00876   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.43e+09  |\n",
      "|    n_updates            | 4510      |\n",
      "|    policy_gradient_loss | -2.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 453       |\n",
      "|    time_elapsed         | 13240     |\n",
      "|    total_timesteps      | 927744    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0118    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.33e+09  |\n",
      "|    n_updates            | 4520      |\n",
      "|    policy_gradient_loss | -3.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.54e+10  |\n",
      "---------------------------------------\n",
      "Episode 372  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 927871\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 454       |\n",
      "|    time_elapsed         | 13268     |\n",
      "|    total_timesteps      | 929792    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0077    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.36e+10  |\n",
      "|    n_updates            | 4530      |\n",
      "|    policy_gradient_loss | -3.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 373  finished with cumulative reward: -4232000.0 and \n",
      "with an average reward of: -1692.1231507397042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 930372\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1692.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.6e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 455          |\n",
      "|    time_elapsed         | 13296        |\n",
      "|    total_timesteps      | 931840       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.00947      |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.43e+09     |\n",
      "|    n_updates            | 4540         |\n",
      "|    policy_gradient_loss | -4.81e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.04e+09     |\n",
      "------------------------------------------\n",
      "Episode 374  finished with cumulative reward: -5711000.0 and \n",
      "with an average reward of: -2283.4866053578567\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 932873\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2284.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 456       |\n",
      "|    time_elapsed         | 13323     |\n",
      "|    total_timesteps      | 933888    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0113    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.77e+09  |\n",
      "|    n_updates            | 4550      |\n",
      "|    policy_gradient_loss | -2.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 375  finished with cumulative reward: -891500.0 and \n",
      "with an average reward of: -356.45741703318674\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 935374\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -356.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.59e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 457          |\n",
      "|    time_elapsed         | 13351        |\n",
      "|    total_timesteps      | 935936       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0216       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.25e+08     |\n",
      "|    n_updates            | 4560         |\n",
      "|    policy_gradient_loss | -9.34e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.64e+09     |\n",
      "------------------------------------------\n",
      "Episode 376  finished with cumulative reward: -7011500.0 and \n",
      "with an average reward of: -2803.4786085565775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 937875\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2804.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 458          |\n",
      "|    time_elapsed         | 13379        |\n",
      "|    total_timesteps      | 937984       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0121       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.12e+09     |\n",
      "|    n_updates            | 4570         |\n",
      "|    policy_gradient_loss | -7.18e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.26e+09     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 459       |\n",
      "|    time_elapsed         | 13400     |\n",
      "|    total_timesteps      | 940032    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00954   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.53e+09  |\n",
      "|    n_updates            | 4580      |\n",
      "|    policy_gradient_loss | -3.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.06e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 377  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 940376\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 460       |\n",
      "|    time_elapsed         | 13428     |\n",
      "|    total_timesteps      | 942080    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0108    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.6e+09   |\n",
      "|    n_updates            | 4590      |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.83e+09  |\n",
      "---------------------------------------\n",
      "Episode 378  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 942877\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 461       |\n",
      "|    time_elapsed         | 13456     |\n",
      "|    total_timesteps      | 944128    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00905   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.47e+09  |\n",
      "|    n_updates            | 4600      |\n",
      "|    policy_gradient_loss | -3.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 379  finished with cumulative reward: -4946000.0 and \n",
      "with an average reward of: -1977.608956417433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 945378\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1978.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 462       |\n",
      "|    time_elapsed         | 13483     |\n",
      "|    total_timesteps      | 946176    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0121    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.28e+09  |\n",
      "|    n_updates            | 4610      |\n",
      "|    policy_gradient_loss | -3.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 380  finished with cumulative reward: -101000.0 and \n",
      "with an average reward of: -40.38384646141543\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 947879\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -40.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 463       |\n",
      "|    time_elapsed         | 13510     |\n",
      "|    total_timesteps      | 948224    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00771   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.91e+09  |\n",
      "|    n_updates            | 4620      |\n",
      "|    policy_gradient_loss | -2.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 464       |\n",
      "|    time_elapsed         | 13531     |\n",
      "|    total_timesteps      | 950272    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0102    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.96e+09  |\n",
      "|    n_updates            | 4630      |\n",
      "|    policy_gradient_loss | -3.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 381  finished with cumulative reward: -5889500.0 and \n",
      "with an average reward of: -2354.858056777289\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 950380\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2355.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 465       |\n",
      "|    time_elapsed         | 13559     |\n",
      "|    total_timesteps      | 952320    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0125    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.36e+09  |\n",
      "|    n_updates            | 4640      |\n",
      "|    policy_gradient_loss | -2.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.64e+09  |\n",
      "---------------------------------------\n",
      "Episode 382  finished with cumulative reward: -4563500.0 and \n",
      "with an average reward of: -1824.670131947221\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 952881\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1825.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 466       |\n",
      "|    time_elapsed         | 13586     |\n",
      "|    total_timesteps      | 954368    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.011     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.96e+09  |\n",
      "|    n_updates            | 4650      |\n",
      "|    policy_gradient_loss | -2.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.29e+10  |\n",
      "---------------------------------------\n",
      "Episode 383  finished with cumulative reward: -1478000.0 and \n",
      "with an average reward of: -590.9636145541783\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 955382\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -591.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 467       |\n",
      "|    time_elapsed         | 13614     |\n",
      "|    total_timesteps      | 956416    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0156    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.84e+08  |\n",
      "|    n_updates            | 4660      |\n",
      "|    policy_gradient_loss | -3.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 384  finished with cumulative reward: -5328500.0 and \n",
      "with an average reward of: -2130.547780887645\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 957883\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2131.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 468       |\n",
      "|    time_elapsed         | 13642     |\n",
      "|    total_timesteps      | 958464    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0177    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.39e+09  |\n",
      "|    n_updates            | 4670      |\n",
      "|    policy_gradient_loss | -2.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.88e+09  |\n",
      "---------------------------------------\n",
      "Episode 385  finished with cumulative reward: -11958500.0 and \n",
      "with an average reward of: -4781.487405037985\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 960384\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4783.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 469       |\n",
      "|    time_elapsed         | 13670     |\n",
      "|    total_timesteps      | 960512    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00991   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.33e+09  |\n",
      "|    n_updates            | 4680      |\n",
      "|    policy_gradient_loss | -2.98e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.47e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 470       |\n",
      "|    time_elapsed         | 13691     |\n",
      "|    total_timesteps      | 962560    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00899   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.36e+10  |\n",
      "|    n_updates            | 4690      |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.74e+10  |\n",
      "---------------------------------------\n",
      "Episode 386  finished with cumulative reward: -2294000.0 and \n",
      "with an average reward of: -917.233106757297\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 962885\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -917.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 471       |\n",
      "|    time_elapsed         | 13719     |\n",
      "|    total_timesteps      | 964608    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0145    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.57e+09  |\n",
      "|    n_updates            | 4700      |\n",
      "|    policy_gradient_loss | -4.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.45e+09  |\n",
      "---------------------------------------\n",
      "Episode 387  finished with cumulative reward: -4334000.0 and \n",
      "with an average reward of: -1732.906837265094\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 965386\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1733.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 472       |\n",
      "|    time_elapsed         | 13746     |\n",
      "|    total_timesteps      | 966656    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0159    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.56e+09  |\n",
      "|    n_updates            | 4710      |\n",
      "|    policy_gradient_loss | -5.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.53e+09  |\n",
      "---------------------------------------\n",
      "Episode 388  finished with cumulative reward: -3390500.0 and \n",
      "with an average reward of: -1355.657736905238\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 967887\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1356.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.47e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 473          |\n",
      "|    time_elapsed         | 13774        |\n",
      "|    total_timesteps      | 968704       |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0137       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.6e+09      |\n",
      "|    n_updates            | 4720         |\n",
      "|    policy_gradient_loss | -6.94e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.24e+09     |\n",
      "------------------------------------------\n",
      "Episode 389  finished with cumulative reward: -4206500.0 and \n",
      "with an average reward of: -1681.9272291083566\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 970388\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1682.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 474       |\n",
      "|    time_elapsed         | 13802     |\n",
      "|    total_timesteps      | 970752    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0113    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.82e+09  |\n",
      "|    n_updates            | 4730      |\n",
      "|    policy_gradient_loss | -2.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 475       |\n",
      "|    time_elapsed         | 13823     |\n",
      "|    total_timesteps      | 972800    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0106    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.81e+09  |\n",
      "|    n_updates            | 4740      |\n",
      "|    policy_gradient_loss | -3.68e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 390  finished with cumulative reward: -585500.0 and \n",
      "with an average reward of: -234.1063574570172\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 972889\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -234.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 476       |\n",
      "|    time_elapsed         | 13851     |\n",
      "|    total_timesteps      | 974848    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0151    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.75e+09  |\n",
      "|    n_updates            | 4750      |\n",
      "|    policy_gradient_loss | -3.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.71e+09  |\n",
      "---------------------------------------\n",
      "Episode 391  finished with cumulative reward: -3441500.0 and \n",
      "with an average reward of: -1376.0495801679328\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 975390\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1376.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 477       |\n",
      "|    time_elapsed         | 13879     |\n",
      "|    total_timesteps      | 976896    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00857   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.94e+09  |\n",
      "|    n_updates            | 4760      |\n",
      "|    policy_gradient_loss | -3.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.51e+09  |\n",
      "---------------------------------------\n",
      "Episode 392  finished with cumulative reward: -4946000.0 and \n",
      "with an average reward of: -1977.608956417433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 977891\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1978.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 478       |\n",
      "|    time_elapsed         | 13907     |\n",
      "|    total_timesteps      | 978944    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.015     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.2e+09   |\n",
      "|    n_updates            | 4770      |\n",
      "|    policy_gradient_loss | -4.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.29e+09  |\n",
      "---------------------------------------\n",
      "Episode 393  finished with cumulative reward: -2319500.0 and \n",
      "with an average reward of: -927.4290283886445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 980392\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -927.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 479       |\n",
      "|    time_elapsed         | 13935     |\n",
      "|    total_timesteps      | 980992    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0149    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.35e+09  |\n",
      "|    n_updates            | 4780      |\n",
      "|    policy_gradient_loss | -3.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.75e+09  |\n",
      "---------------------------------------\n",
      "Episode 394  finished with cumulative reward: -5762000.0 and \n",
      "with an average reward of: -2303.878448620552\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 982893\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2304.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 480       |\n",
      "|    time_elapsed         | 13963     |\n",
      "|    total_timesteps      | 983040    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0171    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.75e+09  |\n",
      "|    n_updates            | 4790      |\n",
      "|    policy_gradient_loss | -1.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.85e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 481       |\n",
      "|    time_elapsed         | 13983     |\n",
      "|    total_timesteps      | 985088    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0113    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.94e+09  |\n",
      "|    n_updates            | 4800      |\n",
      "|    policy_gradient_loss | -3.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 395  finished with cumulative reward: -3212000.0 and \n",
      "with an average reward of: -1284.2862854858056\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 985394\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1284.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.36e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 482           |\n",
      "|    time_elapsed         | 14011         |\n",
      "|    total_timesteps      | 987136        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0162        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4e+07         |\n",
      "|    n_updates            | 4810          |\n",
      "|    policy_gradient_loss | -6.51e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.99e+09      |\n",
      "-------------------------------------------\n",
      "Episode 396  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 987895\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.36e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 70            |\n",
      "|    iterations           | 483           |\n",
      "|    time_elapsed         | 14039         |\n",
      "|    total_timesteps      | 989184        |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0149        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.98e+09      |\n",
      "|    n_updates            | 4820          |\n",
      "|    policy_gradient_loss | -1.16e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.77e+10      |\n",
      "-------------------------------------------\n",
      "Episode 397  finished with cumulative reward: -11576000.0 and \n",
      "with an average reward of: -4628.548580567773\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 990396\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4630.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 484       |\n",
      "|    time_elapsed         | 14067     |\n",
      "|    total_timesteps      | 991232    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0125    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.37e+10  |\n",
      "|    n_updates            | 4830      |\n",
      "|    policy_gradient_loss | -4.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.77e+10  |\n",
      "---------------------------------------\n",
      "Episode 398  finished with cumulative reward: -2931500.0 and \n",
      "with an average reward of: -1172.1311475409836\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 992897\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1172.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.37e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 485       |\n",
      "|    time_elapsed         | 14095     |\n",
      "|    total_timesteps      | 993280    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0113    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.35e+09  |\n",
      "|    n_updates            | 4840      |\n",
      "|    policy_gradient_loss | -2.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.37e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 486       |\n",
      "|    time_elapsed         | 14115     |\n",
      "|    total_timesteps      | 995328    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0119    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.86e+09  |\n",
      "|    n_updates            | 4850      |\n",
      "|    policy_gradient_loss | -2.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.77e+09  |\n",
      "---------------------------------------\n",
      "Episode 399  finished with cumulative reward: -5201000.0 and \n",
      "with an average reward of: -2079.5681727309075\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 995398\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2080.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 487       |\n",
      "|    time_elapsed         | 14143     |\n",
      "|    total_timesteps      | 997376    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0109    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.01e+09  |\n",
      "|    n_updates            | 4860      |\n",
      "|    policy_gradient_loss | -2.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.52e+09  |\n",
      "---------------------------------------\n",
      "Episode 400  finished with cumulative reward: -7878500.0 and \n",
      "with an average reward of: -3150.139944022391\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 997899\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3151.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 488       |\n",
      "|    time_elapsed         | 14172     |\n",
      "|    total_timesteps      | 999424    |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.013     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.05e+09  |\n",
      "|    n_updates            | 4870      |\n",
      "|    policy_gradient_loss | -4.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 401  finished with cumulative reward: -9969500.0 and \n",
      "with an average reward of: -3986.205517792883\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1000400\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3987.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 489       |\n",
      "|    time_elapsed         | 14200     |\n",
      "|    total_timesteps      | 1001472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0122    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.43e+10  |\n",
      "|    n_updates            | 4880      |\n",
      "|    policy_gradient_loss | -2.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.61e+10  |\n",
      "---------------------------------------\n",
      "Episode 402  finished with cumulative reward: -5838500.0 and \n",
      "with an average reward of: -2334.466213514594\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1002901\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2335.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.49e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 490       |\n",
      "|    time_elapsed         | 14228     |\n",
      "|    total_timesteps      | 1003520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0118    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.2e+10   |\n",
      "|    n_updates            | 4890      |\n",
      "|    policy_gradient_loss | -2.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.27e+10  |\n",
      "---------------------------------------\n",
      "Episode 403  finished with cumulative reward: -2268500.0 and \n",
      "with an average reward of: -907.0371851259496\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1005402\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -907.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 491       |\n",
      "|    time_elapsed         | 14257     |\n",
      "|    total_timesteps      | 1005568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0178    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.71e+09  |\n",
      "|    n_updates            | 4900      |\n",
      "|    policy_gradient_loss | -4.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.63e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 492       |\n",
      "|    time_elapsed         | 14279     |\n",
      "|    total_timesteps      | 1007616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0149    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.82e+09  |\n",
      "|    n_updates            | 4910      |\n",
      "|    policy_gradient_loss | -3.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.61e+09  |\n",
      "---------------------------------------\n",
      "Episode 404  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1007903\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 493       |\n",
      "|    time_elapsed         | 14307     |\n",
      "|    total_timesteps      | 1009664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0143    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.57e+09  |\n",
      "|    n_updates            | 4920      |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 405  finished with cumulative reward: -2574500.0 and \n",
      "with an average reward of: -1029.3882447021192\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1010404\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1029.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 494       |\n",
      "|    time_elapsed         | 14336     |\n",
      "|    total_timesteps      | 1011712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0135    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.57e+09  |\n",
      "|    n_updates            | 4930      |\n",
      "|    policy_gradient_loss | -1.5e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 406  finished with cumulative reward: -8516000.0 and \n",
      "with an average reward of: -3405.0379848060775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1012905\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3406.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.49e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 495          |\n",
      "|    time_elapsed         | 14364        |\n",
      "|    total_timesteps      | 1013760      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0283       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.24e+08     |\n",
      "|    n_updates            | 4940         |\n",
      "|    policy_gradient_loss | -4.41e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.92e+08     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 407  finished with cumulative reward: -6680000.0 and \n",
      "with an average reward of: -2670.9316273490604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1015406\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2672.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 496       |\n",
      "|    time_elapsed         | 14392     |\n",
      "|    total_timesteps      | 1015808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0117    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.65e+10  |\n",
      "|    n_updates            | 4950      |\n",
      "|    policy_gradient_loss | -2.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.56e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 497       |\n",
      "|    time_elapsed         | 14413     |\n",
      "|    total_timesteps      | 1017856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.011     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.97e+09  |\n",
      "|    n_updates            | 4960      |\n",
      "|    policy_gradient_loss | -2.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 408  finished with cumulative reward: -6221000.0 and \n",
      "with an average reward of: -2487.405037984806\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1017907\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2488.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 498       |\n",
      "|    time_elapsed         | 14442     |\n",
      "|    total_timesteps      | 1019904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0145    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.99e+09  |\n",
      "|    n_updates            | 4970      |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.5e+10   |\n",
      "---------------------------------------\n",
      "Episode 409  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1020408\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.49e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 499       |\n",
      "|    time_elapsed         | 14470     |\n",
      "|    total_timesteps      | 1021952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0123    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.27e+09  |\n",
      "|    n_updates            | 4980      |\n",
      "|    policy_gradient_loss | -3.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.71e+09  |\n",
      "---------------------------------------\n",
      "Episode 410  finished with cumulative reward: -5099000.0 and \n",
      "with an average reward of: -2038.7844862055179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1022909\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2039.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 500       |\n",
      "|    time_elapsed         | 14498     |\n",
      "|    total_timesteps      | 1024000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0127    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.17e+09  |\n",
      "|    n_updates            | 4990      |\n",
      "|    policy_gradient_loss | -2.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 411  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1025410\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 501       |\n",
      "|    time_elapsed         | 14525     |\n",
      "|    total_timesteps      | 1026048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0133    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.35e+09  |\n",
      "|    n_updates            | 5000      |\n",
      "|    policy_gradient_loss | -2.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.04e+09  |\n",
      "---------------------------------------\n",
      "Episode 412  finished with cumulative reward: -7853000.0 and \n",
      "with an average reward of: -3139.9440223910437\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1027911\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3141.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 502       |\n",
      "|    time_elapsed         | 14552     |\n",
      "|    total_timesteps      | 1028096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0113    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.54e+09  |\n",
      "|    n_updates            | 5010      |\n",
      "|    policy_gradient_loss | -3.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.89e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 503       |\n",
      "|    time_elapsed         | 14573     |\n",
      "|    total_timesteps      | 1030144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0119    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.21e+10  |\n",
      "|    n_updates            | 5020      |\n",
      "|    policy_gradient_loss | -3.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.85e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 413  finished with cumulative reward: -6603500.0 and \n",
      "with an average reward of: -2640.343862455018\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1030412\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2641.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 504       |\n",
      "|    time_elapsed         | 14601     |\n",
      "|    total_timesteps      | 1032192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00982   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.31e+09  |\n",
      "|    n_updates            | 5030      |\n",
      "|    policy_gradient_loss | -4.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 414  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1032913\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 505       |\n",
      "|    time_elapsed         | 14628     |\n",
      "|    total_timesteps      | 1034240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00989   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+10  |\n",
      "|    n_updates            | 5040      |\n",
      "|    policy_gradient_loss | -2.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.82e+10  |\n",
      "---------------------------------------\n",
      "Episode 415  finished with cumulative reward: -8949500.0 and \n",
      "with an average reward of: -3578.3686525389844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1035414\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3579.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 506       |\n",
      "|    time_elapsed         | 14656     |\n",
      "|    total_timesteps      | 1036288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0125    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.93e+09  |\n",
      "|    n_updates            | 5050      |\n",
      "|    policy_gradient_loss | -1.57e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.8e+09   |\n",
      "---------------------------------------\n",
      "Episode 416  finished with cumulative reward: -254000.0 and \n",
      "with an average reward of: -101.5593762495002\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1037915\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -101.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 507       |\n",
      "|    time_elapsed         | 14683     |\n",
      "|    total_timesteps      | 1038336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0157    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.35e+09  |\n",
      "|    n_updates            | 5060      |\n",
      "|    policy_gradient_loss | -2.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.47e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 508          |\n",
      "|    time_elapsed         | 14704        |\n",
      "|    total_timesteps      | 1040384      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0376       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+08     |\n",
      "|    n_updates            | 5070         |\n",
      "|    policy_gradient_loss | -5.52e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.97e+08     |\n",
      "------------------------------------------\n",
      "Episode 417  finished with cumulative reward: -3849500.0 and \n",
      "with an average reward of: -1539.1843262694922\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1040416\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1539.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 509       |\n",
      "|    time_elapsed         | 14732     |\n",
      "|    total_timesteps      | 1042432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0132    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.86e+09  |\n",
      "|    n_updates            | 5080      |\n",
      "|    policy_gradient_loss | -2.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.58e+09  |\n",
      "---------------------------------------\n",
      "Episode 418  finished with cumulative reward: -4742000.0 and \n",
      "with an average reward of: -1896.0415833666534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1042917\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1896.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 510       |\n",
      "|    time_elapsed         | 14760     |\n",
      "|    total_timesteps      | 1044480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0083    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.09e+09  |\n",
      "|    n_updates            | 5090      |\n",
      "|    policy_gradient_loss | -2.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.33e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 419  finished with cumulative reward: -7878500.0 and \n",
      "with an average reward of: -3150.139944022391\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1045418\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3151.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 511       |\n",
      "|    time_elapsed         | 14787     |\n",
      "|    total_timesteps      | 1046528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0113    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 5100      |\n",
      "|    policy_gradient_loss | -1.58e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 420  finished with cumulative reward: -7011500.0 and \n",
      "with an average reward of: -2803.4786085565775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1047919\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2804.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 512       |\n",
      "|    time_elapsed         | 14816     |\n",
      "|    total_timesteps      | 1048576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0149    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.09e+09  |\n",
      "|    n_updates            | 5110      |\n",
      "|    policy_gradient_loss | -2.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 421  finished with cumulative reward: -7955000.0 and \n",
      "with an average reward of: -3180.7277089164336\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1050420\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3182.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.5e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 70       |\n",
      "|    iterations           | 513      |\n",
      "|    time_elapsed         | 14844    |\n",
      "|    total_timesteps      | 1050624  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.012    |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 4.89e+09 |\n",
      "|    n_updates            | 5120     |\n",
      "|    policy_gradient_loss | -2.9e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.42e+10 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.5e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 514       |\n",
      "|    time_elapsed         | 14864     |\n",
      "|    total_timesteps      | 1052672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00941   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.43e+09  |\n",
      "|    n_updates            | 5130      |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.94e+10  |\n",
      "---------------------------------------\n",
      "Episode 422  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1052921\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 515       |\n",
      "|    time_elapsed         | 14892     |\n",
      "|    total_timesteps      | 1054720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0149    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.73e+09  |\n",
      "|    n_updates            | 5140      |\n",
      "|    policy_gradient_loss | -4.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.82e+09  |\n",
      "---------------------------------------\n",
      "Episode 423  finished with cumulative reward: -6986000.0 and \n",
      "with an average reward of: -2793.28268692523\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1055422\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2794.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 516       |\n",
      "|    time_elapsed         | 14920     |\n",
      "|    total_timesteps      | 1056768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0155    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.7e+09   |\n",
      "|    n_updates            | 5150      |\n",
      "|    policy_gradient_loss | -3.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.35e+09  |\n",
      "---------------------------------------\n",
      "Episode 424  finished with cumulative reward: -4691000.0 and \n",
      "with an average reward of: -1875.6497401039585\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1057923\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1876.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 517       |\n",
      "|    time_elapsed         | 14947     |\n",
      "|    total_timesteps      | 1058816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0122    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.89e+09  |\n",
      "|    n_updates            | 5160      |\n",
      "|    policy_gradient_loss | -3.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 425  finished with cumulative reward: -1707500.0 and \n",
      "with an average reward of: -682.7269092363055\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1060424\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -683.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 518       |\n",
      "|    time_elapsed         | 14976     |\n",
      "|    total_timesteps      | 1060864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0138    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.21e+09  |\n",
      "|    n_updates            | 5170      |\n",
      "|    policy_gradient_loss | -2.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 519       |\n",
      "|    time_elapsed         | 14996     |\n",
      "|    total_timesteps      | 1062912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0133    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.13e+09  |\n",
      "|    n_updates            | 5180      |\n",
      "|    policy_gradient_loss | -4.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.01e+09  |\n",
      "---------------------------------------\n",
      "Episode 426  finished with cumulative reward: -7623500.0 and \n",
      "with an average reward of: -3048.1807277089165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1062925\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3049.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 520       |\n",
      "|    time_elapsed         | 15025     |\n",
      "|    total_timesteps      | 1064960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0106    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 5190      |\n",
      "|    policy_gradient_loss | -1.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.88e+10  |\n",
      "---------------------------------------\n",
      "Episode 427  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1065426\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 521       |\n",
      "|    time_elapsed         | 15052     |\n",
      "|    total_timesteps      | 1067008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0143    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.65e+09  |\n",
      "|    n_updates            | 5200      |\n",
      "|    policy_gradient_loss | -1.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.39e+09  |\n",
      "---------------------------------------\n",
      "Episode 428  finished with cumulative reward: -10097000.0 and \n",
      "with an average reward of: -4037.18512594962\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1067927\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4038.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 522       |\n",
      "|    time_elapsed         | 15080     |\n",
      "|    total_timesteps      | 1069056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00967   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.36e+09  |\n",
      "|    n_updates            | 5210      |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 429  finished with cumulative reward: -4767500.0 and \n",
      "with an average reward of: -1906.2375049980008\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1070428\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1907.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 523       |\n",
      "|    time_elapsed         | 15108     |\n",
      "|    total_timesteps      | 1071104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0137    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.95e+09  |\n",
      "|    n_updates            | 5220      |\n",
      "|    policy_gradient_loss | -5.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 430  finished with cumulative reward: -3747500.0 and \n",
      "with an average reward of: -1498.4006397441024\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1072929\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1499.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.36e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 70           |\n",
      "|    iterations           | 524          |\n",
      "|    time_elapsed         | 15137        |\n",
      "|    total_timesteps      | 1073152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0151       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.92e+09     |\n",
      "|    n_updates            | 5230         |\n",
      "|    policy_gradient_loss | -6.34e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.01e+09     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 525       |\n",
      "|    time_elapsed         | 15157     |\n",
      "|    total_timesteps      | 1075200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0126    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.32e+09  |\n",
      "|    n_updates            | 5240      |\n",
      "|    policy_gradient_loss | -2.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 431  finished with cumulative reward: -11244500.0 and \n",
      "with an average reward of: -4496.001599360256\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1075430\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4497.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 526       |\n",
      "|    time_elapsed         | 15185     |\n",
      "|    total_timesteps      | 1077248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.011     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.75e+09  |\n",
      "|    n_updates            | 5250      |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.59e+10  |\n",
      "---------------------------------------\n",
      "Episode 432  finished with cumulative reward: -7266500.0 and \n",
      "with an average reward of: -2905.437824870052\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1077931\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2906.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 527       |\n",
      "|    time_elapsed         | 15213     |\n",
      "|    total_timesteps      | 1079296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0135    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.17e+09  |\n",
      "|    n_updates            | 5260      |\n",
      "|    policy_gradient_loss | -2.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 433  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1080432\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 528       |\n",
      "|    time_elapsed         | 15241     |\n",
      "|    total_timesteps      | 1081344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0133    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.93e+09  |\n",
      "|    n_updates            | 5270      |\n",
      "|    policy_gradient_loss | -2.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 434  finished with cumulative reward: -7496000.0 and \n",
      "with an average reward of: -2997.201119552179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1082933\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2998.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 529       |\n",
      "|    time_elapsed         | 15269     |\n",
      "|    total_timesteps      | 1083392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0141    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.9e+09   |\n",
      "|    n_updates            | 5280      |\n",
      "|    policy_gradient_loss | -3e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "Episode 435  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1085434\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.38e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 530       |\n",
      "|    time_elapsed         | 15297     |\n",
      "|    total_timesteps      | 1085440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0127    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.32e+09  |\n",
      "|    n_updates            | 5290      |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.79e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.38e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 531       |\n",
      "|    time_elapsed         | 15317     |\n",
      "|    total_timesteps      | 1087488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0132    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.3e+09   |\n",
      "|    n_updates            | 5300      |\n",
      "|    policy_gradient_loss | -4.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.91e+09  |\n",
      "---------------------------------------\n",
      "Episode 436  finished with cumulative reward: -5787500.0 and \n",
      "with an average reward of: -2314.0743702518994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1087935\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2315.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.38e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 70        |\n",
      "|    iterations           | 532       |\n",
      "|    time_elapsed         | 15345     |\n",
      "|    total_timesteps      | 1089536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0158    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.79e+09  |\n",
      "|    n_updates            | 5310      |\n",
      "|    policy_gradient_loss | -1.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.64e+09  |\n",
      "---------------------------------------\n",
      "Episode 437  finished with cumulative reward: -3900500.0 and \n",
      "with an average reward of: -1559.5761695321871\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1090436\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1560.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 533       |\n",
      "|    time_elapsed         | 15373     |\n",
      "|    total_timesteps      | 1091584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0152    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.45e+09  |\n",
      "|    n_updates            | 5320      |\n",
      "|    policy_gradient_loss | -3.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 438  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1092937\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 534       |\n",
      "|    time_elapsed         | 15401     |\n",
      "|    total_timesteps      | 1093632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.011     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.93e+09  |\n",
      "|    n_updates            | 5330      |\n",
      "|    policy_gradient_loss | -2.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n",
      "Episode 439  finished with cumulative reward: -7674500.0 and \n",
      "with an average reward of: -3068.572570971611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1095438\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3069.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 535       |\n",
      "|    time_elapsed         | 15428     |\n",
      "|    total_timesteps      | 1095680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0145    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.14e+10  |\n",
      "|    n_updates            | 5340      |\n",
      "|    policy_gradient_loss | -1.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 536       |\n",
      "|    time_elapsed         | 15449     |\n",
      "|    total_timesteps      | 1097728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0107    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.59e+09  |\n",
      "|    n_updates            | 5350      |\n",
      "|    policy_gradient_loss | -2.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 440  finished with cumulative reward: -7190000.0 and \n",
      "with an average reward of: -2874.8500599760096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1097939\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2876.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 537       |\n",
      "|    time_elapsed         | 15477     |\n",
      "|    total_timesteps      | 1099776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0156    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.56e+09  |\n",
      "|    n_updates            | 5360      |\n",
      "|    policy_gradient_loss | -2.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.04e+09  |\n",
      "---------------------------------------\n",
      "Episode 441  finished with cumulative reward: -2651000.0 and \n",
      "with an average reward of: -1059.9760095961615\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1100440\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1060.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.45e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 538       |\n",
      "|    time_elapsed         | 15505     |\n",
      "|    total_timesteps      | 1101824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.012     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.68e+09  |\n",
      "|    n_updates            | 5370      |\n",
      "|    policy_gradient_loss | -2.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.26e+10  |\n",
      "---------------------------------------\n",
      "Episode 442  finished with cumulative reward: -5481500.0 and \n",
      "with an average reward of: -2191.72331067573\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1102941\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2192.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.4e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 539          |\n",
      "|    time_elapsed         | 15533        |\n",
      "|    total_timesteps      | 1103872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0156       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.08e+10     |\n",
      "|    n_updates            | 5380         |\n",
      "|    policy_gradient_loss | -5.93e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.35e+09     |\n",
      "------------------------------------------\n",
      "Episode 443  finished with cumulative reward: -1988000.0 and \n",
      "with an average reward of: -794.8820471811275\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1105442\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -795.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 540       |\n",
      "|    time_elapsed         | 15561     |\n",
      "|    total_timesteps      | 1105920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0104    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.14e+09  |\n",
      "|    n_updates            | 5390      |\n",
      "|    policy_gradient_loss | -2.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.36e+09  |\n",
      "---------------------------------------\n",
      "Episode 444  finished with cumulative reward: -2651000.0 and \n",
      "with an average reward of: -1059.9760095961615\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1107943\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1060.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 541       |\n",
      "|    time_elapsed         | 15589     |\n",
      "|    total_timesteps      | 1107968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0271    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.01e+09  |\n",
      "|    n_updates            | 5400      |\n",
      "|    policy_gradient_loss | -2.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.4e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 542       |\n",
      "|    time_elapsed         | 15610     |\n",
      "|    total_timesteps      | 1110016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0142    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.17e+09  |\n",
      "|    n_updates            | 5410      |\n",
      "|    policy_gradient_loss | -3.68e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.86e+09  |\n",
      "---------------------------------------\n",
      "Episode 445  finished with cumulative reward: -1580000.0 and \n",
      "with an average reward of: -631.7473010795682\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1110444\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -632.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.33e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 543       |\n",
      "|    time_elapsed         | 15638     |\n",
      "|    total_timesteps      | 1112064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0157    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.35e+08  |\n",
      "|    n_updates            | 5420      |\n",
      "|    policy_gradient_loss | -2.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.97e+09  |\n",
      "---------------------------------------\n",
      "Episode 446  finished with cumulative reward: -7470500.0 and \n",
      "with an average reward of: -2987.0051979208315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1112945\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2988.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 544       |\n",
      "|    time_elapsed         | 15666     |\n",
      "|    total_timesteps      | 1114112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0147    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.75e+09  |\n",
      "|    n_updates            | 5430      |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.29e+10  |\n",
      "---------------------------------------\n",
      "Episode 447  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1115446\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 545       |\n",
      "|    time_elapsed         | 15694     |\n",
      "|    total_timesteps      | 1116160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0117    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.31e+09  |\n",
      "|    n_updates            | 5440      |\n",
      "|    policy_gradient_loss | -2.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "Episode 448  finished with cumulative reward: -8720000.0 and \n",
      "with an average reward of: -3486.605357856857\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1117947\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3488.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 546       |\n",
      "|    time_elapsed         | 15722     |\n",
      "|    total_timesteps      | 1118208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0133    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.09e+10  |\n",
      "|    n_updates            | 5450      |\n",
      "|    policy_gradient_loss | -2.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.51e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 547       |\n",
      "|    time_elapsed         | 15742     |\n",
      "|    total_timesteps      | 1120256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0127    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.13e+09  |\n",
      "|    n_updates            | 5460      |\n",
      "|    policy_gradient_loss | -2.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.44e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 449  finished with cumulative reward: -14559500.0 and \n",
      "with an average reward of: -5821.471411435426\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1120448\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5823.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 548       |\n",
      "|    time_elapsed         | 15771     |\n",
      "|    total_timesteps      | 1122304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0104    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+10  |\n",
      "|    n_updates            | 5470      |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 450  finished with cumulative reward: -6476000.0 and \n",
      "with an average reward of: -2589.3642542982807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1122949\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2590.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 549       |\n",
      "|    time_elapsed         | 15799     |\n",
      "|    total_timesteps      | 1124352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.014     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.66e+09  |\n",
      "|    n_updates            | 5480      |\n",
      "|    policy_gradient_loss | -2.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 451  finished with cumulative reward: -9255500.0 and \n",
      "with an average reward of: -3700.719712115154\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1125450\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3702.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 550       |\n",
      "|    time_elapsed         | 15827     |\n",
      "|    total_timesteps      | 1126400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0139    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.48e+09  |\n",
      "|    n_updates            | 5490      |\n",
      "|    policy_gradient_loss | -2.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 452  finished with cumulative reward: -2931500.0 and \n",
      "with an average reward of: -1172.1311475409836\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1127951\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1172.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 551       |\n",
      "|    time_elapsed         | 15855     |\n",
      "|    total_timesteps      | 1128448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.012     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.02e+09  |\n",
      "|    n_updates            | 5500      |\n",
      "|    policy_gradient_loss | -4.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.91e+09  |\n",
      "---------------------------------------\n",
      "Episode 453  finished with cumulative reward: -6501500.0 and \n",
      "with an average reward of: -2599.5601759296283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1130452\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2600.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 552       |\n",
      "|    time_elapsed         | 15883     |\n",
      "|    total_timesteps      | 1130496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.017     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.5e+09   |\n",
      "|    n_updates            | 5510      |\n",
      "|    policy_gradient_loss | -2.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.22e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 553       |\n",
      "|    time_elapsed         | 15904     |\n",
      "|    total_timesteps      | 1132544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0138    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.91e+09  |\n",
      "|    n_updates            | 5520      |\n",
      "|    policy_gradient_loss | -3.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n",
      "Episode 454  finished with cumulative reward: -4385000.0 and \n",
      "with an average reward of: -1753.298680527789\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1132953\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1754.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 554       |\n",
      "|    time_elapsed         | 15932     |\n",
      "|    total_timesteps      | 1134592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0142    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.19e+09  |\n",
      "|    n_updates            | 5530      |\n",
      "|    policy_gradient_loss | -2.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.59e+09  |\n",
      "---------------------------------------\n",
      "Episode 455  finished with cumulative reward: -6629000.0 and \n",
      "with an average reward of: -2650.5397840863657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1135454\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2651.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 555       |\n",
      "|    time_elapsed         | 15960     |\n",
      "|    total_timesteps      | 1136640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0137    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.78e+09  |\n",
      "|    n_updates            | 5540      |\n",
      "|    policy_gradient_loss | -3.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.07e+09  |\n",
      "---------------------------------------\n",
      "Episode 456  finished with cumulative reward: -5583500.0 and \n",
      "with an average reward of: -2232.5069972011197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1137955\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2233.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 556       |\n",
      "|    time_elapsed         | 15988     |\n",
      "|    total_timesteps      | 1138688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0144    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.06e+10  |\n",
      "|    n_updates            | 5550      |\n",
      "|    policy_gradient_loss | -2.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 457  finished with cumulative reward: -5150000.0 and \n",
      "with an average reward of: -2059.1763294682128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1140456\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2060.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 557       |\n",
      "|    time_elapsed         | 16015     |\n",
      "|    total_timesteps      | 1140736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0158    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.74e+09  |\n",
      "|    n_updates            | 5560      |\n",
      "|    policy_gradient_loss | -2.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 558       |\n",
      "|    time_elapsed         | 16036     |\n",
      "|    total_timesteps      | 1142784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0103    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.37e+09  |\n",
      "|    n_updates            | 5570      |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7e+09     |\n",
      "---------------------------------------\n",
      "Episode 458  finished with cumulative reward: -5889500.0 and \n",
      "with an average reward of: -2354.858056777289\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1142957\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2355.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 559       |\n",
      "|    time_elapsed         | 16064     |\n",
      "|    total_timesteps      | 1144832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0135    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.55e+09  |\n",
      "|    n_updates            | 5580      |\n",
      "|    policy_gradient_loss | -2.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.47e+10  |\n",
      "---------------------------------------\n",
      "Episode 459  finished with cumulative reward: -7317500.0 and \n",
      "with an average reward of: -2925.829668132747\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1145458\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2927.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 560       |\n",
      "|    time_elapsed         | 16092     |\n",
      "|    total_timesteps      | 1146880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0157    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.38e+09  |\n",
      "|    n_updates            | 5590      |\n",
      "|    policy_gradient_loss | -3.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 460  finished with cumulative reward: -8796500.0 and \n",
      "with an average reward of: -3517.1931227508994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1147959\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3518.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.61e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 561          |\n",
      "|    time_elapsed         | 16120        |\n",
      "|    total_timesteps      | 1148928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0159       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.31e+09     |\n",
      "|    n_updates            | 5600         |\n",
      "|    policy_gradient_loss | -3.92e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.64e+09     |\n",
      "------------------------------------------\n",
      "Episode 461  finished with cumulative reward: -6297500.0 and \n",
      "with an average reward of: -2517.9928028788486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1150460\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2519.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 562       |\n",
      "|    time_elapsed         | 16148     |\n",
      "|    total_timesteps      | 1150976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0112    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.43e+10  |\n",
      "|    n_updates            | 5610      |\n",
      "|    policy_gradient_loss | -3.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.8e+10   |\n",
      "---------------------------------------\n",
      "Episode 462  finished with cumulative reward: -12162500.0 and \n",
      "with an average reward of: -4863.054778088765\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1152961\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4865.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 563       |\n",
      "|    time_elapsed         | 16177     |\n",
      "|    total_timesteps      | 1153024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0132    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.67e+09  |\n",
      "|    n_updates            | 5620      |\n",
      "|    policy_gradient_loss | -2.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.98e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.67e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 564          |\n",
      "|    time_elapsed         | 16198        |\n",
      "|    total_timesteps      | 1155072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0134       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.76e+09     |\n",
      "|    n_updates            | 5630         |\n",
      "|    policy_gradient_loss | -5.98e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.1e+10      |\n",
      "------------------------------------------\n",
      "Episode 463  finished with cumulative reward: -3339500.0 and \n",
      "with an average reward of: -1335.265893642543\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1155462\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1335.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 565       |\n",
      "|    time_elapsed         | 16226     |\n",
      "|    total_timesteps      | 1157120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0177    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.13e+09  |\n",
      "|    n_updates            | 5640      |\n",
      "|    policy_gradient_loss | -3.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.12e+09  |\n",
      "---------------------------------------\n",
      "Episode 464  finished with cumulative reward: -2574500.0 and \n",
      "with an average reward of: -1029.3882447021192\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1157963\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1029.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 566       |\n",
      "|    time_elapsed         | 16254     |\n",
      "|    total_timesteps      | 1159168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0155    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.87e+09  |\n",
      "|    n_updates            | 5650      |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 465  finished with cumulative reward: -5762000.0 and \n",
      "with an average reward of: -2303.878448620552\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1160464\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2304.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 567       |\n",
      "|    time_elapsed         | 16282     |\n",
      "|    total_timesteps      | 1161216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0166    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.79e+09  |\n",
      "|    n_updates            | 5660      |\n",
      "|    policy_gradient_loss | -2.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 466  finished with cumulative reward: -11142500.0 and \n",
      "with an average reward of: -4455.217912834866\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1162965\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4457.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 568       |\n",
      "|    time_elapsed         | 16310     |\n",
      "|    total_timesteps      | 1163264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0125    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.78e+09  |\n",
      "|    n_updates            | 5670      |\n",
      "|    policy_gradient_loss | -1.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.42e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 569       |\n",
      "|    time_elapsed         | 16331     |\n",
      "|    total_timesteps      | 1165312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00886   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.87e+09  |\n",
      "|    n_updates            | 5680      |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 467  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1165466\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 570       |\n",
      "|    time_elapsed         | 16359     |\n",
      "|    total_timesteps      | 1167360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0148    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.04e+09  |\n",
      "|    n_updates            | 5690      |\n",
      "|    policy_gradient_loss | -3.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 468  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1167967\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 571       |\n",
      "|    time_elapsed         | 16386     |\n",
      "|    total_timesteps      | 1169408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.014     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.31e+09  |\n",
      "|    n_updates            | 5700      |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.18e+09  |\n",
      "---------------------------------------\n",
      "Episode 469  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1170468\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 572       |\n",
      "|    time_elapsed         | 16414     |\n",
      "|    total_timesteps      | 1171456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0159    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.29e+09  |\n",
      "|    n_updates            | 5710      |\n",
      "|    policy_gradient_loss | -1.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.76e+09  |\n",
      "---------------------------------------\n",
      "Episode 470  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1172969\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 573       |\n",
      "|    time_elapsed         | 16442     |\n",
      "|    total_timesteps      | 1173504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0183    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 5720      |\n",
      "|    policy_gradient_loss | -2.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "Episode 471  finished with cumulative reward: -11091500.0 and \n",
      "with an average reward of: -4434.826069572171\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1175470\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4436.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.61e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 71            |\n",
      "|    iterations           | 574           |\n",
      "|    time_elapsed         | 16470         |\n",
      "|    total_timesteps      | 1175552       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0161        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.08e+10      |\n",
      "|    n_updates            | 5730          |\n",
      "|    policy_gradient_loss | -7.61e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.36e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 575       |\n",
      "|    time_elapsed         | 16491     |\n",
      "|    total_timesteps      | 1177600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0118    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.03e+10  |\n",
      "|    n_updates            | 5740      |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.85e+10  |\n",
      "---------------------------------------\n",
      "Episode 472  finished with cumulative reward: -4206500.0 and \n",
      "with an average reward of: -1681.9272291083566\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1177971\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1682.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 576       |\n",
      "|    time_elapsed         | 16519     |\n",
      "|    total_timesteps      | 1179648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0141    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.08e+09  |\n",
      "|    n_updates            | 5750      |\n",
      "|    policy_gradient_loss | -2.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.81e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 473  finished with cumulative reward: -3237500.0 and \n",
      "with an average reward of: -1294.4822071171532\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1180472\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1295.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 577       |\n",
      "|    time_elapsed         | 16547     |\n",
      "|    total_timesteps      | 1181696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.016     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.75e+09  |\n",
      "|    n_updates            | 5760      |\n",
      "|    policy_gradient_loss | -4.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 474  finished with cumulative reward: -7802000.0 and \n",
      "with an average reward of: -3119.5521791283486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1182973\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3120.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 578       |\n",
      "|    time_elapsed         | 16575     |\n",
      "|    total_timesteps      | 1183744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0149    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.1e+09   |\n",
      "|    n_updates            | 5770      |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 475  finished with cumulative reward: -1962500.0 and \n",
      "with an average reward of: -784.6861255497801\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1185474\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -785.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 579       |\n",
      "|    time_elapsed         | 16602     |\n",
      "|    total_timesteps      | 1185792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0129    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.23e+09  |\n",
      "|    n_updates            | 5780      |\n",
      "|    policy_gradient_loss | -2.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 580       |\n",
      "|    time_elapsed         | 16623     |\n",
      "|    total_timesteps      | 1187840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.32e+09  |\n",
      "|    n_updates            | 5790      |\n",
      "|    policy_gradient_loss | -3.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.83e+09  |\n",
      "---------------------------------------\n",
      "Episode 476  finished with cumulative reward: -6297500.0 and \n",
      "with an average reward of: -2517.9928028788486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1187975\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2519.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 581       |\n",
      "|    time_elapsed         | 16651     |\n",
      "|    total_timesteps      | 1189888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0225    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.12e+09  |\n",
      "|    n_updates            | 5800      |\n",
      "|    policy_gradient_loss | -5.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 477  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1190476\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 582       |\n",
      "|    time_elapsed         | 16679     |\n",
      "|    total_timesteps      | 1191936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0136    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.36e+09  |\n",
      "|    n_updates            | 5810      |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+10   |\n",
      "---------------------------------------\n",
      "Episode 478  finished with cumulative reward: -3798500.0 and \n",
      "with an average reward of: -1518.7924830067973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1192977\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1519.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 583       |\n",
      "|    time_elapsed         | 16706     |\n",
      "|    total_timesteps      | 1193984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.17e+07  |\n",
      "|    n_updates            | 5820      |\n",
      "|    policy_gradient_loss | -3.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.95e+08  |\n",
      "---------------------------------------\n",
      "Episode 479  finished with cumulative reward: -4308500.0 and \n",
      "with an average reward of: -1722.7109156337465\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1195478\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1723.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 584       |\n",
      "|    time_elapsed         | 16734     |\n",
      "|    total_timesteps      | 1196032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0151    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.14e+09  |\n",
      "|    n_updates            | 5830      |\n",
      "|    policy_gradient_loss | -2.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 480  finished with cumulative reward: -6246500.0 and \n",
      "with an average reward of: -2497.6009596161534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1197979\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2498.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 585       |\n",
      "|    time_elapsed         | 16762     |\n",
      "|    total_timesteps      | 1198080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0182    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.17e+09  |\n",
      "|    n_updates            | 5840      |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.21e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 586       |\n",
      "|    time_elapsed         | 16783     |\n",
      "|    total_timesteps      | 1200128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0149    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.1e+09   |\n",
      "|    n_updates            | 5850      |\n",
      "|    policy_gradient_loss | -4.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 481  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1200480\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 587       |\n",
      "|    time_elapsed         | 16811     |\n",
      "|    total_timesteps      | 1202176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0202    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.81e+09  |\n",
      "|    n_updates            | 5860      |\n",
      "|    policy_gradient_loss | -3.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.03e+09  |\n",
      "---------------------------------------\n",
      "Episode 482  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1202981\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 588       |\n",
      "|    time_elapsed         | 16839     |\n",
      "|    total_timesteps      | 1204224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0169    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.83e+09  |\n",
      "|    n_updates            | 5870      |\n",
      "|    policy_gradient_loss | -3.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 483  finished with cumulative reward: -9000500.0 and \n",
      "with an average reward of: -3598.7604958016796\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1205482\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3600.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 589       |\n",
      "|    time_elapsed         | 16867     |\n",
      "|    total_timesteps      | 1206272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.014     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.11e+09  |\n",
      "|    n_updates            | 5880      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.54e+10  |\n",
      "---------------------------------------\n",
      "Episode 484  finished with cumulative reward: -8108000.0 and \n",
      "with an average reward of: -3241.903238704518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1207983\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3243.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 590       |\n",
      "|    time_elapsed         | 16895     |\n",
      "|    total_timesteps      | 1208320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0143    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28e+10  |\n",
      "|    n_updates            | 5890      |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.25e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 591       |\n",
      "|    time_elapsed         | 16916     |\n",
      "|    total_timesteps      | 1210368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0154    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.55e+09  |\n",
      "|    n_updates            | 5900      |\n",
      "|    policy_gradient_loss | -2.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 485  finished with cumulative reward: -6629000.0 and \n",
      "with an average reward of: -2650.5397840863657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1210484\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2651.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 592       |\n",
      "|    time_elapsed         | 16945     |\n",
      "|    total_timesteps      | 1212416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0178    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.11e+09  |\n",
      "|    n_updates            | 5910      |\n",
      "|    policy_gradient_loss | -2.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 486  finished with cumulative reward: -5099000.0 and \n",
      "with an average reward of: -2038.7844862055179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1212985\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2039.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 593       |\n",
      "|    time_elapsed         | 16973     |\n",
      "|    total_timesteps      | 1214464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0117    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.46e+09  |\n",
      "|    n_updates            | 5920      |\n",
      "|    policy_gradient_loss | -1.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 487  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1215486\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 594          |\n",
      "|    time_elapsed         | 17001        |\n",
      "|    total_timesteps      | 1216512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0149       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.38e+09     |\n",
      "|    n_updates            | 5930         |\n",
      "|    policy_gradient_loss | -4.71e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.18e+10     |\n",
      "------------------------------------------\n",
      "Episode 488  finished with cumulative reward: -5634500.0 and \n",
      "with an average reward of: -2252.8988404638144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1217987\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2253.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 595       |\n",
      "|    time_elapsed         | 17029     |\n",
      "|    total_timesteps      | 1218560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0164    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.31e+09  |\n",
      "|    n_updates            | 5940      |\n",
      "|    policy_gradient_loss | -2.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 489  finished with cumulative reward: -3161000.0 and \n",
      "with an average reward of: -1263.8944422231107\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1220488\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1264.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 596       |\n",
      "|    time_elapsed         | 17057     |\n",
      "|    total_timesteps      | 1220608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0147    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.06e+09  |\n",
      "|    n_updates            | 5950      |\n",
      "|    policy_gradient_loss | -7.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 597       |\n",
      "|    time_elapsed         | 17078     |\n",
      "|    total_timesteps      | 1222656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0194    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.43e+09  |\n",
      "|    n_updates            | 5960      |\n",
      "|    policy_gradient_loss | -3.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 490  finished with cumulative reward: -8363000.0 and \n",
      "with an average reward of: -3343.862455017993\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1222989\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3345.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 598       |\n",
      "|    time_elapsed         | 17106     |\n",
      "|    total_timesteps      | 1224704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0135    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.02e+09  |\n",
      "|    n_updates            | 5970      |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.96e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 491  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1225490\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 599       |\n",
      "|    time_elapsed         | 17133     |\n",
      "|    total_timesteps      | 1226752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0165    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.06e+09  |\n",
      "|    n_updates            | 5980      |\n",
      "|    policy_gradient_loss | -4e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n",
      "Episode 492  finished with cumulative reward: -10556000.0 and \n",
      "with an average reward of: -4220.711715313874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1227991\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4222.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 600       |\n",
      "|    time_elapsed         | 17161     |\n",
      "|    total_timesteps      | 1228800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0139    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.02e+09  |\n",
      "|    n_updates            | 5990      |\n",
      "|    policy_gradient_loss | -2.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.59e+10  |\n",
      "---------------------------------------\n",
      "Episode 493  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1230492\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 601       |\n",
      "|    time_elapsed         | 17189     |\n",
      "|    total_timesteps      | 1230848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0171    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.01e+09  |\n",
      "|    n_updates            | 6000      |\n",
      "|    policy_gradient_loss | -4.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 602       |\n",
      "|    time_elapsed         | 17209     |\n",
      "|    total_timesteps      | 1232896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.017     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.83e+09  |\n",
      "|    n_updates            | 6010      |\n",
      "|    policy_gradient_loss | -3.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.01e+09  |\n",
      "---------------------------------------\n",
      "Episode 494  finished with cumulative reward: -2039000.0 and \n",
      "with an average reward of: -815.2738904438224\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1232993\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -815.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 603       |\n",
      "|    time_elapsed         | 17237     |\n",
      "|    total_timesteps      | 1234944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0216    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.2e+09   |\n",
      "|    n_updates            | 6020      |\n",
      "|    policy_gradient_loss | -3.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.11e+09  |\n",
      "---------------------------------------\n",
      "Episode 495  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1235494\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 604       |\n",
      "|    time_elapsed         | 17265     |\n",
      "|    total_timesteps      | 1236992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0124    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+10  |\n",
      "|    n_updates            | 6030      |\n",
      "|    policy_gradient_loss | -2.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n",
      "Episode 496  finished with cumulative reward: -8898500.0 and \n",
      "with an average reward of: -3557.9768092762893\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1237995\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3559.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 605       |\n",
      "|    time_elapsed         | 17294     |\n",
      "|    total_timesteps      | 1239040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0122    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.19e+09  |\n",
      "|    n_updates            | 6040      |\n",
      "|    policy_gradient_loss | -1.31e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.26e+10  |\n",
      "---------------------------------------\n",
      "Episode 497  finished with cumulative reward: -10862000.0 and \n",
      "with an average reward of: -4343.062774890044\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1240496\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4344.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 606       |\n",
      "|    time_elapsed         | 17322     |\n",
      "|    total_timesteps      | 1241088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0166    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.23e+09  |\n",
      "|    n_updates            | 6050      |\n",
      "|    policy_gradient_loss | -2.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.26e+10  |\n",
      "---------------------------------------\n",
      "Episode 498  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1242997\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 607       |\n",
      "|    time_elapsed         | 17350     |\n",
      "|    total_timesteps      | 1243136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0146    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.98e+09  |\n",
      "|    n_updates            | 6060      |\n",
      "|    policy_gradient_loss | -1.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.34e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 608       |\n",
      "|    time_elapsed         | 17372     |\n",
      "|    total_timesteps      | 1245184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0157    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.74e+09  |\n",
      "|    n_updates            | 6070      |\n",
      "|    policy_gradient_loss | -1.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 499  finished with cumulative reward: -6705500.0 and \n",
      "with an average reward of: -2681.127548980408\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1245498\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2682.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 609       |\n",
      "|    time_elapsed         | 17399     |\n",
      "|    total_timesteps      | 1247232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.016     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.58e+09  |\n",
      "|    n_updates            | 6080      |\n",
      "|    policy_gradient_loss | -1.96e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 500  finished with cumulative reward: -2523500.0 and \n",
      "with an average reward of: -1008.9964014394242\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1247999\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1009.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 610       |\n",
      "|    time_elapsed         | 17427     |\n",
      "|    total_timesteps      | 1249280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0196    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.71e+09  |\n",
      "|    n_updates            | 6090      |\n",
      "|    policy_gradient_loss | -2.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 501  finished with cumulative reward: -891500.0 and \n",
      "with an average reward of: -356.45741703318674\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1250500\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -356.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.83e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 611          |\n",
      "|    time_elapsed         | 17455        |\n",
      "|    total_timesteps      | 1251328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0206       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.63e+09     |\n",
      "|    n_updates            | 6100         |\n",
      "|    policy_gradient_loss | -3.28e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.32e+09     |\n",
      "------------------------------------------\n",
      "Episode 502  finished with cumulative reward: -9791000.0 and \n",
      "with an average reward of: -3914.8340663734507\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1253001\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3916.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 612       |\n",
      "|    time_elapsed         | 17483     |\n",
      "|    total_timesteps      | 1253376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0155    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 6110      |\n",
      "|    policy_gradient_loss | -1.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 613       |\n",
      "|    time_elapsed         | 17504     |\n",
      "|    total_timesteps      | 1255424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0172    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.35e+09  |\n",
      "|    n_updates            | 6120      |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.09e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 503  finished with cumulative reward: -9408500.0 and \n",
      "with an average reward of: -3761.895241903239\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1255502\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3763.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 614       |\n",
      "|    time_elapsed         | 17533     |\n",
      "|    total_timesteps      | 1257472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0171    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.4e+10   |\n",
      "|    n_updates            | 6130      |\n",
      "|    policy_gradient_loss | -5.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.58e+10  |\n",
      "---------------------------------------\n",
      "Episode 504  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1258003\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 615       |\n",
      "|    time_elapsed         | 17561     |\n",
      "|    total_timesteps      | 1259520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0169    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.49e+09  |\n",
      "|    n_updates            | 6140      |\n",
      "|    policy_gradient_loss | -3.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 505  finished with cumulative reward: -10148000.0 and \n",
      "with an average reward of: -4057.576969212315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1260504\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4059.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 616       |\n",
      "|    time_elapsed         | 17589     |\n",
      "|    total_timesteps      | 1261568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0154    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.88e+09  |\n",
      "|    n_updates            | 6150      |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.72e+10  |\n",
      "---------------------------------------\n",
      "Episode 506  finished with cumulative reward: -5430500.0 and \n",
      "with an average reward of: -2171.3314674130347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1263005\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2172.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 617       |\n",
      "|    time_elapsed         | 17617     |\n",
      "|    total_timesteps      | 1263616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0116    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.12e+10  |\n",
      "|    n_updates            | 6160      |\n",
      "|    policy_gradient_loss | -1.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.14e+10  |\n",
      "---------------------------------------\n",
      "Episode 507  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1265506\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 618       |\n",
      "|    time_elapsed         | 17645     |\n",
      "|    total_timesteps      | 1265664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.018     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.68e+09  |\n",
      "|    n_updates            | 6170      |\n",
      "|    policy_gradient_loss | -5.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 619       |\n",
      "|    time_elapsed         | 17666     |\n",
      "|    total_timesteps      | 1267712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0173    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.33e+09  |\n",
      "|    n_updates            | 6180      |\n",
      "|    policy_gradient_loss | -2.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 508  finished with cumulative reward: -1350500.0 and \n",
      "with an average reward of: -539.984006397441\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1268007\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -540.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 620       |\n",
      "|    time_elapsed         | 17694     |\n",
      "|    total_timesteps      | 1269760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0173    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.94e+09  |\n",
      "|    n_updates            | 6190      |\n",
      "|    policy_gradient_loss | -3e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4e+09     |\n",
      "---------------------------------------\n",
      "Episode 509  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1270508\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 621       |\n",
      "|    time_elapsed         | 17722     |\n",
      "|    total_timesteps      | 1271808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.36e+09  |\n",
      "|    n_updates            | 6200      |\n",
      "|    policy_gradient_loss | -1.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.18e+09  |\n",
      "---------------------------------------\n",
      "Episode 510  finished with cumulative reward: -3798500.0 and \n",
      "with an average reward of: -1518.7924830067973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1273009\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1519.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 622       |\n",
      "|    time_elapsed         | 17749     |\n",
      "|    total_timesteps      | 1273856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0169    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.28e+09  |\n",
      "|    n_updates            | 6210      |\n",
      "|    policy_gradient_loss | -2.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 511  finished with cumulative reward: -5685500.0 and \n",
      "with an average reward of: -2273.2906837265095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1275510\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2274.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 623       |\n",
      "|    time_elapsed         | 17777     |\n",
      "|    total_timesteps      | 1275904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0174    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.8e+09   |\n",
      "|    n_updates            | 6220      |\n",
      "|    policy_gradient_loss | -3.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.08e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 624       |\n",
      "|    time_elapsed         | 17797     |\n",
      "|    total_timesteps      | 1277952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0128    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.67e+09  |\n",
      "|    n_updates            | 6230      |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 512  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1278011\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 625       |\n",
      "|    time_elapsed         | 17825     |\n",
      "|    total_timesteps      | 1280000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0164    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.1e+09   |\n",
      "|    n_updates            | 6240      |\n",
      "|    policy_gradient_loss | -1.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 513  finished with cumulative reward: -11601500.0 and \n",
      "with an average reward of: -4638.744502199121\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1280512\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4640.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 626       |\n",
      "|    time_elapsed         | 17853     |\n",
      "|    total_timesteps      | 1282048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0124    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.4e+10   |\n",
      "|    n_updates            | 6250      |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.81e+10  |\n",
      "---------------------------------------\n",
      "Episode 514  finished with cumulative reward: -4767500.0 and \n",
      "with an average reward of: -1906.2375049980008\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1283013\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1907.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 627       |\n",
      "|    time_elapsed         | 17881     |\n",
      "|    total_timesteps      | 1284096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0199    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.64e+09  |\n",
      "|    n_updates            | 6260      |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.72e+09  |\n",
      "---------------------------------------\n",
      "Episode 515  finished with cumulative reward: -5787500.0 and \n",
      "with an average reward of: -2314.0743702518994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1285514\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2315.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 628       |\n",
      "|    time_elapsed         | 17908     |\n",
      "|    total_timesteps      | 1286144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0164    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.31e+09  |\n",
      "|    n_updates            | 6270      |\n",
      "|    policy_gradient_loss | -1.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 516  finished with cumulative reward: -1835000.0 and \n",
      "with an average reward of: -733.7065173930428\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1288015\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -734.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 629       |\n",
      "|    time_elapsed         | 17936     |\n",
      "|    total_timesteps      | 1288192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0151    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.44e+09  |\n",
      "|    n_updates            | 6280      |\n",
      "|    policy_gradient_loss | -2.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.94e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 630          |\n",
      "|    time_elapsed         | 17957        |\n",
      "|    total_timesteps      | 1290240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0278       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.35e+08     |\n",
      "|    n_updates            | 6290         |\n",
      "|    policy_gradient_loss | -5.13e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.92e+09     |\n",
      "------------------------------------------\n",
      "Episode 517  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1290516\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 631       |\n",
      "|    time_elapsed         | 17985     |\n",
      "|    total_timesteps      | 1292288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0196    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.73e+09  |\n",
      "|    n_updates            | 6300      |\n",
      "|    policy_gradient_loss | -2.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.91e+09  |\n",
      "---------------------------------------\n",
      "Episode 518  finished with cumulative reward: -1656500.0 and \n",
      "with an average reward of: -662.3350659736105\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1293017\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -662.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 632       |\n",
      "|    time_elapsed         | 18014     |\n",
      "|    total_timesteps      | 1294336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.023     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.08e+09  |\n",
      "|    n_updates            | 6310      |\n",
      "|    policy_gradient_loss | -3.98e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.74e+09  |\n",
      "---------------------------------------\n",
      "Episode 519  finished with cumulative reward: -5583500.0 and \n",
      "with an average reward of: -2232.5069972011197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1295518\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2233.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 633       |\n",
      "|    time_elapsed         | 18042     |\n",
      "|    total_timesteps      | 1296384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.017     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.43e+09  |\n",
      "|    n_updates            | 6320      |\n",
      "|    policy_gradient_loss | -2.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.82e+09  |\n",
      "---------------------------------------\n",
      "Episode 520  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1298019\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 634       |\n",
      "|    time_elapsed         | 18070     |\n",
      "|    total_timesteps      | 1298432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.014     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.73e+09  |\n",
      "|    n_updates            | 6330      |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.45e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 635       |\n",
      "|    time_elapsed         | 18091     |\n",
      "|    total_timesteps      | 1300480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0178    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.42e+09  |\n",
      "|    n_updates            | 6340      |\n",
      "|    policy_gradient_loss | -2.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 521  finished with cumulative reward: -7343000.0 and \n",
      "with an average reward of: -2936.0255897640945\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1300520\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2937.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 636       |\n",
      "|    time_elapsed         | 18119     |\n",
      "|    total_timesteps      | 1302528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0174    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.01e+09  |\n",
      "|    n_updates            | 6350      |\n",
      "|    policy_gradient_loss | -2.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.9e+10   |\n",
      "---------------------------------------\n",
      "Episode 522  finished with cumulative reward: -7929500.0 and \n",
      "with an average reward of: -3170.531787285086\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1303021\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3171.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 637       |\n",
      "|    time_elapsed         | 18147     |\n",
      "|    total_timesteps      | 1304576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.87e+09  |\n",
      "|    n_updates            | 6360      |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.87e+09  |\n",
      "---------------------------------------\n",
      "Episode 523  finished with cumulative reward: -10224500.0 and \n",
      "with an average reward of: -4088.1647341063576\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1305522\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4089.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.94e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 71            |\n",
      "|    iterations           | 638           |\n",
      "|    time_elapsed         | 18175         |\n",
      "|    total_timesteps      | 1306624       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0153        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.26e+10      |\n",
      "|    n_updates            | 6370          |\n",
      "|    policy_gradient_loss | -9.36e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.22e+10      |\n",
      "-------------------------------------------\n",
      "Episode 524  finished with cumulative reward: -10352000.0 and \n",
      "with an average reward of: -4139.144342263095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1308023\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4140.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 639       |\n",
      "|    time_elapsed         | 18202     |\n",
      "|    total_timesteps      | 1308672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0177    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.23e+09  |\n",
      "|    n_updates            | 6380      |\n",
      "|    policy_gradient_loss | -2.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+10  |\n",
      "---------------------------------------\n",
      "Episode 525  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1310524\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 640       |\n",
      "|    time_elapsed         | 18230     |\n",
      "|    total_timesteps      | 1310720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0166    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.9e+09   |\n",
      "|    n_updates            | 6390      |\n",
      "|    policy_gradient_loss | -1.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 641       |\n",
      "|    time_elapsed         | 18251     |\n",
      "|    total_timesteps      | 1312768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0164    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.5e+09   |\n",
      "|    n_updates            | 6400      |\n",
      "|    policy_gradient_loss | -2.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.21e+10  |\n",
      "---------------------------------------\n",
      "Episode 526  finished with cumulative reward: -5456000.0 and \n",
      "with an average reward of: -2181.5273890443823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1313025\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2182.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 642       |\n",
      "|    time_elapsed         | 18279     |\n",
      "|    total_timesteps      | 1314816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0157    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.29e+09  |\n",
      "|    n_updates            | 6410      |\n",
      "|    policy_gradient_loss | -2.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 527  finished with cumulative reward: -1452500.0 and \n",
      "with an average reward of: -580.7676929228309\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1315526\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -581.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 643       |\n",
      "|    time_elapsed         | 18306     |\n",
      "|    total_timesteps      | 1316864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.027     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.98e+08  |\n",
      "|    n_updates            | 6420      |\n",
      "|    policy_gradient_loss | -2.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.81e+09  |\n",
      "---------------------------------------\n",
      "Episode 528  finished with cumulative reward: -3926000.0 and \n",
      "with an average reward of: -1569.7720911635347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1318027\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1570.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 644       |\n",
      "|    time_elapsed         | 18334     |\n",
      "|    total_timesteps      | 1318912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0268    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.01e+09  |\n",
      "|    n_updates            | 6430      |\n",
      "|    policy_gradient_loss | -2.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.54e+09  |\n",
      "---------------------------------------\n",
      "Episode 529  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1320528\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 645       |\n",
      "|    time_elapsed         | 18362     |\n",
      "|    total_timesteps      | 1320960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0178    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.31e+09  |\n",
      "|    n_updates            | 6440      |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.25e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 646       |\n",
      "|    time_elapsed         | 18383     |\n",
      "|    total_timesteps      | 1323008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0214    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.36e+09  |\n",
      "|    n_updates            | 6450      |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 530  finished with cumulative reward: -5048000.0 and \n",
      "with an average reward of: -2018.392642942823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1323029\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2019.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 647       |\n",
      "|    time_elapsed         | 18411     |\n",
      "|    total_timesteps      | 1325056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.3e+09   |\n",
      "|    n_updates            | 6460      |\n",
      "|    policy_gradient_loss | -3.58e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 531  finished with cumulative reward: -7241000.0 and \n",
      "with an average reward of: -2895.2419032387047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1325530\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2896.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.91e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 71           |\n",
      "|    iterations           | 648          |\n",
      "|    time_elapsed         | 18438        |\n",
      "|    total_timesteps      | 1327104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0167       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.29e+09     |\n",
      "|    n_updates            | 6470         |\n",
      "|    policy_gradient_loss | -7.32e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.35e+10     |\n",
      "------------------------------------------\n",
      "Episode 532  finished with cumulative reward: -1580000.0 and \n",
      "with an average reward of: -631.7473010795682\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1328031\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -632.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 649       |\n",
      "|    time_elapsed         | 18467     |\n",
      "|    total_timesteps      | 1329152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0202    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.12e+09  |\n",
      "|    n_updates            | 6480      |\n",
      "|    policy_gradient_loss | -5.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.14e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 533  finished with cumulative reward: -6807500.0 and \n",
      "with an average reward of: -2721.911235505798\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1330532\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2723.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 650       |\n",
      "|    time_elapsed         | 18495     |\n",
      "|    total_timesteps      | 1331200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0177    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.33e+08  |\n",
      "|    n_updates            | 6490      |\n",
      "|    policy_gradient_loss | -2.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 534  finished with cumulative reward: -6603500.0 and \n",
      "with an average reward of: -2640.343862455018\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1333033\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2641.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 71        |\n",
      "|    iterations           | 651       |\n",
      "|    time_elapsed         | 18523     |\n",
      "|    total_timesteps      | 1333248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0152    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.34e+09  |\n",
      "|    n_updates            | 6500      |\n",
      "|    policy_gradient_loss | -1.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 652       |\n",
      "|    time_elapsed         | 18544     |\n",
      "|    total_timesteps      | 1335296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.84e+09  |\n",
      "|    n_updates            | 6510      |\n",
      "|    policy_gradient_loss | -2.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n",
      "Episode 535  finished with cumulative reward: -8388500.0 and \n",
      "with an average reward of: -3354.05837664934\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1335534\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3355.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 653       |\n",
      "|    time_elapsed         | 18573     |\n",
      "|    total_timesteps      | 1337344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.00959   |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.15e+10  |\n",
      "|    n_updates            | 6520      |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 536  finished with cumulative reward: -5124500.0 and \n",
      "with an average reward of: -2048.980407836865\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1338035\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2049.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 654       |\n",
      "|    time_elapsed         | 18601     |\n",
      "|    total_timesteps      | 1339392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0175    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.95e+09  |\n",
      "|    n_updates            | 6530      |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.28e+09  |\n",
      "---------------------------------------\n",
      "Episode 537  finished with cumulative reward: -10607000.0 and \n",
      "with an average reward of: -4241.103558576569\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1340536\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4242.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 655       |\n",
      "|    time_elapsed         | 18628     |\n",
      "|    total_timesteps      | 1341440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 6540      |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 538  finished with cumulative reward: -3620000.0 and \n",
      "with an average reward of: -1447.421031587365\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1343037\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1448.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 656       |\n",
      "|    time_elapsed         | 18656     |\n",
      "|    total_timesteps      | 1343488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0161    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.68e+09  |\n",
      "|    n_updates            | 6550      |\n",
      "|    policy_gradient_loss | -1.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.94e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 657          |\n",
      "|    time_elapsed         | 18677        |\n",
      "|    total_timesteps      | 1345536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0173       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.15e+09     |\n",
      "|    n_updates            | 6560         |\n",
      "|    policy_gradient_loss | -9.04e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.06e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 539  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1345538\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 658       |\n",
      "|    time_elapsed         | 18705     |\n",
      "|    total_timesteps      | 1347584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0161    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e+10  |\n",
      "|    n_updates            | 6570      |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 540  finished with cumulative reward: -6986000.0 and \n",
      "with an average reward of: -2793.28268692523\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1348039\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2794.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 659       |\n",
      "|    time_elapsed         | 18733     |\n",
      "|    total_timesteps      | 1349632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0164    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.37e+09  |\n",
      "|    n_updates            | 6580      |\n",
      "|    policy_gradient_loss | -2.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 541  finished with cumulative reward: -8261000.0 and \n",
      "with an average reward of: -3303.078768492603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1350540\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3304.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 660       |\n",
      "|    time_elapsed         | 18762     |\n",
      "|    total_timesteps      | 1351680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0183    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.22e+09  |\n",
      "|    n_updates            | 6590      |\n",
      "|    policy_gradient_loss | -4.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.54e+10  |\n",
      "---------------------------------------\n",
      "Episode 542  finished with cumulative reward: -4232000.0 and \n",
      "with an average reward of: -1692.1231507397042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1353041\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1692.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 661       |\n",
      "|    time_elapsed         | 18790     |\n",
      "|    total_timesteps      | 1353728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0178    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.55e+09  |\n",
      "|    n_updates            | 6600      |\n",
      "|    policy_gradient_loss | -2.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 543  finished with cumulative reward: -16523000.0 and \n",
      "with an average reward of: -6606.55737704918\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1355542\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -6609.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 662       |\n",
      "|    time_elapsed         | 18818     |\n",
      "|    total_timesteps      | 1355776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.014     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+10  |\n",
      "|    n_updates            | 6610      |\n",
      "|    policy_gradient_loss | -2.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.15e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 663       |\n",
      "|    time_elapsed         | 18839     |\n",
      "|    total_timesteps      | 1357824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0135    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.66e+10  |\n",
      "|    n_updates            | 6620      |\n",
      "|    policy_gradient_loss | -2.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.73e+10  |\n",
      "---------------------------------------\n",
      "Episode 544  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1358043\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 664       |\n",
      "|    time_elapsed         | 18866     |\n",
      "|    total_timesteps      | 1359872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0173    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.68e+09  |\n",
      "|    n_updates            | 6630      |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n",
      "Episode 545  finished with cumulative reward: -5073500.0 and \n",
      "with an average reward of: -2028.5885645741703\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1360544\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2029.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 665       |\n",
      "|    time_elapsed         | 18894     |\n",
      "|    total_timesteps      | 1361920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.023     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.99e+09  |\n",
      "|    n_updates            | 6640      |\n",
      "|    policy_gradient_loss | -2.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.52e+09  |\n",
      "---------------------------------------\n",
      "Episode 546  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1363045\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.15e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 666       |\n",
      "|    time_elapsed         | 18922     |\n",
      "|    total_timesteps      | 1363968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0137    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.79e+09  |\n",
      "|    n_updates            | 6650      |\n",
      "|    policy_gradient_loss | -2.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 547  finished with cumulative reward: -1682000.0 and \n",
      "with an average reward of: -672.530987604958\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1365546\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -672.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 667       |\n",
      "|    time_elapsed         | 18950     |\n",
      "|    total_timesteps      | 1366016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0146    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.79e+09  |\n",
      "|    n_updates            | 6660      |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 548  finished with cumulative reward: -11295500.0 and \n",
      "with an average reward of: -4516.393442622951\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1368047\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4518.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 668       |\n",
      "|    time_elapsed         | 18978     |\n",
      "|    total_timesteps      | 1368064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0187    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.88e+09  |\n",
      "|    n_updates            | 6670      |\n",
      "|    policy_gradient_loss | -3.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.97e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 669       |\n",
      "|    time_elapsed         | 18998     |\n",
      "|    total_timesteps      | 1370112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0146    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.25e+10  |\n",
      "|    n_updates            | 6680      |\n",
      "|    policy_gradient_loss | -3.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 549  finished with cumulative reward: -6807500.0 and \n",
      "with an average reward of: -2721.911235505798\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1370548\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2723.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 670       |\n",
      "|    time_elapsed         | 19026     |\n",
      "|    total_timesteps      | 1372160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0188    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.42e+09  |\n",
      "|    n_updates            | 6690      |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 550  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1373049\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 671       |\n",
      "|    time_elapsed         | 19054     |\n",
      "|    total_timesteps      | 1374208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0159    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.71e+09  |\n",
      "|    n_updates            | 6700      |\n",
      "|    policy_gradient_loss | -2.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.9e+10   |\n",
      "---------------------------------------\n",
      "Episode 551  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1375550\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 672       |\n",
      "|    time_elapsed         | 19082     |\n",
      "|    total_timesteps      | 1376256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0174    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.54e+09  |\n",
      "|    n_updates            | 6710      |\n",
      "|    policy_gradient_loss | -4.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.42e+09  |\n",
      "---------------------------------------\n",
      "Episode 552  finished with cumulative reward: -6017000.0 and \n",
      "with an average reward of: -2405.837664934026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1378051\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2406.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 673       |\n",
      "|    time_elapsed         | 19109     |\n",
      "|    total_timesteps      | 1378304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0184    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.7e+09   |\n",
      "|    n_updates            | 6720      |\n",
      "|    policy_gradient_loss | -3.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.42e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 674       |\n",
      "|    time_elapsed         | 19130     |\n",
      "|    total_timesteps      | 1380352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0159    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.17e+09  |\n",
      "|    n_updates            | 6730      |\n",
      "|    policy_gradient_loss | -3.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.67e+09  |\n",
      "---------------------------------------\n",
      "Episode 553  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1380552\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 675       |\n",
      "|    time_elapsed         | 19159     |\n",
      "|    total_timesteps      | 1382400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0168    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.27e+09  |\n",
      "|    n_updates            | 6740      |\n",
      "|    policy_gradient_loss | -3.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 554  finished with cumulative reward: -10097000.0 and \n",
      "with an average reward of: -4037.18512594962\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1383053\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4038.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 676       |\n",
      "|    time_elapsed         | 19187     |\n",
      "|    total_timesteps      | 1384448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0146    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.06e+09  |\n",
      "|    n_updates            | 6750      |\n",
      "|    policy_gradient_loss | -3.68e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.94e+10  |\n",
      "---------------------------------------\n",
      "Episode 555  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1385554\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 677       |\n",
      "|    time_elapsed         | 19215     |\n",
      "|    total_timesteps      | 1386496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0172    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.63e+09  |\n",
      "|    n_updates            | 6760      |\n",
      "|    policy_gradient_loss | -3.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.93e+10  |\n",
      "---------------------------------------\n",
      "Episode 556  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1388055\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 678       |\n",
      "|    time_elapsed         | 19243     |\n",
      "|    total_timesteps      | 1388544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.019     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.34e+09  |\n",
      "|    n_updates            | 6770      |\n",
      "|    policy_gradient_loss | -2.68e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 557  finished with cumulative reward: -1554500.0 and \n",
      "with an average reward of: -621.5513794482207\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1390556\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -621.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 679       |\n",
      "|    time_elapsed         | 19271     |\n",
      "|    total_timesteps      | 1390592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.11e+09  |\n",
      "|    n_updates            | 6780      |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.38e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 680       |\n",
      "|    time_elapsed         | 19291     |\n",
      "|    total_timesteps      | 1392640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0259    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.92e+08  |\n",
      "|    n_updates            | 6790      |\n",
      "|    policy_gradient_loss | -2.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.21e+09  |\n",
      "---------------------------------------\n",
      "Episode 558  finished with cumulative reward: -7394000.0 and \n",
      "with an average reward of: -2956.4174330267892\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1393057\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2957.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 681       |\n",
      "|    time_elapsed         | 19319     |\n",
      "|    total_timesteps      | 1394688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0126    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.7e+09   |\n",
      "|    n_updates            | 6800      |\n",
      "|    policy_gradient_loss | -2.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.53e+10  |\n",
      "---------------------------------------\n",
      "Episode 559  finished with cumulative reward: -1401500.0 and \n",
      "with an average reward of: -560.3758496601359\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1395558\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -560.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 682       |\n",
      "|    time_elapsed         | 19346     |\n",
      "|    total_timesteps      | 1396736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0229    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.15e+09  |\n",
      "|    n_updates            | 6810      |\n",
      "|    policy_gradient_loss | -2.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.41e+09  |\n",
      "---------------------------------------\n",
      "Episode 560  finished with cumulative reward: -10326500.0 and \n",
      "with an average reward of: -4128.9484206317475\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1398059\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4130.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 683       |\n",
      "|    time_elapsed         | 19374     |\n",
      "|    total_timesteps      | 1398784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0234    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.5e+09   |\n",
      "|    n_updates            | 6820      |\n",
      "|    policy_gradient_loss | -3.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 561  finished with cumulative reward: -2217500.0 and \n",
      "with an average reward of: -886.6453418632547\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1400560\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -887.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 684       |\n",
      "|    time_elapsed         | 19402     |\n",
      "|    total_timesteps      | 1400832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0135    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.67e+09  |\n",
      "|    n_updates            | 6830      |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2e+10     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 685       |\n",
      "|    time_elapsed         | 19423     |\n",
      "|    total_timesteps      | 1402880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0252    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.06e+09  |\n",
      "|    n_updates            | 6840      |\n",
      "|    policy_gradient_loss | -4.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.35e+09  |\n",
      "---------------------------------------\n",
      "Episode 562  finished with cumulative reward: -9459500.0 and \n",
      "with an average reward of: -3782.2870851659336\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1403061\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3783.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 686       |\n",
      "|    time_elapsed         | 19452     |\n",
      "|    total_timesteps      | 1404928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0156    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.34e+10  |\n",
      "|    n_updates            | 6850      |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 563  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1405562\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 687       |\n",
      "|    time_elapsed         | 19480     |\n",
      "|    total_timesteps      | 1406976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0159    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.56e+09  |\n",
      "|    n_updates            | 6860      |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 564  finished with cumulative reward: -3722000.0 and \n",
      "with an average reward of: -1488.2047181127548\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1408063\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1488.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 688       |\n",
      "|    time_elapsed         | 19508     |\n",
      "|    total_timesteps      | 1409024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0192    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.46e+09  |\n",
      "|    n_updates            | 6870      |\n",
      "|    policy_gradient_loss | -2.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 565  finished with cumulative reward: -3467000.0 and \n",
      "with an average reward of: -1386.2455017992802\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1410564\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1386.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 689       |\n",
      "|    time_elapsed         | 19536     |\n",
      "|    total_timesteps      | 1411072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0237    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.74e+08  |\n",
      "|    n_updates            | 6880      |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.67e+09  |\n",
      "---------------------------------------\n",
      "Episode 566  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1413065\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 690       |\n",
      "|    time_elapsed         | 19564     |\n",
      "|    total_timesteps      | 1413120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.99e+09  |\n",
      "|    n_updates            | 6890      |\n",
      "|    policy_gradient_loss | -1.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.36e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 691       |\n",
      "|    time_elapsed         | 19585     |\n",
      "|    total_timesteps      | 1415168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0171    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.59e+09  |\n",
      "|    n_updates            | 6900      |\n",
      "|    policy_gradient_loss | -9.25e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 567  finished with cumulative reward: -4257500.0 and \n",
      "with an average reward of: -1702.3190723710516\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1415566\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1703.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 692       |\n",
      "|    time_elapsed         | 19613     |\n",
      "|    total_timesteps      | 1417216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0192    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.11e+09  |\n",
      "|    n_updates            | 6910      |\n",
      "|    policy_gradient_loss | -2.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n",
      "Episode 568  finished with cumulative reward: -9128000.0 and \n",
      "with an average reward of: -3649.7401039584165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1418067\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3651.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 693       |\n",
      "|    time_elapsed         | 19641     |\n",
      "|    total_timesteps      | 1419264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.019     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.5e+09   |\n",
      "|    n_updates            | 6920      |\n",
      "|    policy_gradient_loss | -1.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.59e+10  |\n",
      "---------------------------------------\n",
      "Episode 569  finished with cumulative reward: -6884000.0 and \n",
      "with an average reward of: -2752.49900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1420568\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2753.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 694       |\n",
      "|    time_elapsed         | 19669     |\n",
      "|    total_timesteps      | 1421312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0157    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.89e+09  |\n",
      "|    n_updates            | 6930      |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.45e+09  |\n",
      "---------------------------------------\n",
      "Episode 570  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1423069\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 695       |\n",
      "|    time_elapsed         | 19698     |\n",
      "|    total_timesteps      | 1423360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0183    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.6e+09   |\n",
      "|    n_updates            | 6940      |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 696       |\n",
      "|    time_elapsed         | 19719     |\n",
      "|    total_timesteps      | 1425408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0131    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.94e+09  |\n",
      "|    n_updates            | 6950      |\n",
      "|    policy_gradient_loss | -1.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 571  finished with cumulative reward: -4742000.0 and \n",
      "with an average reward of: -1896.0415833666534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1425570\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1896.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 697       |\n",
      "|    time_elapsed         | 19747     |\n",
      "|    total_timesteps      | 1427456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0199    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.04e+09  |\n",
      "|    n_updates            | 6960      |\n",
      "|    policy_gradient_loss | -3.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.25e+09  |\n",
      "---------------------------------------\n",
      "Episode 572  finished with cumulative reward: -7164500.0 and \n",
      "with an average reward of: -2864.654138344662\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1428071\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2865.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 698       |\n",
      "|    time_elapsed         | 19775     |\n",
      "|    total_timesteps      | 1429504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.013     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.85e+09  |\n",
      "|    n_updates            | 6970      |\n",
      "|    policy_gradient_loss | -2.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.44e+10  |\n",
      "---------------------------------------\n",
      "Episode 573  finished with cumulative reward: -1146500.0 and \n",
      "with an average reward of: -458.41663334666134\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1430572\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -458.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 699       |\n",
      "|    time_elapsed         | 19803     |\n",
      "|    total_timesteps      | 1431552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.81e+09  |\n",
      "|    n_updates            | 6980      |\n",
      "|    policy_gradient_loss | -2.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.24e+09  |\n",
      "---------------------------------------\n",
      "Episode 574  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1433073\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 700       |\n",
      "|    time_elapsed         | 19831     |\n",
      "|    total_timesteps      | 1433600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.86e+08  |\n",
      "|    n_updates            | 6990      |\n",
      "|    policy_gradient_loss | -2.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.96e+09  |\n",
      "---------------------------------------\n",
      "Episode 575  finished with cumulative reward: -7496000.0 and \n",
      "with an average reward of: -2997.201119552179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1435574\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2998.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 701       |\n",
      "|    time_elapsed         | 19858     |\n",
      "|    total_timesteps      | 1435648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0171    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.21e+10  |\n",
      "|    n_updates            | 7000      |\n",
      "|    policy_gradient_loss | -1.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 702       |\n",
      "|    time_elapsed         | 19878     |\n",
      "|    total_timesteps      | 1437696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0185    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 7010      |\n",
      "|    policy_gradient_loss | -2.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.21e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 576  finished with cumulative reward: -3467000.0 and \n",
      "with an average reward of: -1386.2455017992802\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1438075\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1386.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 703       |\n",
      "|    time_elapsed         | 19906     |\n",
      "|    total_timesteps      | 1439744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0285    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.98e+08  |\n",
      "|    n_updates            | 7020      |\n",
      "|    policy_gradient_loss | -2.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.81e+09  |\n",
      "---------------------------------------\n",
      "Episode 577  finished with cumulative reward: -6705500.0 and \n",
      "with an average reward of: -2681.127548980408\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1440576\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2682.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 704       |\n",
      "|    time_elapsed         | 19933     |\n",
      "|    total_timesteps      | 1441792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0171    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.22e+10  |\n",
      "|    n_updates            | 7030      |\n",
      "|    policy_gradient_loss | -9.02e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 578  finished with cumulative reward: -6935000.0 and \n",
      "with an average reward of: -2772.890843662535\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1443077\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2774.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 705       |\n",
      "|    time_elapsed         | 19961     |\n",
      "|    total_timesteps      | 1443840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.85e+09  |\n",
      "|    n_updates            | 7040      |\n",
      "|    policy_gradient_loss | -1.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 579  finished with cumulative reward: -968000.0 and \n",
      "with an average reward of: -387.0451819272291\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1445578\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -387.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 706       |\n",
      "|    time_elapsed         | 19989     |\n",
      "|    total_timesteps      | 1445888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.7e+09   |\n",
      "|    n_updates            | 7050      |\n",
      "|    policy_gradient_loss | -2.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.91e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 707          |\n",
      "|    time_elapsed         | 20010        |\n",
      "|    total_timesteps      | 1447936      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0246       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.19e+09     |\n",
      "|    n_updates            | 7060         |\n",
      "|    policy_gradient_loss | -3.44e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.83e+09     |\n",
      "------------------------------------------\n",
      "Episode 580  finished with cumulative reward: -7088000.0 and \n",
      "with an average reward of: -2834.0663734506197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1448079\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2835.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 708       |\n",
      "|    time_elapsed         | 20038     |\n",
      "|    total_timesteps      | 1449984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.012     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.28e+09  |\n",
      "|    n_updates            | 7070      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.51e+10  |\n",
      "---------------------------------------\n",
      "Episode 581  finished with cumulative reward: -5762000.0 and \n",
      "with an average reward of: -2303.878448620552\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1450580\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2304.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 709       |\n",
      "|    time_elapsed         | 20066     |\n",
      "|    total_timesteps      | 1452032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.25e+09  |\n",
      "|    n_updates            | 7080      |\n",
      "|    policy_gradient_loss | -2.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 582  finished with cumulative reward: -3926000.0 and \n",
      "with an average reward of: -1569.7720911635347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1453081\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1570.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 710       |\n",
      "|    time_elapsed         | 20093     |\n",
      "|    total_timesteps      | 1454080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.15e+09  |\n",
      "|    n_updates            | 7090      |\n",
      "|    policy_gradient_loss | -1.98e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.26e+09  |\n",
      "---------------------------------------\n",
      "Episode 583  finished with cumulative reward: -3390500.0 and \n",
      "with an average reward of: -1355.657736905238\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1455582\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1356.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 711       |\n",
      "|    time_elapsed         | 20121     |\n",
      "|    total_timesteps      | 1456128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.28e+09  |\n",
      "|    n_updates            | 7100      |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 584  finished with cumulative reward: -3824000.0 and \n",
      "with an average reward of: -1528.9884046381446\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1458083\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1529.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.83e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 712          |\n",
      "|    time_elapsed         | 20149        |\n",
      "|    total_timesteps      | 1458176      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0191       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.78e+09     |\n",
      "|    n_updates            | 7110         |\n",
      "|    policy_gradient_loss | -5.69e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.5e+09      |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 713       |\n",
      "|    time_elapsed         | 20170     |\n",
      "|    total_timesteps      | 1460224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.4e+08   |\n",
      "|    n_updates            | 7120      |\n",
      "|    policy_gradient_loss | -2.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.29e+09  |\n",
      "---------------------------------------\n",
      "Episode 585  finished with cumulative reward: -4385000.0 and \n",
      "with an average reward of: -1753.298680527789\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1460584\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1754.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 714       |\n",
      "|    time_elapsed         | 20198     |\n",
      "|    total_timesteps      | 1462272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0183    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.01e+09  |\n",
      "|    n_updates            | 7130      |\n",
      "|    policy_gradient_loss | -3.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.26e+10  |\n",
      "---------------------------------------\n",
      "Episode 586  finished with cumulative reward: -8541500.0 and \n",
      "with an average reward of: -3415.233906437425\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1463085\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3416.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 715       |\n",
      "|    time_elapsed         | 20226     |\n",
      "|    total_timesteps      | 1464320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.1e+09   |\n",
      "|    n_updates            | 7140      |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 587  finished with cumulative reward: -9612500.0 and \n",
      "with an average reward of: -3843.4626149540186\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1465586\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3845.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 716       |\n",
      "|    time_elapsed         | 20254     |\n",
      "|    total_timesteps      | 1466368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.018     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.19e+09  |\n",
      "|    n_updates            | 7150      |\n",
      "|    policy_gradient_loss | -1.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 588  finished with cumulative reward: -8516000.0 and \n",
      "with an average reward of: -3405.0379848060775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1468087\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3406.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 717       |\n",
      "|    time_elapsed         | 20283     |\n",
      "|    total_timesteps      | 1468416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0132    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 7160      |\n",
      "|    policy_gradient_loss | -3.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.69e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 718       |\n",
      "|    time_elapsed         | 20304     |\n",
      "|    total_timesteps      | 1470464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0129    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.44e+09  |\n",
      "|    n_updates            | 7170      |\n",
      "|    policy_gradient_loss | -1.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.93e+10  |\n",
      "---------------------------------------\n",
      "Episode 589  finished with cumulative reward: -7700000.0 and \n",
      "with an average reward of: -3078.7684926029588\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1470588\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3080.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 719       |\n",
      "|    time_elapsed         | 20332     |\n",
      "|    total_timesteps      | 1472512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0175    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.6e+09   |\n",
      "|    n_updates            | 7180      |\n",
      "|    policy_gradient_loss | -1.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.67e+10  |\n",
      "---------------------------------------\n",
      "Episode 590  finished with cumulative reward: -6705500.0 and \n",
      "with an average reward of: -2681.127548980408\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1473089\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2682.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 720       |\n",
      "|    time_elapsed         | 20360     |\n",
      "|    total_timesteps      | 1474560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0174    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.84e+09  |\n",
      "|    n_updates            | 7190      |\n",
      "|    policy_gradient_loss | -9.99e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1e+10     |\n",
      "---------------------------------------\n",
      "Episode 591  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1475590\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 721       |\n",
      "|    time_elapsed         | 20388     |\n",
      "|    total_timesteps      | 1476608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.019     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.97e+09  |\n",
      "|    n_updates            | 7200      |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 592  finished with cumulative reward: -9842000.0 and \n",
      "with an average reward of: -3935.2259096361454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1478091\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3936.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.9e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 72            |\n",
      "|    iterations           | 722           |\n",
      "|    time_elapsed         | 20417         |\n",
      "|    total_timesteps      | 1478656       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0161        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.15e+09      |\n",
      "|    n_updates            | 7210          |\n",
      "|    policy_gradient_loss | -4.37e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.02e+10      |\n",
      "-------------------------------------------\n",
      "Episode 593  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1480592\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 723       |\n",
      "|    time_elapsed         | 20445     |\n",
      "|    total_timesteps      | 1480704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0135    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.04e+10  |\n",
      "|    n_updates            | 7220      |\n",
      "|    policy_gradient_loss | -1.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.37e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 724       |\n",
      "|    time_elapsed         | 20466     |\n",
      "|    total_timesteps      | 1482752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0168    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.77e+09  |\n",
      "|    n_updates            | 7230      |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 594  finished with cumulative reward: -4487000.0 and \n",
      "with an average reward of: -1794.0823670531788\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1483093\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1794.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 725       |\n",
      "|    time_elapsed         | 20494     |\n",
      "|    total_timesteps      | 1484800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.018     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.52e+09  |\n",
      "|    n_updates            | 7240      |\n",
      "|    policy_gradient_loss | -2.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 595  finished with cumulative reward: -3900500.0 and \n",
      "with an average reward of: -1559.5761695321871\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1485594\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1560.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 726       |\n",
      "|    time_elapsed         | 20522     |\n",
      "|    total_timesteps      | 1486848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0206    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.48e+09  |\n",
      "|    n_updates            | 7250      |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.72e+09  |\n",
      "---------------------------------------\n",
      "Episode 596  finished with cumulative reward: -5736500.0 and \n",
      "with an average reward of: -2293.6825269892042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1488095\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2294.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 727       |\n",
      "|    time_elapsed         | 20550     |\n",
      "|    total_timesteps      | 1488896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0181    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.03e+09  |\n",
      "|    n_updates            | 7260      |\n",
      "|    policy_gradient_loss | -1.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 597  finished with cumulative reward: -7700000.0 and \n",
      "with an average reward of: -3078.7684926029588\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1490596\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3080.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 728       |\n",
      "|    time_elapsed         | 20578     |\n",
      "|    total_timesteps      | 1490944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0168    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.95e+09  |\n",
      "|    n_updates            | 7270      |\n",
      "|    policy_gradient_loss | -1.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 729       |\n",
      "|    time_elapsed         | 20599     |\n",
      "|    total_timesteps      | 1492992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0179    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.35e+09  |\n",
      "|    n_updates            | 7280      |\n",
      "|    policy_gradient_loss | -3.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 598  finished with cumulative reward: -8057000.0 and \n",
      "with an average reward of: -3221.5113954418234\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1493097\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3222.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 730       |\n",
      "|    time_elapsed         | 20627     |\n",
      "|    total_timesteps      | 1495040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0145    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.16e+09  |\n",
      "|    n_updates            | 7290      |\n",
      "|    policy_gradient_loss | -2.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 599  finished with cumulative reward: -9561500.0 and \n",
      "with an average reward of: -3823.0707716913234\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1495598\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3824.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 731       |\n",
      "|    time_elapsed         | 20656     |\n",
      "|    total_timesteps      | 1497088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0221    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.17e+10  |\n",
      "|    n_updates            | 7300      |\n",
      "|    policy_gradient_loss | -2.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.37e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 600  finished with cumulative reward: -7827500.0 and \n",
      "with an average reward of: -3129.748100759696\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1498099\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3131.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 732       |\n",
      "|    time_elapsed         | 20684     |\n",
      "|    total_timesteps      | 1499136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0158    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.6e+09   |\n",
      "|    n_updates            | 7310      |\n",
      "|    policy_gradient_loss | -3.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.44e+10  |\n",
      "---------------------------------------\n",
      "Episode 601  finished with cumulative reward: -5481500.0 and \n",
      "with an average reward of: -2191.72331067573\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1500600\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2192.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 733       |\n",
      "|    time_elapsed         | 20712     |\n",
      "|    total_timesteps      | 1501184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.02      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.51e+09  |\n",
      "|    n_updates            | 7320      |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 602  finished with cumulative reward: -7572500.0 and \n",
      "with an average reward of: -3027.7888844462213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1503101\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3029.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 734       |\n",
      "|    time_elapsed         | 20739     |\n",
      "|    total_timesteps      | 1503232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0181    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.06e+09  |\n",
      "|    n_updates            | 7330      |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.87e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 735       |\n",
      "|    time_elapsed         | 20760     |\n",
      "|    total_timesteps      | 1505280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0211    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.26e+10  |\n",
      "|    n_updates            | 7340      |\n",
      "|    policy_gradient_loss | -2.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.51e+10  |\n",
      "---------------------------------------\n",
      "Episode 603  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1505602\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 736       |\n",
      "|    time_elapsed         | 20788     |\n",
      "|    total_timesteps      | 1507328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0159    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.09e+09  |\n",
      "|    n_updates            | 7350      |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.8e+10   |\n",
      "---------------------------------------\n",
      "Episode 604  finished with cumulative reward: -8082500.0 and \n",
      "with an average reward of: -3231.7073170731705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1508103\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3233.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 737       |\n",
      "|    time_elapsed         | 20815     |\n",
      "|    total_timesteps      | 1509376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0246    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.65e+09  |\n",
      "|    n_updates            | 7360      |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 605  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1510604\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 738       |\n",
      "|    time_elapsed         | 20843     |\n",
      "|    total_timesteps      | 1511424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.33e+09  |\n",
      "|    n_updates            | 7370      |\n",
      "|    policy_gradient_loss | -1.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 606  finished with cumulative reward: -8388500.0 and \n",
      "with an average reward of: -3354.05837664934\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1513105\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3355.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 739       |\n",
      "|    time_elapsed         | 20871     |\n",
      "|    total_timesteps      | 1513472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0156    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.97e+09  |\n",
      "|    n_updates            | 7380      |\n",
      "|    policy_gradient_loss | -1.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 740       |\n",
      "|    time_elapsed         | 20892     |\n",
      "|    total_timesteps      | 1515520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0176    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.24e+10  |\n",
      "|    n_updates            | 7390      |\n",
      "|    policy_gradient_loss | -1.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.74e+10  |\n",
      "---------------------------------------\n",
      "Episode 607  finished with cumulative reward: -9561500.0 and \n",
      "with an average reward of: -3823.0707716913234\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1515606\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3824.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 741       |\n",
      "|    time_elapsed         | 20920     |\n",
      "|    total_timesteps      | 1517568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0137    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 7400      |\n",
      "|    policy_gradient_loss | -1.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.34e+10  |\n",
      "---------------------------------------\n",
      "Episode 608  finished with cumulative reward: -7241000.0 and \n",
      "with an average reward of: -2895.2419032387047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1518107\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2896.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 742       |\n",
      "|    time_elapsed         | 20948     |\n",
      "|    total_timesteps      | 1519616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0214    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.35e+09  |\n",
      "|    n_updates            | 7410      |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.52e+09  |\n",
      "---------------------------------------\n",
      "Episode 609  finished with cumulative reward: -3543500.0 and \n",
      "with an average reward of: -1416.8332666933227\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1520608\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1417.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 743       |\n",
      "|    time_elapsed         | 20977     |\n",
      "|    total_timesteps      | 1521664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0194    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.36e+09  |\n",
      "|    n_updates            | 7420      |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "Episode 610  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1523109\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 744       |\n",
      "|    time_elapsed         | 21004     |\n",
      "|    total_timesteps      | 1523712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0179    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.83e+09  |\n",
      "|    n_updates            | 7430      |\n",
      "|    policy_gradient_loss | -2.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 611  finished with cumulative reward: -11321000.0 and \n",
      "with an average reward of: -4526.589364254298\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1525610\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4528.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 745       |\n",
      "|    time_elapsed         | 21032     |\n",
      "|    total_timesteps      | 1525760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0213    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.57e+09  |\n",
      "|    n_updates            | 7440      |\n",
      "|    policy_gradient_loss | -1.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.79e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 746       |\n",
      "|    time_elapsed         | 21052     |\n",
      "|    total_timesteps      | 1527808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0167    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.42e+10  |\n",
      "|    n_updates            | 7450      |\n",
      "|    policy_gradient_loss | -1.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.43e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 612  finished with cumulative reward: -6246500.0 and \n",
      "with an average reward of: -2497.6009596161534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1528111\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2498.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 747       |\n",
      "|    time_elapsed         | 21079     |\n",
      "|    total_timesteps      | 1529856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0181    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9e+09     |\n",
      "|    n_updates            | 7460      |\n",
      "|    policy_gradient_loss | -2.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 613  finished with cumulative reward: -764000.0 and \n",
      "with an average reward of: -305.47780887644944\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1530612\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -305.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 748       |\n",
      "|    time_elapsed         | 21106     |\n",
      "|    total_timesteps      | 1531904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0211    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.61e+09  |\n",
      "|    n_updates            | 7470      |\n",
      "|    policy_gradient_loss | -3.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.52e+09  |\n",
      "---------------------------------------\n",
      "Episode 614  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1533113\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 749       |\n",
      "|    time_elapsed         | 21135     |\n",
      "|    total_timesteps      | 1533952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.21e+09  |\n",
      "|    n_updates            | 7480      |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.27e+09  |\n",
      "---------------------------------------\n",
      "Episode 615  finished with cumulative reward: -1733000.0 and \n",
      "with an average reward of: -692.9228308676529\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1535614\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -693.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 750       |\n",
      "|    time_elapsed         | 21162     |\n",
      "|    total_timesteps      | 1536000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.014     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.76e+09  |\n",
      "|    n_updates            | 7490      |\n",
      "|    policy_gradient_loss | -1.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 751       |\n",
      "|    time_elapsed         | 21183     |\n",
      "|    total_timesteps      | 1538048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0244    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.58e+09  |\n",
      "|    n_updates            | 7500      |\n",
      "|    policy_gradient_loss | -2.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.44e+09  |\n",
      "---------------------------------------\n",
      "Episode 616  finished with cumulative reward: -6221000.0 and \n",
      "with an average reward of: -2487.405037984806\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1538115\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2488.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 752       |\n",
      "|    time_elapsed         | 21211     |\n",
      "|    total_timesteps      | 1540096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0122    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.17e+10  |\n",
      "|    n_updates            | 7510      |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 617  finished with cumulative reward: -8949500.0 and \n",
      "with an average reward of: -3578.3686525389844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1540616\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3579.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 753       |\n",
      "|    time_elapsed         | 21239     |\n",
      "|    total_timesteps      | 1542144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0156    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.03e+10  |\n",
      "|    n_updates            | 7520      |\n",
      "|    policy_gradient_loss | -1.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.95e+10  |\n",
      "---------------------------------------\n",
      "Episode 618  finished with cumulative reward: -2141000.0 and \n",
      "with an average reward of: -856.0575769692123\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1543117\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -856.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 754       |\n",
      "|    time_elapsed         | 21266     |\n",
      "|    total_timesteps      | 1544192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0253    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.68e+09  |\n",
      "|    n_updates            | 7530      |\n",
      "|    policy_gradient_loss | -3.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.87e+09  |\n",
      "---------------------------------------\n",
      "Episode 619  finished with cumulative reward: -6374000.0 and \n",
      "with an average reward of: -2548.580567772891\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1545618\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2549.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 755       |\n",
      "|    time_elapsed         | 21293     |\n",
      "|    total_timesteps      | 1546240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0257    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.63e+09  |\n",
      "|    n_updates            | 7540      |\n",
      "|    policy_gradient_loss | -1.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.63e+09  |\n",
      "---------------------------------------\n",
      "Episode 620  finished with cumulative reward: -7368500.0 and \n",
      "with an average reward of: -2946.2215113954417\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1548119\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2947.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 756       |\n",
      "|    time_elapsed         | 21320     |\n",
      "|    total_timesteps      | 1548288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.017     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.18e+09  |\n",
      "|    n_updates            | 7550      |\n",
      "|    policy_gradient_loss | -1.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.99e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 757       |\n",
      "|    time_elapsed         | 21341     |\n",
      "|    total_timesteps      | 1550336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0166    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.79e+09  |\n",
      "|    n_updates            | 7560      |\n",
      "|    policy_gradient_loss | -1.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.17e+10  |\n",
      "---------------------------------------\n",
      "Episode 621  finished with cumulative reward: -3161000.0 and \n",
      "with an average reward of: -1263.8944422231107\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1550620\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1264.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 758       |\n",
      "|    time_elapsed         | 21369     |\n",
      "|    total_timesteps      | 1552384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0249    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.82e+09  |\n",
      "|    n_updates            | 7570      |\n",
      "|    policy_gradient_loss | -4.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.18e+09  |\n",
      "---------------------------------------\n",
      "Episode 622  finished with cumulative reward: -8592500.0 and \n",
      "with an average reward of: -3435.6257497001197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1553121\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3437.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 759       |\n",
      "|    time_elapsed         | 21397     |\n",
      "|    total_timesteps      | 1554432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.12e+10  |\n",
      "|    n_updates            | 7580      |\n",
      "|    policy_gradient_loss | -1.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.9e+10   |\n",
      "---------------------------------------\n",
      "Episode 623  finished with cumulative reward: -12188000.0 and \n",
      "with an average reward of: -4873.250699720112\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1555622\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4875.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 760       |\n",
      "|    time_elapsed         | 21425     |\n",
      "|    total_timesteps      | 1556480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.02      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.37e+09  |\n",
      "|    n_updates            | 7590      |\n",
      "|    policy_gradient_loss | -2.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 624  finished with cumulative reward: -9000500.0 and \n",
      "with an average reward of: -3598.7604958016796\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1558123\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3600.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 761          |\n",
      "|    time_elapsed         | 21452        |\n",
      "|    total_timesteps      | 1558528      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0163       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.56e+10     |\n",
      "|    n_updates            | 7600         |\n",
      "|    policy_gradient_loss | -5.5e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.7e+10      |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 762       |\n",
      "|    time_elapsed         | 21473     |\n",
      "|    total_timesteps      | 1560576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0185    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.37e+09  |\n",
      "|    n_updates            | 7610      |\n",
      "|    policy_gradient_loss | -4.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 625  finished with cumulative reward: -5583500.0 and \n",
      "with an average reward of: -2232.5069972011197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1560624\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2233.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 763       |\n",
      "|    time_elapsed         | 21500     |\n",
      "|    total_timesteps      | 1562624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0223    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.94e+09  |\n",
      "|    n_updates            | 7620      |\n",
      "|    policy_gradient_loss | -2.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 626  finished with cumulative reward: -3824000.0 and \n",
      "with an average reward of: -1528.9884046381446\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1563125\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1529.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 764       |\n",
      "|    time_elapsed         | 21527     |\n",
      "|    total_timesteps      | 1564672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.024     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.76e+09  |\n",
      "|    n_updates            | 7630      |\n",
      "|    policy_gradient_loss | -4.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.84e+09  |\n",
      "---------------------------------------\n",
      "Episode 627  finished with cumulative reward: -4971500.0 and \n",
      "with an average reward of: -1987.8048780487804\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1565626\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1988.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 765       |\n",
      "|    time_elapsed         | 21555     |\n",
      "|    total_timesteps      | 1566720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0154    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4e+09     |\n",
      "|    n_updates            | 7640      |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.82e+09  |\n",
      "---------------------------------------\n",
      "Episode 628  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1568127\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 766       |\n",
      "|    time_elapsed         | 21582     |\n",
      "|    total_timesteps      | 1568768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0153    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.21e+10  |\n",
      "|    n_updates            | 7650      |\n",
      "|    policy_gradient_loss | -3.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 629  finished with cumulative reward: -13565000.0 and \n",
      "with an average reward of: -5423.830467812875\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1570628\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5426.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 767       |\n",
      "|    time_elapsed         | 21610     |\n",
      "|    total_timesteps      | 1570816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0201    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8e+09     |\n",
      "|    n_updates            | 7660      |\n",
      "|    policy_gradient_loss | -2.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 768       |\n",
      "|    time_elapsed         | 21632     |\n",
      "|    total_timesteps      | 1572864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0178    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.76e+09  |\n",
      "|    n_updates            | 7670      |\n",
      "|    policy_gradient_loss | -1.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.23e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 630  finished with cumulative reward: -3824000.0 and \n",
      "with an average reward of: -1528.9884046381446\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1573129\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1529.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 769       |\n",
      "|    time_elapsed         | 21660     |\n",
      "|    total_timesteps      | 1574912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.05e+09  |\n",
      "|    n_updates            | 7680      |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.73e+09  |\n",
      "---------------------------------------\n",
      "Episode 631  finished with cumulative reward: -4614500.0 and \n",
      "with an average reward of: -1845.061975209916\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1575630\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1845.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 770       |\n",
      "|    time_elapsed         | 21688     |\n",
      "|    total_timesteps      | 1576960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.22e+09  |\n",
      "|    n_updates            | 7690      |\n",
      "|    policy_gradient_loss | -2.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.29e+09  |\n",
      "---------------------------------------\n",
      "Episode 632  finished with cumulative reward: -6501500.0 and \n",
      "with an average reward of: -2599.5601759296283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1578131\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2600.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.24e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 72            |\n",
      "|    iterations           | 771           |\n",
      "|    time_elapsed         | 21717         |\n",
      "|    total_timesteps      | 1579008       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0225        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.3e+09       |\n",
      "|    n_updates            | 7700          |\n",
      "|    policy_gradient_loss | -5.88e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.47e+09      |\n",
      "-------------------------------------------\n",
      "Episode 633  finished with cumulative reward: -4844000.0 and \n",
      "with an average reward of: -1936.8252698920433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1580632\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1937.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 772       |\n",
      "|    time_elapsed         | 21746     |\n",
      "|    total_timesteps      | 1581056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.71e+09  |\n",
      "|    n_updates            | 7710      |\n",
      "|    policy_gradient_loss | -3.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.89e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 773       |\n",
      "|    time_elapsed         | 21770     |\n",
      "|    total_timesteps      | 1583104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0201    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.33e+09  |\n",
      "|    n_updates            | 7720      |\n",
      "|    policy_gradient_loss | -1.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 634  finished with cumulative reward: -7547000.0 and \n",
      "with an average reward of: -3017.592962814874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1583133\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3018.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 774       |\n",
      "|    time_elapsed         | 21800     |\n",
      "|    total_timesteps      | 1585152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0219    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.49e+09  |\n",
      "|    n_updates            | 7730      |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 635  finished with cumulative reward: -2217500.0 and \n",
      "with an average reward of: -886.6453418632547\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1585634\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -887.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 775       |\n",
      "|    time_elapsed         | 21828     |\n",
      "|    total_timesteps      | 1587200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0248    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.55e+09  |\n",
      "|    n_updates            | 7740      |\n",
      "|    policy_gradient_loss | -1.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.98e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 636  finished with cumulative reward: -8414000.0 and \n",
      "with an average reward of: -3364.2542982806876\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1588135\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3365.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 776       |\n",
      "|    time_elapsed         | 21857     |\n",
      "|    total_timesteps      | 1589248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0222    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.57e+09  |\n",
      "|    n_updates            | 7750      |\n",
      "|    policy_gradient_loss | -1.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.34e+10  |\n",
      "---------------------------------------\n",
      "Episode 637  finished with cumulative reward: -4359500.0 and \n",
      "with an average reward of: -1743.1027588964414\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1590636\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1743.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 777       |\n",
      "|    time_elapsed         | 21885     |\n",
      "|    total_timesteps      | 1591296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0234    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.19e+09  |\n",
      "|    n_updates            | 7760      |\n",
      "|    policy_gradient_loss | -3.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 638  finished with cumulative reward: -2396000.0 and \n",
      "with an average reward of: -958.0167932826869\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1593137\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -958.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 778       |\n",
      "|    time_elapsed         | 21913     |\n",
      "|    total_timesteps      | 1593344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.85e+09  |\n",
      "|    n_updates            | 7770      |\n",
      "|    policy_gradient_loss | -3.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.14e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 779       |\n",
      "|    time_elapsed         | 21934     |\n",
      "|    total_timesteps      | 1595392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0211    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.58e+09  |\n",
      "|    n_updates            | 7780      |\n",
      "|    policy_gradient_loss | -2.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.94e+09  |\n",
      "---------------------------------------\n",
      "Episode 639  finished with cumulative reward: -6705500.0 and \n",
      "with an average reward of: -2681.127548980408\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1595638\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2682.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.15e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 780       |\n",
      "|    time_elapsed         | 21964     |\n",
      "|    total_timesteps      | 1597440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0195    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.27e+09  |\n",
      "|    n_updates            | 7790      |\n",
      "|    policy_gradient_loss | -1.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.33e+10  |\n",
      "---------------------------------------\n",
      "Episode 640  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1598139\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 781       |\n",
      "|    time_elapsed         | 21991     |\n",
      "|    total_timesteps      | 1599488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.023     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.18e+09  |\n",
      "|    n_updates            | 7800      |\n",
      "|    policy_gradient_loss | -1.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.3e+09   |\n",
      "---------------------------------------\n",
      "Episode 641  finished with cumulative reward: -3926000.0 and \n",
      "with an average reward of: -1569.7720911635347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1600640\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1570.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 782       |\n",
      "|    time_elapsed         | 22020     |\n",
      "|    total_timesteps      | 1601536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.31e+09  |\n",
      "|    n_updates            | 7810      |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.02e+09  |\n",
      "---------------------------------------\n",
      "Episode 642  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1603141\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 783       |\n",
      "|    time_elapsed         | 22047     |\n",
      "|    total_timesteps      | 1603584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0228    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.1e+09   |\n",
      "|    n_updates            | 7820      |\n",
      "|    policy_gradient_loss | -1.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.11e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 784       |\n",
      "|    time_elapsed         | 22068     |\n",
      "|    total_timesteps      | 1605632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0202    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.17e+09  |\n",
      "|    n_updates            | 7830      |\n",
      "|    policy_gradient_loss | -1.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 643  finished with cumulative reward: -5099000.0 and \n",
      "with an average reward of: -2038.7844862055179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1605642\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2039.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 785       |\n",
      "|    time_elapsed         | 22096     |\n",
      "|    total_timesteps      | 1607680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.019     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.59e+09  |\n",
      "|    n_updates            | 7840      |\n",
      "|    policy_gradient_loss | -6.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 644  finished with cumulative reward: -3620000.0 and \n",
      "with an average reward of: -1447.421031587365\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1608143\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1448.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 786       |\n",
      "|    time_elapsed         | 22124     |\n",
      "|    total_timesteps      | 1609728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0226    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.3e+09   |\n",
      "|    n_updates            | 7850      |\n",
      "|    policy_gradient_loss | -2.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6e+09     |\n",
      "---------------------------------------\n",
      "Episode 645  finished with cumulative reward: -4334000.0 and \n",
      "with an average reward of: -1732.906837265094\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1610644\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1733.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 787       |\n",
      "|    time_elapsed         | 22152     |\n",
      "|    total_timesteps      | 1611776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0242    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.85e+09  |\n",
      "|    n_updates            | 7860      |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 646  finished with cumulative reward: -7062500.0 and \n",
      "with an average reward of: -2823.870451819272\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1613145\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2825.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 788       |\n",
      "|    time_elapsed         | 22181     |\n",
      "|    total_timesteps      | 1613824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0181    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.25e+10  |\n",
      "|    n_updates            | 7870      |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.97e+10  |\n",
      "---------------------------------------\n",
      "Episode 647  finished with cumulative reward: -3237500.0 and \n",
      "with an average reward of: -1294.4822071171532\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1615646\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1295.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.98e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 789          |\n",
      "|    time_elapsed         | 22208        |\n",
      "|    total_timesteps      | 1615872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0268       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.45e+09     |\n",
      "|    n_updates            | 7880         |\n",
      "|    policy_gradient_loss | -3.72e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.68e+09     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 790       |\n",
      "|    time_elapsed         | 22228     |\n",
      "|    total_timesteps      | 1617920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0244    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.56e+09  |\n",
      "|    n_updates            | 7890      |\n",
      "|    policy_gradient_loss | -2.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.23e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 648  finished with cumulative reward: -6935000.0 and \n",
      "with an average reward of: -2772.890843662535\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1618147\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2774.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 791       |\n",
      "|    time_elapsed         | 22257     |\n",
      "|    total_timesteps      | 1619968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0184    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.1e+10   |\n",
      "|    n_updates            | 7900      |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 649  finished with cumulative reward: -2906000.0 and \n",
      "with an average reward of: -1161.935225909636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1620648\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1162.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 792       |\n",
      "|    time_elapsed         | 22284     |\n",
      "|    total_timesteps      | 1622016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0236    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+09  |\n",
      "|    n_updates            | 7910      |\n",
      "|    policy_gradient_loss | -2.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.82e+09  |\n",
      "---------------------------------------\n",
      "Episode 650  finished with cumulative reward: -8133500.0 and \n",
      "with an average reward of: -3252.0991603358657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1623149\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3253.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 793       |\n",
      "|    time_elapsed         | 22312     |\n",
      "|    total_timesteps      | 1624064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0213    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.11e+09  |\n",
      "|    n_updates            | 7920      |\n",
      "|    policy_gradient_loss | -1.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 651  finished with cumulative reward: -3594500.0 and \n",
      "with an average reward of: -1437.2251099560176\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1625650\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1437.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 794       |\n",
      "|    time_elapsed         | 22340     |\n",
      "|    total_timesteps      | 1626112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.36e+09  |\n",
      "|    n_updates            | 7930      |\n",
      "|    policy_gradient_loss | -3.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.72e+09  |\n",
      "---------------------------------------\n",
      "Episode 652  finished with cumulative reward: -3747500.0 and \n",
      "with an average reward of: -1498.4006397441024\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1628151\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1499.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 795       |\n",
      "|    time_elapsed         | 22367     |\n",
      "|    total_timesteps      | 1628160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0204    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.27e+09  |\n",
      "|    n_updates            | 7940      |\n",
      "|    policy_gradient_loss | -3.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.67e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 796       |\n",
      "|    time_elapsed         | 22388     |\n",
      "|    total_timesteps      | 1630208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0163    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.29e+09  |\n",
      "|    n_updates            | 7950      |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.61e+09  |\n",
      "---------------------------------------\n",
      "Episode 653  finished with cumulative reward: -3875000.0 and \n",
      "with an average reward of: -1549.3802479008398\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1630652\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1550.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 797       |\n",
      "|    time_elapsed         | 22416     |\n",
      "|    total_timesteps      | 1632256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0173    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.5e+09   |\n",
      "|    n_updates            | 7960      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.62e+09  |\n",
      "---------------------------------------\n",
      "Episode 654  finished with cumulative reward: -7190000.0 and \n",
      "with an average reward of: -2874.8500599760096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1633153\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2876.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 798       |\n",
      "|    time_elapsed         | 22443     |\n",
      "|    total_timesteps      | 1634304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0226    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.03e+10  |\n",
      "|    n_updates            | 7970      |\n",
      "|    policy_gradient_loss | -1.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.74e+10  |\n",
      "---------------------------------------\n",
      "Episode 655  finished with cumulative reward: -6578000.0 and \n",
      "with an average reward of: -2630.1479408236705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1635654\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2631.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 799       |\n",
      "|    time_elapsed         | 22470     |\n",
      "|    total_timesteps      | 1636352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.52e+09  |\n",
      "|    n_updates            | 7980      |\n",
      "|    policy_gradient_loss | -2.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.94e+09  |\n",
      "---------------------------------------\n",
      "Episode 656  finished with cumulative reward: -4818500.0 and \n",
      "with an average reward of: -1926.6293482606957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1638155\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1927.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 800       |\n",
      "|    time_elapsed         | 22497     |\n",
      "|    total_timesteps      | 1638400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.017     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.21e+09  |\n",
      "|    n_updates            | 7990      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.08e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 801       |\n",
      "|    time_elapsed         | 22518     |\n",
      "|    total_timesteps      | 1640448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0249    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.21e+09  |\n",
      "|    n_updates            | 8000      |\n",
      "|    policy_gradient_loss | -1.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.14e+09  |\n",
      "---------------------------------------\n",
      "Episode 657  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1640656\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.9e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 72       |\n",
      "|    iterations           | 802      |\n",
      "|    time_elapsed         | 22547    |\n",
      "|    total_timesteps      | 1642496  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0235   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.39e+09 |\n",
      "|    n_updates            | 8010     |\n",
      "|    policy_gradient_loss | -2e-06   |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 8.98e+09 |\n",
      "--------------------------------------\n",
      "Episode 658  finished with cumulative reward: -4691000.0 and \n",
      "with an average reward of: -1875.6497401039585\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1643157\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1876.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 803       |\n",
      "|    time_elapsed         | 22575     |\n",
      "|    total_timesteps      | 1644544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0235    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.39e+09  |\n",
      "|    n_updates            | 8020      |\n",
      "|    policy_gradient_loss | -2.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.83e+09  |\n",
      "---------------------------------------\n",
      "Episode 659  finished with cumulative reward: -3365000.0 and \n",
      "with an average reward of: -1345.4618152738904\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1645658\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1346.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 804       |\n",
      "|    time_elapsed         | 22604     |\n",
      "|    total_timesteps      | 1646592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.35e+09  |\n",
      "|    n_updates            | 8030      |\n",
      "|    policy_gradient_loss | -2.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.65e+09  |\n",
      "---------------------------------------\n",
      "Episode 660  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1648159\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 805       |\n",
      "|    time_elapsed         | 22632     |\n",
      "|    total_timesteps      | 1648640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.94e+09  |\n",
      "|    n_updates            | 8040      |\n",
      "|    policy_gradient_loss | -2.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.72e+09  |\n",
      "---------------------------------------\n",
      "Episode 661  finished with cumulative reward: -8720000.0 and \n",
      "with an average reward of: -3486.605357856857\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1650660\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3488.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 806       |\n",
      "|    time_elapsed         | 22660     |\n",
      "|    total_timesteps      | 1650688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0252    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.88e+09  |\n",
      "|    n_updates            | 8050      |\n",
      "|    policy_gradient_loss | -2.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.9e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 72       |\n",
      "|    iterations           | 807      |\n",
      "|    time_elapsed         | 22681    |\n",
      "|    total_timesteps      | 1652736  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0206   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 9.32e+09 |\n",
      "|    n_updates            | 8060     |\n",
      "|    policy_gradient_loss | -1e-06   |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.99e+10 |\n",
      "--------------------------------------\n",
      "Episode 662  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1653161\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 808       |\n",
      "|    time_elapsed         | 22709     |\n",
      "|    total_timesteps      | 1654784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0193    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.06e+10  |\n",
      "|    n_updates            | 8070      |\n",
      "|    policy_gradient_loss | -2.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.93e+10  |\n",
      "---------------------------------------\n",
      "Episode 663  finished with cumulative reward: -2370500.0 and \n",
      "with an average reward of: -947.8208716513394\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1655662\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -948.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 809       |\n",
      "|    time_elapsed         | 22737     |\n",
      "|    total_timesteps      | 1656832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0256    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.22e+09  |\n",
      "|    n_updates            | 8080      |\n",
      "|    policy_gradient_loss | -1.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.51e+09  |\n",
      "---------------------------------------\n",
      "Episode 664  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1658163\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 810       |\n",
      "|    time_elapsed         | 22765     |\n",
      "|    total_timesteps      | 1658880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0212    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.84e+09  |\n",
      "|    n_updates            | 8090      |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 665  finished with cumulative reward: -7215500.0 and \n",
      "with an average reward of: -2885.045981607357\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1660664\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2886.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 811       |\n",
      "|    time_elapsed         | 22794     |\n",
      "|    total_timesteps      | 1660928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0208    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.78e+09  |\n",
      "|    n_updates            | 8100      |\n",
      "|    policy_gradient_loss | -1.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 812       |\n",
      "|    time_elapsed         | 22815     |\n",
      "|    total_timesteps      | 1662976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0201    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.42e+09  |\n",
      "|    n_updates            | 8110      |\n",
      "|    policy_gradient_loss | -2.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.33e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 666  finished with cumulative reward: -8618000.0 and \n",
      "with an average reward of: -3445.8216713314673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1663165\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3447.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 813       |\n",
      "|    time_elapsed         | 22843     |\n",
      "|    total_timesteps      | 1665024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.71e+09  |\n",
      "|    n_updates            | 8120      |\n",
      "|    policy_gradient_loss | -2.87e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+10  |\n",
      "---------------------------------------\n",
      "Episode 667  finished with cumulative reward: -7343000.0 and \n",
      "with an average reward of: -2936.0255897640945\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1665666\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2937.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 814       |\n",
      "|    time_elapsed         | 22871     |\n",
      "|    total_timesteps      | 1667072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.57e+09  |\n",
      "|    n_updates            | 8130      |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.87e+10  |\n",
      "---------------------------------------\n",
      "Episode 668  finished with cumulative reward: -5201000.0 and \n",
      "with an average reward of: -2079.5681727309075\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1668167\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2080.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 815       |\n",
      "|    time_elapsed         | 22899     |\n",
      "|    total_timesteps      | 1669120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.22e+09  |\n",
      "|    n_updates            | 8140      |\n",
      "|    policy_gradient_loss | -3.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 669  finished with cumulative reward: -11397500.0 and \n",
      "with an average reward of: -4557.177129148341\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1670668\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4559.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 816       |\n",
      "|    time_elapsed         | 22926     |\n",
      "|    total_timesteps      | 1671168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0227    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.12e+09  |\n",
      "|    n_updates            | 8150      |\n",
      "|    policy_gradient_loss | -3.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.82e+09  |\n",
      "---------------------------------------\n",
      "Episode 670  finished with cumulative reward: -5711000.0 and \n",
      "with an average reward of: -2283.4866053578567\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1673169\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2284.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 817       |\n",
      "|    time_elapsed         | 22954     |\n",
      "|    total_timesteps      | 1673216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0185    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.36e+09  |\n",
      "|    n_updates            | 8160      |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.66e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 818       |\n",
      "|    time_elapsed         | 22975     |\n",
      "|    total_timesteps      | 1675264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0226    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.19e+09  |\n",
      "|    n_updates            | 8170      |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 671  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1675670\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 819       |\n",
      "|    time_elapsed         | 23003     |\n",
      "|    total_timesteps      | 1677312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.27e+09  |\n",
      "|    n_updates            | 8180      |\n",
      "|    policy_gradient_loss | -2.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.59e+10  |\n",
      "---------------------------------------\n",
      "Episode 672  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1678171\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 820       |\n",
      "|    time_elapsed         | 23032     |\n",
      "|    total_timesteps      | 1679360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.32e+09  |\n",
      "|    n_updates            | 8190      |\n",
      "|    policy_gradient_loss | -1.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 673  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1680672\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 821       |\n",
      "|    time_elapsed         | 23060     |\n",
      "|    total_timesteps      | 1681408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0207    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.35e+09  |\n",
      "|    n_updates            | 8200      |\n",
      "|    policy_gradient_loss | -3.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 674  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1683173\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 822       |\n",
      "|    time_elapsed         | 23088     |\n",
      "|    total_timesteps      | 1683456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0209    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.78e+09  |\n",
      "|    n_updates            | 8210      |\n",
      "|    policy_gradient_loss | -1.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.83e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 823       |\n",
      "|    time_elapsed         | 23109     |\n",
      "|    total_timesteps      | 1685504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.22e+09  |\n",
      "|    n_updates            | 8220      |\n",
      "|    policy_gradient_loss | -2.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.06e+10  |\n",
      "---------------------------------------\n",
      "Episode 675  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1685674\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 824       |\n",
      "|    time_elapsed         | 23136     |\n",
      "|    total_timesteps      | 1687552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.71e+09  |\n",
      "|    n_updates            | 8230      |\n",
      "|    policy_gradient_loss | -2.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.25e+10  |\n",
      "---------------------------------------\n",
      "Episode 676  finished with cumulative reward: -11168000.0 and \n",
      "with an average reward of: -4465.413834466213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1688175\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4467.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 825       |\n",
      "|    time_elapsed         | 23164     |\n",
      "|    total_timesteps      | 1689600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.87e+09  |\n",
      "|    n_updates            | 8240      |\n",
      "|    policy_gradient_loss | -1.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 677  finished with cumulative reward: -9842000.0 and \n",
      "with an average reward of: -3935.2259096361454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1690676\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3936.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.15e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 826       |\n",
      "|    time_elapsed         | 23193     |\n",
      "|    total_timesteps      | 1691648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.39e+09  |\n",
      "|    n_updates            | 8250      |\n",
      "|    policy_gradient_loss | -2.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.78e+10  |\n",
      "---------------------------------------\n",
      "Episode 678  finished with cumulative reward: -6297500.0 and \n",
      "with an average reward of: -2517.9928028788486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1693177\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2519.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.15e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 827       |\n",
      "|    time_elapsed         | 23220     |\n",
      "|    total_timesteps      | 1693696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0207    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.23e+09  |\n",
      "|    n_updates            | 8260      |\n",
      "|    policy_gradient_loss | -3.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.58e+10  |\n",
      "---------------------------------------\n",
      "Episode 679  finished with cumulative reward: -2982500.0 and \n",
      "with an average reward of: -1192.5229908036786\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1695678\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1193.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 828       |\n",
      "|    time_elapsed         | 23248     |\n",
      "|    total_timesteps      | 1695744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0188    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.13e+10  |\n",
      "|    n_updates            | 8270      |\n",
      "|    policy_gradient_loss | -2.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.25e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.17e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 829          |\n",
      "|    time_elapsed         | 23269        |\n",
      "|    total_timesteps      | 1697792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.024        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.68e+09     |\n",
      "|    n_updates            | 8280         |\n",
      "|    policy_gradient_loss | -5.56e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.09e+09     |\n",
      "------------------------------------------\n",
      "Episode 680  finished with cumulative reward: -3722000.0 and \n",
      "with an average reward of: -1488.2047181127548\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1698179\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1488.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 830       |\n",
      "|    time_elapsed         | 23296     |\n",
      "|    total_timesteps      | 1699840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.75e+09  |\n",
      "|    n_updates            | 8290      |\n",
      "|    policy_gradient_loss | -1.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.16e+09  |\n",
      "---------------------------------------\n",
      "Episode 681  finished with cumulative reward: -177500.0 and \n",
      "with an average reward of: -70.97161135545781\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1700680\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -71.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 72           |\n",
      "|    iterations           | 831          |\n",
      "|    time_elapsed         | 23323        |\n",
      "|    total_timesteps      | 1701888      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0303       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.38e+08     |\n",
      "|    n_updates            | 8300         |\n",
      "|    policy_gradient_loss | -4.98e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.7e+09      |\n",
      "------------------------------------------\n",
      "Episode 682  finished with cumulative reward: -10275500.0 and \n",
      "with an average reward of: -4108.556577369052\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1703181\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4110.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 832       |\n",
      "|    time_elapsed         | 23351     |\n",
      "|    total_timesteps      | 1703936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0245    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.45e+09  |\n",
      "|    n_updates            | 8310      |\n",
      "|    policy_gradient_loss | -2.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.11e+09  |\n",
      "---------------------------------------\n",
      "Episode 683  finished with cumulative reward: -5201000.0 and \n",
      "with an average reward of: -2079.5681727309075\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1705682\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2080.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 833       |\n",
      "|    time_elapsed         | 23379     |\n",
      "|    total_timesteps      | 1705984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0192    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.22e+10  |\n",
      "|    n_updates            | 8320      |\n",
      "|    policy_gradient_loss | -1.96e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.67e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 834       |\n",
      "|    time_elapsed         | 23400     |\n",
      "|    total_timesteps      | 1708032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.015     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.23e+09  |\n",
      "|    n_updates            | 8330      |\n",
      "|    policy_gradient_loss | -1.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 684  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1708183\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 835       |\n",
      "|    time_elapsed         | 23429     |\n",
      "|    total_timesteps      | 1710080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0202    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.44e+09  |\n",
      "|    n_updates            | 8340      |\n",
      "|    policy_gradient_loss | -1.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "Episode 685  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1710684\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 836       |\n",
      "|    time_elapsed         | 23457     |\n",
      "|    total_timesteps      | 1712128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0199    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.24e+10  |\n",
      "|    n_updates            | 8350      |\n",
      "|    policy_gradient_loss | -1.31e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.06e+10  |\n",
      "---------------------------------------\n",
      "Episode 686  finished with cumulative reward: -4487000.0 and \n",
      "with an average reward of: -1794.0823670531788\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1713185\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1794.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 837       |\n",
      "|    time_elapsed         | 23485     |\n",
      "|    total_timesteps      | 1714176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0204    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.44e+09  |\n",
      "|    n_updates            | 8360      |\n",
      "|    policy_gradient_loss | -2e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 687  finished with cumulative reward: -5915000.0 and \n",
      "with an average reward of: -2365.0539784086363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1715686\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2366.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.13e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 72            |\n",
      "|    iterations           | 838           |\n",
      "|    time_elapsed         | 23513         |\n",
      "|    total_timesteps      | 1716224       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0237        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.94e+09      |\n",
      "|    n_updates            | 8370          |\n",
      "|    policy_gradient_loss | -6.87e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.41e+10      |\n",
      "-------------------------------------------\n",
      "Episode 688  finished with cumulative reward: -10301000.0 and \n",
      "with an average reward of: -4118.752499000399\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1718187\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4120.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 72        |\n",
      "|    iterations           | 839       |\n",
      "|    time_elapsed         | 23541     |\n",
      "|    total_timesteps      | 1718272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.65e+09  |\n",
      "|    n_updates            | 8380      |\n",
      "|    policy_gradient_loss | -2.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 840       |\n",
      "|    time_elapsed         | 23562     |\n",
      "|    total_timesteps      | 1720320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0184    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.43e+10  |\n",
      "|    n_updates            | 8390      |\n",
      "|    policy_gradient_loss | -1.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.21e+10  |\n",
      "---------------------------------------\n",
      "Episode 689  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1720688\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 841       |\n",
      "|    time_elapsed         | 23590     |\n",
      "|    total_timesteps      | 1722368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0222    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.51e+09  |\n",
      "|    n_updates            | 8400      |\n",
      "|    policy_gradient_loss | -1.98e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.62e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 690  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1723189\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 842       |\n",
      "|    time_elapsed         | 23617     |\n",
      "|    total_timesteps      | 1724416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0206    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.46e+09  |\n",
      "|    n_updates            | 8410      |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 691  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1725690\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 843       |\n",
      "|    time_elapsed         | 23645     |\n",
      "|    total_timesteps      | 1726464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0216    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.86e+09  |\n",
      "|    n_updates            | 8420      |\n",
      "|    policy_gradient_loss | -2.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.33e+10  |\n",
      "---------------------------------------\n",
      "Episode 692  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1728191\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 844       |\n",
      "|    time_elapsed         | 23673     |\n",
      "|    total_timesteps      | 1728512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0227    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.76e+09  |\n",
      "|    n_updates            | 8430      |\n",
      "|    policy_gradient_loss | -2.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.35e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 845       |\n",
      "|    time_elapsed         | 23694     |\n",
      "|    total_timesteps      | 1730560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0242    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.71e+09  |\n",
      "|    n_updates            | 8440      |\n",
      "|    policy_gradient_loss | -4.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.66e+09  |\n",
      "---------------------------------------\n",
      "Episode 693  finished with cumulative reward: -6603500.0 and \n",
      "with an average reward of: -2640.343862455018\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1730692\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2641.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 846       |\n",
      "|    time_elapsed         | 23723     |\n",
      "|    total_timesteps      | 1732608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0191    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5e+09     |\n",
      "|    n_updates            | 8450      |\n",
      "|    policy_gradient_loss | -1.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n",
      "Episode 694  finished with cumulative reward: -5966000.0 and \n",
      "with an average reward of: -2385.4458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1733193\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2386.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 847       |\n",
      "|    time_elapsed         | 23752     |\n",
      "|    total_timesteps      | 1734656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.24e+09  |\n",
      "|    n_updates            | 8460      |\n",
      "|    policy_gradient_loss | -1.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.47e+10  |\n",
      "---------------------------------------\n",
      "Episode 695  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1735694\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 848       |\n",
      "|    time_elapsed         | 23780     |\n",
      "|    total_timesteps      | 1736704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0207    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.69e+09  |\n",
      "|    n_updates            | 8470      |\n",
      "|    policy_gradient_loss | -2.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 696  finished with cumulative reward: -7955000.0 and \n",
      "with an average reward of: -3180.7277089164336\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1738195\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3182.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 849       |\n",
      "|    time_elapsed         | 23808     |\n",
      "|    total_timesteps      | 1738752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0299    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.92e+09  |\n",
      "|    n_updates            | 8480      |\n",
      "|    policy_gradient_loss | -2.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.58e+09  |\n",
      "---------------------------------------\n",
      "Episode 697  finished with cumulative reward: -6807500.0 and \n",
      "with an average reward of: -2721.911235505798\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1740696\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2723.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 850       |\n",
      "|    time_elapsed         | 23836     |\n",
      "|    total_timesteps      | 1740800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 8490      |\n",
      "|    policy_gradient_loss | -2.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.96e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 851       |\n",
      "|    time_elapsed         | 23857     |\n",
      "|    total_timesteps      | 1742848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0148    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.91e+09  |\n",
      "|    n_updates            | 8500      |\n",
      "|    policy_gradient_loss | -9.86e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "Episode 698  finished with cumulative reward: -7164500.0 and \n",
      "with an average reward of: -2864.654138344662\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1743197\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2865.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 852       |\n",
      "|    time_elapsed         | 23885     |\n",
      "|    total_timesteps      | 1744896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0191    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.48e+09  |\n",
      "|    n_updates            | 8510      |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1e+10     |\n",
      "---------------------------------------\n",
      "Episode 699  finished with cumulative reward: -2574500.0 and \n",
      "with an average reward of: -1029.3882447021192\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1745698\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1029.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -6e+06   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 73       |\n",
      "|    iterations           | 853      |\n",
      "|    time_elapsed         | 23913    |\n",
      "|    total_timesteps      | 1746944  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0195   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 3.6e+09  |\n",
      "|    n_updates            | 8520     |\n",
      "|    policy_gradient_loss | -3.1e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 8.89e+09 |\n",
      "--------------------------------------\n",
      "Episode 700  finished with cumulative reward: -9893000.0 and \n",
      "with an average reward of: -3955.6177528988405\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1748199\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3957.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 854       |\n",
      "|    time_elapsed         | 23942     |\n",
      "|    total_timesteps      | 1748992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0187    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.53e+09  |\n",
      "|    n_updates            | 8530      |\n",
      "|    policy_gradient_loss | -1.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+10  |\n",
      "---------------------------------------\n",
      "Episode 701  finished with cumulative reward: -3594500.0 and \n",
      "with an average reward of: -1437.2251099560176\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1750700\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1437.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -6e+06   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 73       |\n",
      "|    iterations           | 855      |\n",
      "|    time_elapsed         | 23969    |\n",
      "|    total_timesteps      | 1751040  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0179   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.6e+10  |\n",
      "|    n_updates            | 8540     |\n",
      "|    policy_gradient_loss | -1e-06   |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.75e+10 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 856       |\n",
      "|    time_elapsed         | 23990     |\n",
      "|    total_timesteps      | 1753088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0282    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.11e+09  |\n",
      "|    n_updates            | 8550      |\n",
      "|    policy_gradient_loss | -2.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.12e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 702  finished with cumulative reward: -2829500.0 and \n",
      "with an average reward of: -1131.3474610155938\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1753201\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1131.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 857       |\n",
      "|    time_elapsed         | 24018     |\n",
      "|    total_timesteps      | 1755136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0228    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.02e+09  |\n",
      "|    n_updates            | 8560      |\n",
      "|    policy_gradient_loss | -2.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.1e+09   |\n",
      "---------------------------------------\n",
      "Episode 703  finished with cumulative reward: -7419500.0 and \n",
      "with an average reward of: -2966.613354658137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1755702\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2967.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 858       |\n",
      "|    time_elapsed         | 24046     |\n",
      "|    total_timesteps      | 1757184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.026     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.6e+09   |\n",
      "|    n_updates            | 8570      |\n",
      "|    policy_gradient_loss | -1.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 704  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1758203\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 859       |\n",
      "|    time_elapsed         | 24074     |\n",
      "|    total_timesteps      | 1759232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0211    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.23e+09  |\n",
      "|    n_updates            | 8580      |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "Episode 705  finished with cumulative reward: -15248000.0 and \n",
      "with an average reward of: -6096.761295481807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1760704\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -6099.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 860       |\n",
      "|    time_elapsed         | 24102     |\n",
      "|    total_timesteps      | 1761280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+10  |\n",
      "|    n_updates            | 8590      |\n",
      "|    policy_gradient_loss | -4.68e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 706  finished with cumulative reward: -8159000.0 and \n",
      "with an average reward of: -3262.2950819672133\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1763205\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3263.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 861       |\n",
      "|    time_elapsed         | 24131     |\n",
      "|    total_timesteps      | 1763328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.019     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+10  |\n",
      "|    n_updates            | 8600      |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.39e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 862       |\n",
      "|    time_elapsed         | 24151     |\n",
      "|    total_timesteps      | 1765376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0232    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.68e+09  |\n",
      "|    n_updates            | 8610      |\n",
      "|    policy_gradient_loss | -4.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n",
      "Episode 707  finished with cumulative reward: -8414000.0 and \n",
      "with an average reward of: -3364.2542982806876\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1765706\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3365.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 863       |\n",
      "|    time_elapsed         | 24179     |\n",
      "|    total_timesteps      | 1767424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0218    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.65e+09  |\n",
      "|    n_updates            | 8620      |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.51e+10  |\n",
      "---------------------------------------\n",
      "Episode 708  finished with cumulative reward: -8949500.0 and \n",
      "with an average reward of: -3578.3686525389844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1768207\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3579.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 864       |\n",
      "|    time_elapsed         | 24208     |\n",
      "|    total_timesteps      | 1769472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0226    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.44e+09  |\n",
      "|    n_updates            | 8630      |\n",
      "|    policy_gradient_loss | -3.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 709  finished with cumulative reward: -6323000.0 and \n",
      "with an average reward of: -2528.1887245101957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1770708\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2529.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 865       |\n",
      "|    time_elapsed         | 24235     |\n",
      "|    total_timesteps      | 1771520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0279    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.74e+09  |\n",
      "|    n_updates            | 8640      |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 710  finished with cumulative reward: -1886000.0 and \n",
      "with an average reward of: -754.0983606557377\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1773209\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -754.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 866       |\n",
      "|    time_elapsed         | 24263     |\n",
      "|    total_timesteps      | 1773568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.54e+09  |\n",
      "|    n_updates            | 8650      |\n",
      "|    policy_gradient_loss | -5.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 867       |\n",
      "|    time_elapsed         | 24284     |\n",
      "|    total_timesteps      | 1775616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0262    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.34e+09  |\n",
      "|    n_updates            | 8660      |\n",
      "|    policy_gradient_loss | -2e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.01e+09  |\n",
      "---------------------------------------\n",
      "Episode 711  finished with cumulative reward: -3747500.0 and \n",
      "with an average reward of: -1498.4006397441024\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1775710\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1499.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 868       |\n",
      "|    time_elapsed         | 24312     |\n",
      "|    total_timesteps      | 1777664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0261    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.85e+09  |\n",
      "|    n_updates            | 8670      |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.73e+09  |\n",
      "---------------------------------------\n",
      "Episode 712  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1778211\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 869       |\n",
      "|    time_elapsed         | 24341     |\n",
      "|    total_timesteps      | 1779712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0208    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.75e+09  |\n",
      "|    n_updates            | 8680      |\n",
      "|    policy_gradient_loss | -1.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 713  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1780712\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 870       |\n",
      "|    time_elapsed         | 24370     |\n",
      "|    total_timesteps      | 1781760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0206    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.24e+09  |\n",
      "|    n_updates            | 8690      |\n",
      "|    policy_gradient_loss | -1.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 714  finished with cumulative reward: -9332000.0 and \n",
      "with an average reward of: -3731.307477009196\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1783213\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3732.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 871       |\n",
      "|    time_elapsed         | 24398     |\n",
      "|    total_timesteps      | 1783808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0187    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.73e+09  |\n",
      "|    n_updates            | 8700      |\n",
      "|    policy_gradient_loss | -1.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 715  finished with cumulative reward: -3824000.0 and \n",
      "with an average reward of: -1528.9884046381446\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1785714\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1529.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 872       |\n",
      "|    time_elapsed         | 24426     |\n",
      "|    total_timesteps      | 1785856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0192    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.95e+09  |\n",
      "|    n_updates            | 8710      |\n",
      "|    policy_gradient_loss | -3.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.54e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 873       |\n",
      "|    time_elapsed         | 24447     |\n",
      "|    total_timesteps      | 1787904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0243    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.4e+09   |\n",
      "|    n_updates            | 8720      |\n",
      "|    policy_gradient_loss | -3.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.23e+09  |\n",
      "---------------------------------------\n",
      "Episode 716  finished with cumulative reward: -6323000.0 and \n",
      "with an average reward of: -2528.1887245101957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1788215\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2529.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 874       |\n",
      "|    time_elapsed         | 24476     |\n",
      "|    total_timesteps      | 1789952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0191    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.52e+09  |\n",
      "|    n_updates            | 8730      |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 717  finished with cumulative reward: -10046000.0 and \n",
      "with an average reward of: -4016.793282686925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1790716\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4018.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 875       |\n",
      "|    time_elapsed         | 24504     |\n",
      "|    total_timesteps      | 1792000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0206    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.83e+09  |\n",
      "|    n_updates            | 8740      |\n",
      "|    policy_gradient_loss | -1.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.85e+09  |\n",
      "---------------------------------------\n",
      "Episode 718  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1793217\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 876       |\n",
      "|    time_elapsed         | 24532     |\n",
      "|    total_timesteps      | 1794048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0188    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 8750      |\n",
      "|    policy_gradient_loss | -3.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.88e+10  |\n",
      "---------------------------------------\n",
      "Episode 719  finished with cumulative reward: -7088000.0 and \n",
      "with an average reward of: -2834.0663734506197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1795718\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2835.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 877       |\n",
      "|    time_elapsed         | 24561     |\n",
      "|    total_timesteps      | 1796096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0192    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+10  |\n",
      "|    n_updates            | 8760      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 878       |\n",
      "|    time_elapsed         | 24582     |\n",
      "|    total_timesteps      | 1798144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.021     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.29e+09  |\n",
      "|    n_updates            | 8770      |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1e+10     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 720  finished with cumulative reward: -3416000.0 and \n",
      "with an average reward of: -1365.8536585365853\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1798219\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1366.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 879       |\n",
      "|    time_elapsed         | 24610     |\n",
      "|    total_timesteps      | 1800192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0242    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.14e+09  |\n",
      "|    n_updates            | 8780      |\n",
      "|    policy_gradient_loss | -2.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.46e+09  |\n",
      "---------------------------------------\n",
      "Episode 721  finished with cumulative reward: -4181000.0 and \n",
      "with an average reward of: -1671.7313074770093\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1800720\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1672.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 880       |\n",
      "|    time_elapsed         | 24638     |\n",
      "|    total_timesteps      | 1802240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0237    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.39e+09  |\n",
      "|    n_updates            | 8790      |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.99e+09  |\n",
      "---------------------------------------\n",
      "Episode 722  finished with cumulative reward: -8439500.0 and \n",
      "with an average reward of: -3374.450219912035\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1803221\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3375.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 881       |\n",
      "|    time_elapsed         | 24666     |\n",
      "|    total_timesteps      | 1804288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0251    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.17e+09  |\n",
      "|    n_updates            | 8800      |\n",
      "|    policy_gradient_loss | -2.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n",
      "Episode 723  finished with cumulative reward: -4028000.0 and \n",
      "with an average reward of: -1610.5557776889245\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1805722\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1611.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 882       |\n",
      "|    time_elapsed         | 24693     |\n",
      "|    total_timesteps      | 1806336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.03e+09  |\n",
      "|    n_updates            | 8810      |\n",
      "|    policy_gradient_loss | -2.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 724  finished with cumulative reward: -7190000.0 and \n",
      "with an average reward of: -2874.8500599760096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1808223\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2876.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 883       |\n",
      "|    time_elapsed         | 24721     |\n",
      "|    total_timesteps      | 1808384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0223    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.08e+09  |\n",
      "|    n_updates            | 8820      |\n",
      "|    policy_gradient_loss | -3.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.2e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 884       |\n",
      "|    time_elapsed         | 24742     |\n",
      "|    total_timesteps      | 1810432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0191    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.09e+09  |\n",
      "|    n_updates            | 8830      |\n",
      "|    policy_gradient_loss | -2e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 725  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1810724\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 885       |\n",
      "|    time_elapsed         | 24771     |\n",
      "|    total_timesteps      | 1812480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.71e+09  |\n",
      "|    n_updates            | 8840      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.19e+09  |\n",
      "---------------------------------------\n",
      "Episode 726  finished with cumulative reward: -3518000.0 and \n",
      "with an average reward of: -1406.637345061975\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1813225\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1407.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 886       |\n",
      "|    time_elapsed         | 24799     |\n",
      "|    total_timesteps      | 1814528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0184    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.59e+09  |\n",
      "|    n_updates            | 8850      |\n",
      "|    policy_gradient_loss | -1.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.66e+09  |\n",
      "---------------------------------------\n",
      "Episode 727  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1815726\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 887       |\n",
      "|    time_elapsed         | 24827     |\n",
      "|    total_timesteps      | 1816576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.66e+09  |\n",
      "|    n_updates            | 8860      |\n",
      "|    policy_gradient_loss | -1.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 728  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1818227\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 888       |\n",
      "|    time_elapsed         | 24855     |\n",
      "|    total_timesteps      | 1818624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0239    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.31e+09  |\n",
      "|    n_updates            | 8870      |\n",
      "|    policy_gradient_loss | -1.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.29e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 889       |\n",
      "|    time_elapsed         | 24877     |\n",
      "|    total_timesteps      | 1820672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0235    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.84e+09  |\n",
      "|    n_updates            | 8880      |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.51e+09  |\n",
      "---------------------------------------\n",
      "Episode 729  finished with cumulative reward: -6884000.0 and \n",
      "with an average reward of: -2752.49900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1820728\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2753.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 890       |\n",
      "|    time_elapsed         | 24905     |\n",
      "|    total_timesteps      | 1822720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0208    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+10  |\n",
      "|    n_updates            | 8890      |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.77e+10  |\n",
      "---------------------------------------\n",
      "Episode 730  finished with cumulative reward: -5634500.0 and \n",
      "with an average reward of: -2252.8988404638144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1823229\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2253.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 891       |\n",
      "|    time_elapsed         | 24933     |\n",
      "|    total_timesteps      | 1824768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.3e+09   |\n",
      "|    n_updates            | 8900      |\n",
      "|    policy_gradient_loss | -2.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.03e+09  |\n",
      "---------------------------------------\n",
      "Episode 731  finished with cumulative reward: -13539500.0 and \n",
      "with an average reward of: -5413.634546181527\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1825730\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5415.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 892       |\n",
      "|    time_elapsed         | 24961     |\n",
      "|    total_timesteps      | 1826816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0182    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.21e+09  |\n",
      "|    n_updates            | 8910      |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 732  finished with cumulative reward: -4563500.0 and \n",
      "with an average reward of: -1824.670131947221\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1828231\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1825.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 893       |\n",
      "|    time_elapsed         | 24990     |\n",
      "|    total_timesteps      | 1828864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.12e+10  |\n",
      "|    n_updates            | 8920      |\n",
      "|    policy_gradient_loss | -1.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 733  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1830732\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 894       |\n",
      "|    time_elapsed         | 25018     |\n",
      "|    total_timesteps      | 1830912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0335    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.61e+09  |\n",
      "|    n_updates            | 8930      |\n",
      "|    policy_gradient_loss | -2.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.55e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 895       |\n",
      "|    time_elapsed         | 25039     |\n",
      "|    total_timesteps      | 1832960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0229    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.75e+09  |\n",
      "|    n_updates            | 8940      |\n",
      "|    policy_gradient_loss | -9.45e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 734  finished with cumulative reward: -5430500.0 and \n",
      "with an average reward of: -2171.3314674130347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1833233\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2172.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 896       |\n",
      "|    time_elapsed         | 25067     |\n",
      "|    total_timesteps      | 1835008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0196    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.66e+09  |\n",
      "|    n_updates            | 8950      |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.72e+09  |\n",
      "---------------------------------------\n",
      "Episode 735  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1835734\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 897       |\n",
      "|    time_elapsed         | 25096     |\n",
      "|    total_timesteps      | 1837056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0198    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.05e+10  |\n",
      "|    n_updates            | 8960      |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.78e+10  |\n",
      "---------------------------------------\n",
      "Episode 736  finished with cumulative reward: -12009500.0 and \n",
      "with an average reward of: -4801.87924830068\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1838235\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4803.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 898       |\n",
      "|    time_elapsed         | 25123     |\n",
      "|    total_timesteps      | 1839104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1e+10     |\n",
      "|    n_updates            | 8970      |\n",
      "|    policy_gradient_loss | -3.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.94e+10  |\n",
      "---------------------------------------\n",
      "Episode 737  finished with cumulative reward: -7929500.0 and \n",
      "with an average reward of: -3170.531787285086\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1840736\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3171.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 899       |\n",
      "|    time_elapsed         | 25151     |\n",
      "|    total_timesteps      | 1841152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0221    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.19e+10  |\n",
      "|    n_updates            | 8980      |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.21e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 900       |\n",
      "|    time_elapsed         | 25172     |\n",
      "|    total_timesteps      | 1843200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.018     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.14e+10  |\n",
      "|    n_updates            | 8990      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.51e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 738  finished with cumulative reward: -3696500.0 and \n",
      "with an average reward of: -1478.0087964814074\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1843237\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1478.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6e+06       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 901          |\n",
      "|    time_elapsed         | 25201        |\n",
      "|    total_timesteps      | 1845248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.947651e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0305       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.15e+09     |\n",
      "|    n_updates            | 9000         |\n",
      "|    policy_gradient_loss | -8.99e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.83e+09     |\n",
      "------------------------------------------\n",
      "Episode 739  finished with cumulative reward: -5277500.0 and \n",
      "with an average reward of: -2110.15593762495\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1845738\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2111.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 902       |\n",
      "|    time_elapsed         | 25229     |\n",
      "|    total_timesteps      | 1847296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.29e+09  |\n",
      "|    n_updates            | 9010      |\n",
      "|    policy_gradient_loss | -3.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.46e+09  |\n",
      "---------------------------------------\n",
      "Episode 740  finished with cumulative reward: -3696500.0 and \n",
      "with an average reward of: -1478.0087964814074\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1848239\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1478.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 903       |\n",
      "|    time_elapsed         | 25258     |\n",
      "|    total_timesteps      | 1849344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0224    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.7e+09   |\n",
      "|    n_updates            | 9020      |\n",
      "|    policy_gradient_loss | -1.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.82e+10  |\n",
      "---------------------------------------\n",
      "Episode 741  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1850740\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 904       |\n",
      "|    time_elapsed         | 25287     |\n",
      "|    total_timesteps      | 1851392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0145    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.89e+09  |\n",
      "|    n_updates            | 9030      |\n",
      "|    policy_gradient_loss | -2.87e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 742  finished with cumulative reward: -4232000.0 and \n",
      "with an average reward of: -1692.1231507397042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1853241\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1692.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 905       |\n",
      "|    time_elapsed         | 25315     |\n",
      "|    total_timesteps      | 1853440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0216    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.48e+09  |\n",
      "|    n_updates            | 9040      |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.46e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 906       |\n",
      "|    time_elapsed         | 25335     |\n",
      "|    total_timesteps      | 1855488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.024     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.3e+09   |\n",
      "|    n_updates            | 9050      |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.16e+09  |\n",
      "---------------------------------------\n",
      "Episode 743  finished with cumulative reward: -8337500.0 and \n",
      "with an average reward of: -3333.6665333866454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1855742\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3335.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 907       |\n",
      "|    time_elapsed         | 25363     |\n",
      "|    total_timesteps      | 1857536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0238    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.68e+09  |\n",
      "|    n_updates            | 9060      |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.34e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 744  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1858243\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 908       |\n",
      "|    time_elapsed         | 25391     |\n",
      "|    total_timesteps      | 1859584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0197    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.82e+10  |\n",
      "|    n_updates            | 9070      |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.53e+10  |\n",
      "---------------------------------------\n",
      "Episode 745  finished with cumulative reward: -8516000.0 and \n",
      "with an average reward of: -3405.0379848060775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1860744\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3406.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 909       |\n",
      "|    time_elapsed         | 25419     |\n",
      "|    total_timesteps      | 1861632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0226    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.89e+09  |\n",
      "|    n_updates            | 9080      |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n",
      "Episode 746  finished with cumulative reward: -6425000.0 and \n",
      "with an average reward of: -2568.972411035586\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1863245\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2570.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 910       |\n",
      "|    time_elapsed         | 25447     |\n",
      "|    total_timesteps      | 1863680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.99e+09  |\n",
      "|    n_updates            | 9090      |\n",
      "|    policy_gradient_loss | -2.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.29e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 911       |\n",
      "|    time_elapsed         | 25468     |\n",
      "|    total_timesteps      | 1865728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.57e+09  |\n",
      "|    n_updates            | 9100      |\n",
      "|    policy_gradient_loss | -1.68e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n",
      "Episode 747  finished with cumulative reward: -1580000.0 and \n",
      "with an average reward of: -631.7473010795682\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1865746\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -632.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 912       |\n",
      "|    time_elapsed         | 25496     |\n",
      "|    total_timesteps      | 1867776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.79e+09  |\n",
      "|    n_updates            | 9110      |\n",
      "|    policy_gradient_loss | -3.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 748  finished with cumulative reward: -3620000.0 and \n",
      "with an average reward of: -1447.421031587365\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1868247\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1448.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.01e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 913          |\n",
      "|    time_elapsed         | 25525        |\n",
      "|    total_timesteps      | 1869824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0371       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.36e+09     |\n",
      "|    n_updates            | 9120         |\n",
      "|    policy_gradient_loss | -4.54e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.78e+09     |\n",
      "------------------------------------------\n",
      "Episode 749  finished with cumulative reward: -1274000.0 and \n",
      "with an average reward of: -509.39624150339864\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1870748\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -509.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 914       |\n",
      "|    time_elapsed         | 25552     |\n",
      "|    total_timesteps      | 1871872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.18e+09  |\n",
      "|    n_updates            | 9130      |\n",
      "|    policy_gradient_loss | -3.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.38e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 750  finished with cumulative reward: -2600000.0 and \n",
      "with an average reward of: -1039.5841663334666\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1873249\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1040.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 915       |\n",
      "|    time_elapsed         | 25580     |\n",
      "|    total_timesteps      | 1873920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.52e+09  |\n",
      "|    n_updates            | 9140      |\n",
      "|    policy_gradient_loss | -4.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.21e+09  |\n",
      "---------------------------------------\n",
      "Episode 751  finished with cumulative reward: -6017000.0 and \n",
      "with an average reward of: -2405.837664934026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1875750\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2406.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 916       |\n",
      "|    time_elapsed         | 25608     |\n",
      "|    total_timesteps      | 1875968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.42e+09  |\n",
      "|    n_updates            | 9150      |\n",
      "|    policy_gradient_loss | -1.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 917       |\n",
      "|    time_elapsed         | 25629     |\n",
      "|    total_timesteps      | 1878016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.033     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.13e+09  |\n",
      "|    n_updates            | 9160      |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.56e+09  |\n",
      "---------------------------------------\n",
      "Episode 752  finished with cumulative reward: -2268500.0 and \n",
      "with an average reward of: -907.0371851259496\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1878251\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -907.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 918       |\n",
      "|    time_elapsed         | 25658     |\n",
      "|    total_timesteps      | 1880064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0352    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.42e+09  |\n",
      "|    n_updates            | 9170      |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.44e+09  |\n",
      "---------------------------------------\n",
      "Episode 753  finished with cumulative reward: -4206500.0 and \n",
      "with an average reward of: -1681.9272291083566\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1880752\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1682.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 919       |\n",
      "|    time_elapsed         | 25686     |\n",
      "|    total_timesteps      | 1882112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0235    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.28e+09  |\n",
      "|    n_updates            | 9180      |\n",
      "|    policy_gradient_loss | -1.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.29e+10  |\n",
      "---------------------------------------\n",
      "Episode 754  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1883253\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 920       |\n",
      "|    time_elapsed         | 25714     |\n",
      "|    total_timesteps      | 1884160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0197    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.55e+09  |\n",
      "|    n_updates            | 9190      |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 755  finished with cumulative reward: -5481500.0 and \n",
      "with an average reward of: -2191.72331067573\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1885754\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2192.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 921       |\n",
      "|    time_elapsed         | 25743     |\n",
      "|    total_timesteps      | 1886208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+09  |\n",
      "|    n_updates            | 9200      |\n",
      "|    policy_gradient_loss | -2.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 756  finished with cumulative reward: -4461500.0 and \n",
      "with an average reward of: -1783.8864454218312\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1888255\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1784.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 922       |\n",
      "|    time_elapsed         | 25771     |\n",
      "|    total_timesteps      | 1888256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0232    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.46e+09  |\n",
      "|    n_updates            | 9210      |\n",
      "|    policy_gradient_loss | -2.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.46e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 923       |\n",
      "|    time_elapsed         | 25791     |\n",
      "|    total_timesteps      | 1890304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.55e+09  |\n",
      "|    n_updates            | 9220      |\n",
      "|    policy_gradient_loss | -2.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.18e+09  |\n",
      "---------------------------------------\n",
      "Episode 757  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1890756\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 924       |\n",
      "|    time_elapsed         | 25819     |\n",
      "|    total_timesteps      | 1892352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0216    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.6e+09   |\n",
      "|    n_updates            | 9230      |\n",
      "|    policy_gradient_loss | -1.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n",
      "Episode 758  finished with cumulative reward: -8847500.0 and \n",
      "with an average reward of: -3537.5849660135946\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1893257\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3539.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 925       |\n",
      "|    time_elapsed         | 25847     |\n",
      "|    total_timesteps      | 1894400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0219    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.62e+09  |\n",
      "|    n_updates            | 9240      |\n",
      "|    policy_gradient_loss | -1.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.98e+10  |\n",
      "---------------------------------------\n",
      "Episode 759  finished with cumulative reward: -2141000.0 and \n",
      "with an average reward of: -856.0575769692123\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1895758\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -856.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 926       |\n",
      "|    time_elapsed         | 25875     |\n",
      "|    total_timesteps      | 1896448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0306    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.05e+09  |\n",
      "|    n_updates            | 9250      |\n",
      "|    policy_gradient_loss | -2.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.75e+09  |\n",
      "---------------------------------------\n",
      "Episode 760  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1898259\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 927       |\n",
      "|    time_elapsed         | 25905     |\n",
      "|    total_timesteps      | 1898496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.15e+08  |\n",
      "|    n_updates            | 9260      |\n",
      "|    policy_gradient_loss | -2.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.39e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 928       |\n",
      "|    time_elapsed         | 25926     |\n",
      "|    total_timesteps      | 1900544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.32e+09  |\n",
      "|    n_updates            | 9270      |\n",
      "|    policy_gradient_loss | -2.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 761  finished with cumulative reward: -4742000.0 and \n",
      "with an average reward of: -1896.0415833666534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1900760\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1896.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 929       |\n",
      "|    time_elapsed         | 25954     |\n",
      "|    total_timesteps      | 1902592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0221    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.79e+09  |\n",
      "|    n_updates            | 9280      |\n",
      "|    policy_gradient_loss | -2.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.58e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 762  finished with cumulative reward: -13590500.0 and \n",
      "with an average reward of: -5434.026389444222\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1903261\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5436.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 930       |\n",
      "|    time_elapsed         | 25983     |\n",
      "|    total_timesteps      | 1904640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0221    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.16e+09  |\n",
      "|    n_updates            | 9290      |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 763  finished with cumulative reward: -5252000.0 and \n",
      "with an average reward of: -2099.9600159936026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1905762\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2100.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 931       |\n",
      "|    time_elapsed         | 26011     |\n",
      "|    total_timesteps      | 1906688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0152    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.55e+10  |\n",
      "|    n_updates            | 9300      |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.29e+10  |\n",
      "---------------------------------------\n",
      "Episode 764  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1908263\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 932       |\n",
      "|    time_elapsed         | 26040     |\n",
      "|    total_timesteps      | 1908736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0253    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.39e+09  |\n",
      "|    n_updates            | 9310      |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 765  finished with cumulative reward: -5634500.0 and \n",
      "with an average reward of: -2252.8988404638144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1910764\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2253.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 933       |\n",
      "|    time_elapsed         | 26067     |\n",
      "|    total_timesteps      | 1910784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0259    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.27e+09  |\n",
      "|    n_updates            | 9320      |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.01e+09  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.97e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 934          |\n",
      "|    time_elapsed         | 26088        |\n",
      "|    total_timesteps      | 1912832      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0276       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.14e+09     |\n",
      "|    n_updates            | 9330         |\n",
      "|    policy_gradient_loss | -4.48e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.38e+10     |\n",
      "------------------------------------------\n",
      "Episode 766  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1913265\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 935       |\n",
      "|    time_elapsed         | 26116     |\n",
      "|    total_timesteps      | 1914880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.99e+09  |\n",
      "|    n_updates            | 9340      |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.61e+09  |\n",
      "---------------------------------------\n",
      "Episode 767  finished with cumulative reward: -5328500.0 and \n",
      "with an average reward of: -2130.547780887645\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1915766\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2131.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 936       |\n",
      "|    time_elapsed         | 26144     |\n",
      "|    total_timesteps      | 1916928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.74e+09  |\n",
      "|    n_updates            | 9350      |\n",
      "|    policy_gradient_loss | -1.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.88e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 768  finished with cumulative reward: -11499500.0 and \n",
      "with an average reward of: -4597.96081567373\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1918267\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4599.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 937       |\n",
      "|    time_elapsed         | 26173     |\n",
      "|    total_timesteps      | 1918976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0208    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.64e+09  |\n",
      "|    n_updates            | 9360      |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 769  finished with cumulative reward: -1886000.0 and \n",
      "with an average reward of: -754.0983606557377\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1920768\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -754.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 938       |\n",
      "|    time_elapsed         | 26200     |\n",
      "|    total_timesteps      | 1921024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.97e+09  |\n",
      "|    n_updates            | 9370      |\n",
      "|    policy_gradient_loss | -1.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.45e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 939       |\n",
      "|    time_elapsed         | 26221     |\n",
      "|    total_timesteps      | 1923072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0242    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.38e+09  |\n",
      "|    n_updates            | 9380      |\n",
      "|    policy_gradient_loss | -1.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.84e+09  |\n",
      "---------------------------------------\n",
      "Episode 770  finished with cumulative reward: -3620000.0 and \n",
      "with an average reward of: -1447.421031587365\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1923269\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1448.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 940       |\n",
      "|    time_elapsed         | 26250     |\n",
      "|    total_timesteps      | 1925120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.12e+09  |\n",
      "|    n_updates            | 9390      |\n",
      "|    policy_gradient_loss | -1.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.63e+09  |\n",
      "---------------------------------------\n",
      "Episode 771  finished with cumulative reward: -2727500.0 and \n",
      "with an average reward of: -1090.563774490204\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1925770\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1091.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.81e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 941          |\n",
      "|    time_elapsed         | 26278        |\n",
      "|    total_timesteps      | 1927168      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0252       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.17e+08     |\n",
      "|    n_updates            | 9400         |\n",
      "|    policy_gradient_loss | -3.43e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.44e+09     |\n",
      "------------------------------------------\n",
      "Episode 772  finished with cumulative reward: -11168000.0 and \n",
      "with an average reward of: -4465.413834466213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1928271\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4467.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 942       |\n",
      "|    time_elapsed         | 26305     |\n",
      "|    total_timesteps      | 1929216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0206    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 9410      |\n",
      "|    policy_gradient_loss | -1.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.77e+10  |\n",
      "---------------------------------------\n",
      "Episode 773  finished with cumulative reward: -2676500.0 and \n",
      "with an average reward of: -1070.171931227509\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1930772\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1070.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 943       |\n",
      "|    time_elapsed         | 26333     |\n",
      "|    total_timesteps      | 1931264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.44e+09  |\n",
      "|    n_updates            | 9420      |\n",
      "|    policy_gradient_loss | -3.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.47e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 774  finished with cumulative reward: -5022500.0 and \n",
      "with an average reward of: -2008.1967213114754\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1933273\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2009.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 944       |\n",
      "|    time_elapsed         | 26362     |\n",
      "|    total_timesteps      | 1933312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0224    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.3e+09   |\n",
      "|    n_updates            | 9430      |\n",
      "|    policy_gradient_loss | -8.71e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.78e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 945           |\n",
      "|    time_elapsed         | 26383         |\n",
      "|    total_timesteps      | 1935360       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1379598e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0324        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.14e+09      |\n",
      "|    n_updates            | 9440          |\n",
      "|    policy_gradient_loss | -1.77e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.73e+09      |\n",
      "-------------------------------------------\n",
      "Episode 775  finished with cumulative reward: -5991500.0 and \n",
      "with an average reward of: -2395.641743302679\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1935774\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2396.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 946       |\n",
      "|    time_elapsed         | 26412     |\n",
      "|    total_timesteps      | 1937408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.56e+09  |\n",
      "|    n_updates            | 9450      |\n",
      "|    policy_gradient_loss | -4.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.45e+10  |\n",
      "---------------------------------------\n",
      "Episode 776  finished with cumulative reward: -8847500.0 and \n",
      "with an average reward of: -3537.5849660135946\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1938275\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3539.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 947       |\n",
      "|    time_elapsed         | 26441     |\n",
      "|    total_timesteps      | 1939456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0244    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.82e+09  |\n",
      "|    n_updates            | 9460      |\n",
      "|    policy_gradient_loss | -8.49e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.97e+10  |\n",
      "---------------------------------------\n",
      "Episode 777  finished with cumulative reward: -5022500.0 and \n",
      "with an average reward of: -2008.1967213114754\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1940776\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2009.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 948       |\n",
      "|    time_elapsed         | 26468     |\n",
      "|    total_timesteps      | 1941504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.74e+09  |\n",
      "|    n_updates            | 9470      |\n",
      "|    policy_gradient_loss | -1e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.35e+09  |\n",
      "---------------------------------------\n",
      "Episode 778  finished with cumulative reward: -2243000.0 and \n",
      "with an average reward of: -896.8412634946021\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1943277\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -897.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.69e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 949           |\n",
      "|    time_elapsed         | 26496         |\n",
      "|    total_timesteps      | 1943552       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0273        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.55e+09      |\n",
      "|    n_updates            | 9480          |\n",
      "|    policy_gradient_loss | -6.38e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.54e+09      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 950          |\n",
      "|    time_elapsed         | 26517        |\n",
      "|    total_timesteps      | 1945600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0241       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.3e+09      |\n",
      "|    n_updates            | 9490         |\n",
      "|    policy_gradient_loss | -2.66e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.23e+09     |\n",
      "------------------------------------------\n",
      "Episode 779  finished with cumulative reward: -4742000.0 and \n",
      "with an average reward of: -1896.0415833666534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1945778\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1896.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 951           |\n",
      "|    time_elapsed         | 26545         |\n",
      "|    total_timesteps      | 1947648       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7625584e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0303        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.73e+09      |\n",
      "|    n_updates            | 9500          |\n",
      "|    policy_gradient_loss | -1.66e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.08e+10      |\n",
      "-------------------------------------------\n",
      "Episode 780  finished with cumulative reward: -2141000.0 and \n",
      "with an average reward of: -856.0575769692123\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1948279\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -856.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 952       |\n",
      "|    time_elapsed         | 26573     |\n",
      "|    total_timesteps      | 1949696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.47e+09  |\n",
      "|    n_updates            | 9510      |\n",
      "|    policy_gradient_loss | -2.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.64e+09  |\n",
      "---------------------------------------\n",
      "Episode 781  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1950780\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 953       |\n",
      "|    time_elapsed         | 26601     |\n",
      "|    total_timesteps      | 1951744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0453    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.83e+08  |\n",
      "|    n_updates            | 9520      |\n",
      "|    policy_gradient_loss | -1.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.92e+08  |\n",
      "---------------------------------------\n",
      "Episode 782  finished with cumulative reward: -3875000.0 and \n",
      "with an average reward of: -1549.3802479008398\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1953281\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1550.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 954          |\n",
      "|    time_elapsed         | 26630        |\n",
      "|    total_timesteps      | 1953792      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0243       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.89e+09     |\n",
      "|    n_updates            | 9530         |\n",
      "|    policy_gradient_loss | -3.11e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.99e+09     |\n",
      "------------------------------------------\n",
      "Episode 783  finished with cumulative reward: -7419500.0 and \n",
      "with an average reward of: -2966.613354658137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1955782\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2967.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 955       |\n",
      "|    time_elapsed         | 26659     |\n",
      "|    total_timesteps      | 1955840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.3e+09   |\n",
      "|    n_updates            | 9540      |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.79e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 956       |\n",
      "|    time_elapsed         | 26680     |\n",
      "|    total_timesteps      | 1957888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.54e+09  |\n",
      "|    n_updates            | 9550      |\n",
      "|    policy_gradient_loss | -2.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.89e+10  |\n",
      "---------------------------------------\n",
      "Episode 784  finished with cumulative reward: -6425000.0 and \n",
      "with an average reward of: -2568.972411035586\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1958283\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2570.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 957       |\n",
      "|    time_elapsed         | 26708     |\n",
      "|    total_timesteps      | 1959936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.93e+09  |\n",
      "|    n_updates            | 9560      |\n",
      "|    policy_gradient_loss | -3.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.79e+09  |\n",
      "---------------------------------------\n",
      "Episode 785  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1960784\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 958       |\n",
      "|    time_elapsed         | 26735     |\n",
      "|    total_timesteps      | 1961984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0176    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.98e+09  |\n",
      "|    n_updates            | 9570      |\n",
      "|    policy_gradient_loss | -1.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n",
      "Episode 786  finished with cumulative reward: -8261000.0 and \n",
      "with an average reward of: -3303.078768492603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1963285\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3304.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 959       |\n",
      "|    time_elapsed         | 26764     |\n",
      "|    total_timesteps      | 1964032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0194    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.23e+10  |\n",
      "|    n_updates            | 9580      |\n",
      "|    policy_gradient_loss | -2.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.94e+10  |\n",
      "---------------------------------------\n",
      "Episode 787  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1965786\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.7e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 960           |\n",
      "|    time_elapsed         | 26792         |\n",
      "|    total_timesteps      | 1966080       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0268        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.47e+09      |\n",
      "|    n_updates            | 9590          |\n",
      "|    policy_gradient_loss | -3.75e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.45e+09      |\n",
      "-------------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.7e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 73       |\n",
      "|    iterations           | 961      |\n",
      "|    time_elapsed         | 26813    |\n",
      "|    total_timesteps      | 1968128  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0243   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 6.61e+09 |\n",
      "|    n_updates            | 9600     |\n",
      "|    policy_gradient_loss | -1.8e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 8.87e+09 |\n",
      "--------------------------------------\n",
      "Episode 788  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1968287\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 962       |\n",
      "|    time_elapsed         | 26841     |\n",
      "|    total_timesteps      | 1970176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.94e+09  |\n",
      "|    n_updates            | 9610      |\n",
      "|    policy_gradient_loss | -2.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.57e+09  |\n",
      "---------------------------------------\n",
      "Episode 789  finished with cumulative reward: -6629000.0 and \n",
      "with an average reward of: -2650.5397840863657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1970788\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2651.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 963       |\n",
      "|    time_elapsed         | 26869     |\n",
      "|    total_timesteps      | 1972224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0204    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.88e+09  |\n",
      "|    n_updates            | 9620      |\n",
      "|    policy_gradient_loss | -1.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 790  finished with cumulative reward: -4130000.0 and \n",
      "with an average reward of: -1651.3394642143144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1973289\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1652.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 964       |\n",
      "|    time_elapsed         | 26897     |\n",
      "|    total_timesteps      | 1974272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0218    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.3e+09   |\n",
      "|    n_updates            | 9630      |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 791  finished with cumulative reward: -7623500.0 and \n",
      "with an average reward of: -3048.1807277089165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1975790\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3049.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 965          |\n",
      "|    time_elapsed         | 26925        |\n",
      "|    total_timesteps      | 1976320      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.032657e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.027        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.29e+09     |\n",
      "|    n_updates            | 9640         |\n",
      "|    policy_gradient_loss | -1.4e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.79e+09     |\n",
      "------------------------------------------\n",
      "Episode 792  finished with cumulative reward: -2523500.0 and \n",
      "with an average reward of: -1008.9964014394242\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1978291\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1009.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 966       |\n",
      "|    time_elapsed         | 26954     |\n",
      "|    total_timesteps      | 1978368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0249    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.74e+09  |\n",
      "|    n_updates            | 9650      |\n",
      "|    policy_gradient_loss | -1.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 967       |\n",
      "|    time_elapsed         | 26975     |\n",
      "|    total_timesteps      | 1980416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0243    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.89e+09  |\n",
      "|    n_updates            | 9660      |\n",
      "|    policy_gradient_loss | -1.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.74e+09  |\n",
      "---------------------------------------\n",
      "Episode 793  finished with cumulative reward: -5201000.0 and \n",
      "with an average reward of: -2079.5681727309075\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1980792\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2080.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 968       |\n",
      "|    time_elapsed         | 27003     |\n",
      "|    total_timesteps      | 1982464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0253    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.86e+09  |\n",
      "|    n_updates            | 9670      |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n",
      "Episode 794  finished with cumulative reward: -2294000.0 and \n",
      "with an average reward of: -917.233106757297\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1983293\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -917.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 969          |\n",
      "|    time_elapsed         | 27031        |\n",
      "|    total_timesteps      | 1984512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0264       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.4e+09      |\n",
      "|    n_updates            | 9680         |\n",
      "|    policy_gradient_loss | -2.96e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.49e+09     |\n",
      "------------------------------------------\n",
      "Episode 795  finished with cumulative reward: -9026000.0 and \n",
      "with an average reward of: -3608.9564174330267\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1985794\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3610.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 970       |\n",
      "|    time_elapsed         | 27060     |\n",
      "|    total_timesteps      | 1986560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.024     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.04e+09  |\n",
      "|    n_updates            | 9690      |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 796  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1988295\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 971       |\n",
      "|    time_elapsed         | 27088     |\n",
      "|    total_timesteps      | 1988608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.02      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.85e+09  |\n",
      "|    n_updates            | 9700      |\n",
      "|    policy_gradient_loss | -3.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 972       |\n",
      "|    time_elapsed         | 27108     |\n",
      "|    total_timesteps      | 1990656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.32e+09  |\n",
      "|    n_updates            | 9710      |\n",
      "|    policy_gradient_loss | -2.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 797  finished with cumulative reward: -6221000.0 and \n",
      "with an average reward of: -2487.405037984806\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1990796\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2488.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 973       |\n",
      "|    time_elapsed         | 27137     |\n",
      "|    total_timesteps      | 1992704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0211    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.33e+09  |\n",
      "|    n_updates            | 9720      |\n",
      "|    policy_gradient_loss | -3.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.33e+10  |\n",
      "---------------------------------------\n",
      "Episode 798  finished with cumulative reward: -5099000.0 and \n",
      "with an average reward of: -2038.7844862055179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1993297\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2039.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 974       |\n",
      "|    time_elapsed         | 27165     |\n",
      "|    total_timesteps      | 1994752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.18e+09  |\n",
      "|    n_updates            | 9730      |\n",
      "|    policy_gradient_loss | -3.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 799  finished with cumulative reward: -9102500.0 and \n",
      "with an average reward of: -3639.5441823270694\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1995798\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3641.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 975       |\n",
      "|    time_elapsed         | 27193     |\n",
      "|    total_timesteps      | 1996800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0283    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.3e+09   |\n",
      "|    n_updates            | 9740      |\n",
      "|    policy_gradient_loss | -2.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.4e+10   |\n",
      "---------------------------------------\n",
      "Episode 800  finished with cumulative reward: -3798500.0 and \n",
      "with an average reward of: -1518.7924830067973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 1998299\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1519.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 976       |\n",
      "|    time_elapsed         | 27221     |\n",
      "|    total_timesteps      | 1998848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0227    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.5e+09   |\n",
      "|    n_updates            | 9750      |\n",
      "|    policy_gradient_loss | -1.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 801  finished with cumulative reward: -9536000.0 and \n",
      "with an average reward of: -3812.874850059976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2000800\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3814.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 977       |\n",
      "|    time_elapsed         | 27249     |\n",
      "|    total_timesteps      | 2000896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0224    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.58e+09  |\n",
      "|    n_updates            | 9760      |\n",
      "|    policy_gradient_loss | -1.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.45e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 978       |\n",
      "|    time_elapsed         | 27271     |\n",
      "|    total_timesteps      | 2002944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0228    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 9770      |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 802  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2003301\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 979       |\n",
      "|    time_elapsed         | 27299     |\n",
      "|    total_timesteps      | 2004992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.023     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.02e+09  |\n",
      "|    n_updates            | 9780      |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.36e+09  |\n",
      "---------------------------------------\n",
      "Episode 803  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2005802\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.68e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 980           |\n",
      "|    time_elapsed         | 27327         |\n",
      "|    total_timesteps      | 2007040       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0266        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.37e+09      |\n",
      "|    n_updates            | 9790          |\n",
      "|    policy_gradient_loss | -4.32e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.06e+10      |\n",
      "-------------------------------------------\n",
      "Episode 804  finished with cumulative reward: -3212000.0 and \n",
      "with an average reward of: -1284.2862854858056\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2008303\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1284.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 981       |\n",
      "|    time_elapsed         | 27355     |\n",
      "|    total_timesteps      | 2009088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0232    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.07e+09  |\n",
      "|    n_updates            | 9800      |\n",
      "|    policy_gradient_loss | -1.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 805  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2010804\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 982       |\n",
      "|    time_elapsed         | 27383     |\n",
      "|    total_timesteps      | 2011136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.64e+09  |\n",
      "|    n_updates            | 9810      |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.98e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 983       |\n",
      "|    time_elapsed         | 27404     |\n",
      "|    total_timesteps      | 2013184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.29e+09  |\n",
      "|    n_updates            | 9820      |\n",
      "|    policy_gradient_loss | -1.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.58e+09  |\n",
      "---------------------------------------\n",
      "Episode 806  finished with cumulative reward: -3263000.0 and \n",
      "with an average reward of: -1304.6781287485005\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2013305\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1305.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 984       |\n",
      "|    time_elapsed         | 27432     |\n",
      "|    total_timesteps      | 2015232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0278    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.85e+09  |\n",
      "|    n_updates            | 9830      |\n",
      "|    policy_gradient_loss | -1.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.27e+09  |\n",
      "---------------------------------------\n",
      "Episode 807  finished with cumulative reward: -9995000.0 and \n",
      "with an average reward of: -3996.4014394242304\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2015806\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3998.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 985       |\n",
      "|    time_elapsed         | 27461     |\n",
      "|    total_timesteps      | 2017280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0202    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.37e+10  |\n",
      "|    n_updates            | 9840      |\n",
      "|    policy_gradient_loss | -3.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 808  finished with cumulative reward: -3135500.0 and \n",
      "with an average reward of: -1253.6985205917633\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2018307\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1254.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 986       |\n",
      "|    time_elapsed         | 27490     |\n",
      "|    total_timesteps      | 2019328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.97e+09  |\n",
      "|    n_updates            | 9850      |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.88e+09  |\n",
      "---------------------------------------\n",
      "Episode 809  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2020808\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 987       |\n",
      "|    time_elapsed         | 27519     |\n",
      "|    total_timesteps      | 2021376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.03e+09  |\n",
      "|    n_updates            | 9860      |\n",
      "|    policy_gradient_loss | -1.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.23e+09  |\n",
      "---------------------------------------\n",
      "Episode 810  finished with cumulative reward: -2982500.0 and \n",
      "with an average reward of: -1192.5229908036786\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2023309\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1193.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 988       |\n",
      "|    time_elapsed         | 27547     |\n",
      "|    total_timesteps      | 2023424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0286    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.62e+09  |\n",
      "|    n_updates            | 9870      |\n",
      "|    policy_gradient_loss | -2.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.52e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 989       |\n",
      "|    time_elapsed         | 27568     |\n",
      "|    total_timesteps      | 2025472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0259    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.78e+08  |\n",
      "|    n_updates            | 9880      |\n",
      "|    policy_gradient_loss | -1.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.74e+09  |\n",
      "---------------------------------------\n",
      "Episode 811  finished with cumulative reward: -6374000.0 and \n",
      "with an average reward of: -2548.580567772891\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2025810\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2549.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 990       |\n",
      "|    time_elapsed         | 27596     |\n",
      "|    total_timesteps      | 2027520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.07e+09  |\n",
      "|    n_updates            | 9890      |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 812  finished with cumulative reward: -12366500.0 and \n",
      "with an average reward of: -4944.622151139544\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2028311\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4946.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 991       |\n",
      "|    time_elapsed         | 27623     |\n",
      "|    total_timesteps      | 2029568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0192    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.74e+09  |\n",
      "|    n_updates            | 9900      |\n",
      "|    policy_gradient_loss | -1.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.53e+10  |\n",
      "---------------------------------------\n",
      "Episode 813  finished with cumulative reward: -10505000.0 and \n",
      "with an average reward of: -4200.319872051179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2030812\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4202.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 992       |\n",
      "|    time_elapsed         | 27651     |\n",
      "|    total_timesteps      | 2031616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0172    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.91e+10  |\n",
      "|    n_updates            | 9910      |\n",
      "|    policy_gradient_loss | -3.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.63e+10  |\n",
      "---------------------------------------\n",
      "Episode 814  finished with cumulative reward: -4334000.0 and \n",
      "with an average reward of: -1732.906837265094\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2033313\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1733.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 993       |\n",
      "|    time_elapsed         | 27679     |\n",
      "|    total_timesteps      | 2033664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0168    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.64e+09  |\n",
      "|    n_updates            | 9920      |\n",
      "|    policy_gradient_loss | -1.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 994       |\n",
      "|    time_elapsed         | 27700     |\n",
      "|    total_timesteps      | 2035712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.08e+09  |\n",
      "|    n_updates            | 9930      |\n",
      "|    policy_gradient_loss | -1.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 815  finished with cumulative reward: -9357500.0 and \n",
      "with an average reward of: -3741.5033986405438\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2035814\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3743.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 995       |\n",
      "|    time_elapsed         | 27728     |\n",
      "|    total_timesteps      | 2037760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0189    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.24e+10  |\n",
      "|    n_updates            | 9940      |\n",
      "|    policy_gradient_loss | -3.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 816  finished with cumulative reward: -305000.0 and \n",
      "with an average reward of: -121.95121951219512\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2038315\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -122.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 996       |\n",
      "|    time_elapsed         | 27757     |\n",
      "|    total_timesteps      | 2039808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+09  |\n",
      "|    n_updates            | 9950      |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.91e+09  |\n",
      "---------------------------------------\n",
      "Episode 817  finished with cumulative reward: -6272000.0 and \n",
      "with an average reward of: -2507.796881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2040816\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2508.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 997       |\n",
      "|    time_elapsed         | 27785     |\n",
      "|    total_timesteps      | 2041856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0209    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.65e+09  |\n",
      "|    n_updates            | 9960      |\n",
      "|    policy_gradient_loss | -1.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.52e+09  |\n",
      "---------------------------------------\n",
      "Episode 818  finished with cumulative reward: -5787500.0 and \n",
      "with an average reward of: -2314.0743702518994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2043317\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2315.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.5e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 998       |\n",
      "|    time_elapsed         | 27813     |\n",
      "|    total_timesteps      | 2043904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0257    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.99e+09  |\n",
      "|    n_updates            | 9970      |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 819  finished with cumulative reward: -8388500.0 and \n",
      "with an average reward of: -3354.05837664934\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2045818\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3355.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 999       |\n",
      "|    time_elapsed         | 27841     |\n",
      "|    total_timesteps      | 2045952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.026     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.3e+09   |\n",
      "|    n_updates            | 9980      |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.97e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1000      |\n",
      "|    time_elapsed         | 27862     |\n",
      "|    total_timesteps      | 2048000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0249    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.03e+10  |\n",
      "|    n_updates            | 9990      |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.87e+10  |\n",
      "---------------------------------------\n",
      "Episode 820  finished with cumulative reward: -8720000.0 and \n",
      "with an average reward of: -3486.605357856857\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2048319\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3488.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1001      |\n",
      "|    time_elapsed         | 27889     |\n",
      "|    total_timesteps      | 2050048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.019     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8e+09     |\n",
      "|    n_updates            | 10000     |\n",
      "|    policy_gradient_loss | -2.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 821  finished with cumulative reward: -11805500.0 and \n",
      "with an average reward of: -4720.3118752499\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2050820\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4722.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.64e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1002         |\n",
      "|    time_elapsed         | 27917        |\n",
      "|    total_timesteps      | 2052096      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0259       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.86e+09     |\n",
      "|    n_updates            | 10010        |\n",
      "|    policy_gradient_loss | -4.46e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.28e+10     |\n",
      "------------------------------------------\n",
      "Episode 822  finished with cumulative reward: -6348500.0 and \n",
      "with an average reward of: -2538.3846461415433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2053321\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2539.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1003      |\n",
      "|    time_elapsed         | 27945     |\n",
      "|    total_timesteps      | 2054144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0238    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+10  |\n",
      "|    n_updates            | 10020     |\n",
      "|    policy_gradient_loss | -1.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.93e+10  |\n",
      "---------------------------------------\n",
      "Episode 823  finished with cumulative reward: -4512500.0 and \n",
      "with an average reward of: -1804.2782886845262\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2055822\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1805.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1004      |\n",
      "|    time_elapsed         | 27973     |\n",
      "|    total_timesteps      | 2056192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.61e+09  |\n",
      "|    n_updates            | 10030     |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.35e+09  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1005          |\n",
      "|    time_elapsed         | 27994         |\n",
      "|    total_timesteps      | 2058240       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0294        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.86e+09      |\n",
      "|    n_updates            | 10040         |\n",
      "|    policy_gradient_loss | -7.97e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.53e+09      |\n",
      "-------------------------------------------\n",
      "Episode 824  finished with cumulative reward: -10352000.0 and \n",
      "with an average reward of: -4139.144342263095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2058323\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4140.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1006      |\n",
      "|    time_elapsed         | 28021     |\n",
      "|    total_timesteps      | 2060288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0171    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.64e+09  |\n",
      "|    n_updates            | 10050     |\n",
      "|    policy_gradient_loss | -2.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 825  finished with cumulative reward: -5838500.0 and \n",
      "with an average reward of: -2334.466213514594\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2060824\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2335.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1007      |\n",
      "|    time_elapsed         | 28049     |\n",
      "|    total_timesteps      | 2062336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.54e+09  |\n",
      "|    n_updates            | 10060     |\n",
      "|    policy_gradient_loss | -3.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 826  finished with cumulative reward: -10352000.0 and \n",
      "with an average reward of: -4139.144342263095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2063325\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4140.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1008      |\n",
      "|    time_elapsed         | 28077     |\n",
      "|    total_timesteps      | 2064384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0197    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.03e+10  |\n",
      "|    n_updates            | 10070     |\n",
      "|    policy_gradient_loss | -3.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 827  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2065826\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.77e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1009      |\n",
      "|    time_elapsed         | 28105     |\n",
      "|    total_timesteps      | 2066432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.05e+09  |\n",
      "|    n_updates            | 10080     |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.82e+09  |\n",
      "---------------------------------------\n",
      "Episode 828  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2068327\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1010      |\n",
      "|    time_elapsed         | 28132     |\n",
      "|    total_timesteps      | 2068480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0233    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.29e+09  |\n",
      "|    n_updates            | 10090     |\n",
      "|    policy_gradient_loss | -1.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1011      |\n",
      "|    time_elapsed         | 28153     |\n",
      "|    total_timesteps      | 2070528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0243    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.35e+09  |\n",
      "|    n_updates            | 10100     |\n",
      "|    policy_gradient_loss | -8.84e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.12e+09  |\n",
      "---------------------------------------\n",
      "Episode 829  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2070828\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1012      |\n",
      "|    time_elapsed         | 28180     |\n",
      "|    total_timesteps      | 2072576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0296    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.5e+09   |\n",
      "|    n_updates            | 10110     |\n",
      "|    policy_gradient_loss | -2e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.4e+09   |\n",
      "---------------------------------------\n",
      "Episode 830  finished with cumulative reward: -10097000.0 and \n",
      "with an average reward of: -4037.18512594962\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2073329\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4038.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.79e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1013      |\n",
      "|    time_elapsed         | 28208     |\n",
      "|    total_timesteps      | 2074624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0214    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.06e+09  |\n",
      "|    n_updates            | 10120     |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 831  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2075830\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.71e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1014         |\n",
      "|    time_elapsed         | 28236        |\n",
      "|    total_timesteps      | 2076672      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0264       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.11e+09     |\n",
      "|    n_updates            | 10130        |\n",
      "|    policy_gradient_loss | -2.6e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.42e+10     |\n",
      "------------------------------------------\n",
      "Episode 832  finished with cumulative reward: -1707500.0 and \n",
      "with an average reward of: -682.7269092363055\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2078331\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -683.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1015         |\n",
      "|    time_elapsed         | 28263        |\n",
      "|    total_timesteps      | 2078720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0298       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.02e+09     |\n",
      "|    n_updates            | 10140        |\n",
      "|    policy_gradient_loss | -6.32e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.08e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.68e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1016         |\n",
      "|    time_elapsed         | 28283        |\n",
      "|    total_timesteps      | 2080768      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0262       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2e+09        |\n",
      "|    n_updates            | 10150        |\n",
      "|    policy_gradient_loss | -5.7e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.44e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 833  finished with cumulative reward: -3849500.0 and \n",
      "with an average reward of: -1539.1843262694922\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2080832\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1539.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1017      |\n",
      "|    time_elapsed         | 28312     |\n",
      "|    total_timesteps      | 2082816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.58e+09  |\n",
      "|    n_updates            | 10160     |\n",
      "|    policy_gradient_loss | -1.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.79e+09  |\n",
      "---------------------------------------\n",
      "Episode 834  finished with cumulative reward: -6629000.0 and \n",
      "with an average reward of: -2650.5397840863657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2083333\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2651.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1018      |\n",
      "|    time_elapsed         | 28341     |\n",
      "|    total_timesteps      | 2084864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0224    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.46e+09  |\n",
      "|    n_updates            | 10170     |\n",
      "|    policy_gradient_loss | -1.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n",
      "Episode 835  finished with cumulative reward: -4155500.0 and \n",
      "with an average reward of: -1661.5353858456617\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2085834\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1662.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1019      |\n",
      "|    time_elapsed         | 28369     |\n",
      "|    total_timesteps      | 2086912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0244    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.3e+09   |\n",
      "|    n_updates            | 10180     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.3e+09   |\n",
      "---------------------------------------\n",
      "Episode 836  finished with cumulative reward: -738500.0 and \n",
      "with an average reward of: -295.281887245102\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2088335\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -295.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1020      |\n",
      "|    time_elapsed         | 28396     |\n",
      "|    total_timesteps      | 2088960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.97e+08  |\n",
      "|    n_updates            | 10190     |\n",
      "|    policy_gradient_loss | -3.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.6e+09   |\n",
      "---------------------------------------\n",
      "Episode 837  finished with cumulative reward: -10428500.0 and \n",
      "with an average reward of: -4169.732107157137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2090836\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4171.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1021      |\n",
      "|    time_elapsed         | 28424     |\n",
      "|    total_timesteps      | 2091008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.034     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.29e+09  |\n",
      "|    n_updates            | 10200     |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.5e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1022      |\n",
      "|    time_elapsed         | 28445     |\n",
      "|    total_timesteps      | 2093056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0196    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.42e+10  |\n",
      "|    n_updates            | 10210     |\n",
      "|    policy_gradient_loss | -5.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 838  finished with cumulative reward: -12137000.0 and \n",
      "with an average reward of: -4852.8588564574175\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2093337\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4854.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1023      |\n",
      "|    time_elapsed         | 28473     |\n",
      "|    total_timesteps      | 2095104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.021     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.2e+10   |\n",
      "|    n_updates            | 10220     |\n",
      "|    policy_gradient_loss | -4.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.97e+10  |\n",
      "---------------------------------------\n",
      "Episode 839  finished with cumulative reward: -8414000.0 and \n",
      "with an average reward of: -3364.2542982806876\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2095838\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3365.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1024      |\n",
      "|    time_elapsed         | 28501     |\n",
      "|    total_timesteps      | 2097152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0187    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.31e+10  |\n",
      "|    n_updates            | 10230     |\n",
      "|    policy_gradient_loss | -1.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 840  finished with cumulative reward: -1605500.0 and \n",
      "with an average reward of: -641.9432227109156\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2098339\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -642.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1025      |\n",
      "|    time_elapsed         | 28529     |\n",
      "|    total_timesteps      | 2099200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0317    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.33e+09  |\n",
      "|    n_updates            | 10240     |\n",
      "|    policy_gradient_loss | -2.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.17e+09  |\n",
      "---------------------------------------\n",
      "Episode 841  finished with cumulative reward: -1962500.0 and \n",
      "with an average reward of: -784.6861255497801\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2100840\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -785.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1026      |\n",
      "|    time_elapsed         | 28558     |\n",
      "|    total_timesteps      | 2101248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.05e+09  |\n",
      "|    n_updates            | 10250     |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.73e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1027      |\n",
      "|    time_elapsed         | 28579     |\n",
      "|    total_timesteps      | 2103296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0329    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.5e+09   |\n",
      "|    n_updates            | 10260     |\n",
      "|    policy_gradient_loss | -2.31e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 842  finished with cumulative reward: -5073500.0 and \n",
      "with an average reward of: -2028.5885645741703\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2103341\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2029.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1028      |\n",
      "|    time_elapsed         | 28607     |\n",
      "|    total_timesteps      | 2105344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.9e+09   |\n",
      "|    n_updates            | 10270     |\n",
      "|    policy_gradient_loss | -2.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 843  finished with cumulative reward: -4895000.0 and \n",
      "with an average reward of: -1957.2171131547382\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2105842\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1958.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1029      |\n",
      "|    time_elapsed         | 28635     |\n",
      "|    total_timesteps      | 2107392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.17e+09  |\n",
      "|    n_updates            | 10280     |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 844  finished with cumulative reward: -1835000.0 and \n",
      "with an average reward of: -733.7065173930428\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2108343\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -734.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1030      |\n",
      "|    time_elapsed         | 28664     |\n",
      "|    total_timesteps      | 2109440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0239    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.25e+09  |\n",
      "|    n_updates            | 10290     |\n",
      "|    policy_gradient_loss | -2.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.77e+09  |\n",
      "---------------------------------------\n",
      "Episode 845  finished with cumulative reward: -2396000.0 and \n",
      "with an average reward of: -958.0167932826869\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2110844\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -958.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.55e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1031          |\n",
      "|    time_elapsed         | 28693         |\n",
      "|    total_timesteps      | 2111488       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 9.0221874e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0373        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.66e+07      |\n",
      "|    n_updates            | 10300         |\n",
      "|    policy_gradient_loss | -5.61e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.21e+09      |\n",
      "-------------------------------------------\n",
      "Episode 846  finished with cumulative reward: -5048000.0 and \n",
      "with an average reward of: -2018.392642942823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2113345\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2019.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1032      |\n",
      "|    time_elapsed         | 28721     |\n",
      "|    total_timesteps      | 2113536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.87e+09  |\n",
      "|    n_updates            | 10310     |\n",
      "|    policy_gradient_loss | -2.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.09e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1033      |\n",
      "|    time_elapsed         | 28742     |\n",
      "|    total_timesteps      | 2115584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.18e+09  |\n",
      "|    n_updates            | 10320     |\n",
      "|    policy_gradient_loss | -2.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.01e+09  |\n",
      "---------------------------------------\n",
      "Episode 847  finished with cumulative reward: -3212000.0 and \n",
      "with an average reward of: -1284.2862854858056\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2115846\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1284.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1034      |\n",
      "|    time_elapsed         | 28769     |\n",
      "|    total_timesteps      | 2117632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.036     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.4e+09   |\n",
      "|    n_updates            | 10330     |\n",
      "|    policy_gradient_loss | -2.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.57e+09  |\n",
      "---------------------------------------\n",
      "Episode 848  finished with cumulative reward: -10632500.0 and \n",
      "with an average reward of: -4251.2994802079165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2118347\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4253.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1035      |\n",
      "|    time_elapsed         | 28798     |\n",
      "|    total_timesteps      | 2119680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0263    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.34e+10  |\n",
      "|    n_updates            | 10340     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.27e+10  |\n",
      "---------------------------------------\n",
      "Episode 849  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2120848\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1036      |\n",
      "|    time_elapsed         | 28826     |\n",
      "|    total_timesteps      | 2121728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0233    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.59e+09  |\n",
      "|    n_updates            | 10350     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n",
      "Episode 850  finished with cumulative reward: -10377500.0 and \n",
      "with an average reward of: -4149.340263894443\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2123349\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4151.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1037      |\n",
      "|    time_elapsed         | 28854     |\n",
      "|    total_timesteps      | 2123776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.023     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.03e+09  |\n",
      "|    n_updates            | 10360     |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1038      |\n",
      "|    time_elapsed         | 28875     |\n",
      "|    total_timesteps      | 2125824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0248    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.68e+10  |\n",
      "|    n_updates            | 10370     |\n",
      "|    policy_gradient_loss | -4.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.02e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 851  finished with cumulative reward: -7572500.0 and \n",
      "with an average reward of: -3027.7888844462213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2125850\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3029.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.77e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1039      |\n",
      "|    time_elapsed         | 28903     |\n",
      "|    total_timesteps      | 2127872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0247    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.67e+09  |\n",
      "|    n_updates            | 10380     |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 852  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2128351\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.8e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1040         |\n",
      "|    time_elapsed         | 28931        |\n",
      "|    total_timesteps      | 2129920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0303       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6e+09        |\n",
      "|    n_updates            | 10390        |\n",
      "|    policy_gradient_loss | -4.42e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.13e+10     |\n",
      "------------------------------------------\n",
      "Episode 853  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2130852\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1041      |\n",
      "|    time_elapsed         | 28959     |\n",
      "|    total_timesteps      | 2131968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0227    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.37e+09  |\n",
      "|    n_updates            | 10400     |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 854  finished with cumulative reward: -1784000.0 and \n",
      "with an average reward of: -713.3146741303478\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2133353\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -713.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.79e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1042         |\n",
      "|    time_elapsed         | 28988        |\n",
      "|    total_timesteps      | 2134016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0299       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.22e+09     |\n",
      "|    n_updates            | 10410        |\n",
      "|    policy_gradient_loss | -5.15e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.75e+09     |\n",
      "------------------------------------------\n",
      "Episode 855  finished with cumulative reward: -12647000.0 and \n",
      "with an average reward of: -5056.777289084366\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2135854\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5058.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1043      |\n",
      "|    time_elapsed         | 29016     |\n",
      "|    total_timesteps      | 2136064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0234    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.1e+10   |\n",
      "|    n_updates            | 10420     |\n",
      "|    policy_gradient_loss | -3.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.67e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1044      |\n",
      "|    time_elapsed         | 29037     |\n",
      "|    total_timesteps      | 2138112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0234    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.32e+09  |\n",
      "|    n_updates            | 10430     |\n",
      "|    policy_gradient_loss | -1.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 856  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2138355\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.88e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1045          |\n",
      "|    time_elapsed         | 29064         |\n",
      "|    total_timesteps      | 2140160       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1350494e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0236        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.82e+09      |\n",
      "|    n_updates            | 10440         |\n",
      "|    policy_gradient_loss | -7.01e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.61e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 857  finished with cumulative reward: -3441500.0 and \n",
      "with an average reward of: -1376.0495801679328\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2140856\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1376.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.86e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1046         |\n",
      "|    time_elapsed         | 29093        |\n",
      "|    total_timesteps      | 2142208      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0341       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.93e+09     |\n",
      "|    n_updates            | 10450        |\n",
      "|    policy_gradient_loss | -5.73e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.77e+09     |\n",
      "------------------------------------------\n",
      "Episode 858  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2143357\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.81e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1047          |\n",
      "|    time_elapsed         | 29121         |\n",
      "|    total_timesteps      | 2144256       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8230716e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0335        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.98e+09      |\n",
      "|    n_updates            | 10460         |\n",
      "|    policy_gradient_loss | -6.96e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.76e+09      |\n",
      "-------------------------------------------\n",
      "Episode 859  finished with cumulative reward: -6654500.0 and \n",
      "with an average reward of: -2660.735705717713\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2145858\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2661.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1048      |\n",
      "|    time_elapsed         | 29150     |\n",
      "|    total_timesteps      | 2146304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.16e+09  |\n",
      "|    n_updates            | 10470     |\n",
      "|    policy_gradient_loss | -1.31e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.1e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1049      |\n",
      "|    time_elapsed         | 29171     |\n",
      "|    total_timesteps      | 2148352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.13e+09  |\n",
      "|    n_updates            | 10480     |\n",
      "|    policy_gradient_loss | -2.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.71e+09  |\n",
      "---------------------------------------\n",
      "Episode 860  finished with cumulative reward: -3696500.0 and \n",
      "with an average reward of: -1478.0087964814074\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2148359\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1478.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1050      |\n",
      "|    time_elapsed         | 29199     |\n",
      "|    total_timesteps      | 2150400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.45e+09  |\n",
      "|    n_updates            | 10490     |\n",
      "|    policy_gradient_loss | -1.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.51e+09  |\n",
      "---------------------------------------\n",
      "Episode 861  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2150860\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1051      |\n",
      "|    time_elapsed         | 29227     |\n",
      "|    total_timesteps      | 2152448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.028     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.78e+09  |\n",
      "|    n_updates            | 10500     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.53e+09  |\n",
      "---------------------------------------\n",
      "Episode 862  finished with cumulative reward: -10530500.0 and \n",
      "with an average reward of: -4210.515793682527\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2153361\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4212.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1052      |\n",
      "|    time_elapsed         | 29255     |\n",
      "|    total_timesteps      | 2154496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.12e+09  |\n",
      "|    n_updates            | 10510     |\n",
      "|    policy_gradient_loss | -2.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.25e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 863  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2155862\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1053      |\n",
      "|    time_elapsed         | 29283     |\n",
      "|    total_timesteps      | 2156544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.41e+09  |\n",
      "|    n_updates            | 10520     |\n",
      "|    policy_gradient_loss | -3.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.53e+09  |\n",
      "---------------------------------------\n",
      "Episode 864  finished with cumulative reward: -6603500.0 and \n",
      "with an average reward of: -2640.343862455018\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2158363\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2641.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1054      |\n",
      "|    time_elapsed         | 29310     |\n",
      "|    total_timesteps      | 2158592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0186    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.25e+10  |\n",
      "|    n_updates            | 10530     |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.37e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1055      |\n",
      "|    time_elapsed         | 29331     |\n",
      "|    total_timesteps      | 2160640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0213    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+10  |\n",
      "|    n_updates            | 10540     |\n",
      "|    policy_gradient_loss | -3.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 865  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2160864\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1056      |\n",
      "|    time_elapsed         | 29359     |\n",
      "|    total_timesteps      | 2162688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.96e+09  |\n",
      "|    n_updates            | 10550     |\n",
      "|    policy_gradient_loss | -1.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.86e+09  |\n",
      "---------------------------------------\n",
      "Episode 866  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2163365\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.87e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1057         |\n",
      "|    time_elapsed         | 29387        |\n",
      "|    total_timesteps      | 2164736      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0299       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.86e+09     |\n",
      "|    n_updates            | 10560        |\n",
      "|    policy_gradient_loss | -2.29e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.19e+10     |\n",
      "------------------------------------------\n",
      "Episode 867  finished with cumulative reward: -7598000.0 and \n",
      "with an average reward of: -3037.984806077569\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2165866\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3039.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1058      |\n",
      "|    time_elapsed         | 29415     |\n",
      "|    total_timesteps      | 2166784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.83e+09  |\n",
      "|    n_updates            | 10570     |\n",
      "|    policy_gradient_loss | -2.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.57e+09  |\n",
      "---------------------------------------\n",
      "Episode 868  finished with cumulative reward: -14279000.0 and \n",
      "with an average reward of: -5709.316273490604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2168367\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5711.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1059      |\n",
      "|    time_elapsed         | 29444     |\n",
      "|    total_timesteps      | 2168832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.018     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.96e+10  |\n",
      "|    n_updates            | 10580     |\n",
      "|    policy_gradient_loss | -1.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.81e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 869  finished with cumulative reward: -4283000.0 and \n",
      "with an average reward of: -1712.5149940023991\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2170868\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1713.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1060      |\n",
      "|    time_elapsed         | 29472     |\n",
      "|    total_timesteps      | 2170880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0207    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.33e+10  |\n",
      "|    n_updates            | 10590     |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1061      |\n",
      "|    time_elapsed         | 29493     |\n",
      "|    total_timesteps      | 2172928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0227    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.3e+09   |\n",
      "|    n_updates            | 10600     |\n",
      "|    policy_gradient_loss | -2.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 870  finished with cumulative reward: -4614500.0 and \n",
      "with an average reward of: -1845.061975209916\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2173369\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1845.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1062      |\n",
      "|    time_elapsed         | 29521     |\n",
      "|    total_timesteps      | 2174976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0305    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.68e+09  |\n",
      "|    n_updates            | 10610     |\n",
      "|    policy_gradient_loss | -3.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 871  finished with cumulative reward: -8643500.0 and \n",
      "with an average reward of: -3456.017592962815\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2175870\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3457.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1063      |\n",
      "|    time_elapsed         | 29550     |\n",
      "|    total_timesteps      | 2177024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.68e+09  |\n",
      "|    n_updates            | 10620     |\n",
      "|    policy_gradient_loss | -9.77e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 872  finished with cumulative reward: -4257500.0 and \n",
      "with an average reward of: -1702.3190723710516\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2178371\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1703.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.95e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1064          |\n",
      "|    time_elapsed         | 29577         |\n",
      "|    total_timesteps      | 2179072       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0325        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.21e+09      |\n",
      "|    n_updates            | 10630         |\n",
      "|    policy_gradient_loss | -6.73e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.95e+09      |\n",
      "-------------------------------------------\n",
      "Episode 873  finished with cumulative reward: -5915000.0 and \n",
      "with an average reward of: -2365.0539784086363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2180872\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2366.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1065      |\n",
      "|    time_elapsed         | 29606     |\n",
      "|    total_timesteps      | 2181120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0216    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.35e+09  |\n",
      "|    n_updates            | 10640     |\n",
      "|    policy_gradient_loss | -1.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.54e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1066      |\n",
      "|    time_elapsed         | 29627     |\n",
      "|    total_timesteps      | 2183168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.52e+09  |\n",
      "|    n_updates            | 10650     |\n",
      "|    policy_gradient_loss | -9.72e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 874  finished with cumulative reward: -6986000.0 and \n",
      "with an average reward of: -2793.28268692523\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2183373\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2794.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1067      |\n",
      "|    time_elapsed         | 29655     |\n",
      "|    total_timesteps      | 2185216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0181    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.05e+10  |\n",
      "|    n_updates            | 10660     |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 875  finished with cumulative reward: -4971500.0 and \n",
      "with an average reward of: -1987.8048780487804\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2185874\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1988.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1068      |\n",
      "|    time_elapsed         | 29683     |\n",
      "|    total_timesteps      | 2187264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0226    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.55e+09  |\n",
      "|    n_updates            | 10670     |\n",
      "|    policy_gradient_loss | -2.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 876  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2188375\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1069      |\n",
      "|    time_elapsed         | 29711     |\n",
      "|    total_timesteps      | 2189312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0323    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.89e+09  |\n",
      "|    n_updates            | 10680     |\n",
      "|    policy_gradient_loss | -1.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.75e+09  |\n",
      "---------------------------------------\n",
      "Episode 877  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2190876\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1070      |\n",
      "|    time_elapsed         | 29739     |\n",
      "|    total_timesteps      | 2191360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0284    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.08e+09  |\n",
      "|    n_updates            | 10690     |\n",
      "|    policy_gradient_loss | -1.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.92e+09  |\n",
      "---------------------------------------\n",
      "Episode 878  finished with cumulative reward: -11117000.0 and \n",
      "with an average reward of: -4445.021991203518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2193377\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4446.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1071      |\n",
      "|    time_elapsed         | 29767     |\n",
      "|    total_timesteps      | 2193408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0201    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.35e+09  |\n",
      "|    n_updates            | 10700     |\n",
      "|    policy_gradient_loss | -1.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1072      |\n",
      "|    time_elapsed         | 29788     |\n",
      "|    total_timesteps      | 2195456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0191    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.57e+09  |\n",
      "|    n_updates            | 10710     |\n",
      "|    policy_gradient_loss | -7.89e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.84e+10  |\n",
      "---------------------------------------\n",
      "Episode 879  finished with cumulative reward: -3798500.0 and \n",
      "with an average reward of: -1518.7924830067973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2195878\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1519.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.02e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1073         |\n",
      "|    time_elapsed         | 29817        |\n",
      "|    total_timesteps      | 2197504      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0248       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.43e+09     |\n",
      "|    n_updates            | 10720        |\n",
      "|    policy_gradient_loss | -2.06e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.08e+09     |\n",
      "------------------------------------------\n",
      "Episode 880  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2198379\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1074      |\n",
      "|    time_elapsed         | 29845     |\n",
      "|    total_timesteps      | 2199552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.46e+09  |\n",
      "|    n_updates            | 10730     |\n",
      "|    policy_gradient_loss | -2.87e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 881  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2200880\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -6.1e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 73       |\n",
      "|    iterations           | 1075     |\n",
      "|    time_elapsed         | 29872    |\n",
      "|    total_timesteps      | 2201600  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0237   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.21e+10 |\n",
      "|    n_updates            | 10740    |\n",
      "|    policy_gradient_loss | -3.4e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 2.21e+10 |\n",
      "--------------------------------------\n",
      "Episode 882  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2203381\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1076      |\n",
      "|    time_elapsed         | 29900     |\n",
      "|    total_timesteps      | 2203648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0235    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.45e+09  |\n",
      "|    n_updates            | 10750     |\n",
      "|    policy_gradient_loss | -1.94e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.77e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1077      |\n",
      "|    time_elapsed         | 29921     |\n",
      "|    total_timesteps      | 2205696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0274    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.65e+09  |\n",
      "|    n_updates            | 10760     |\n",
      "|    policy_gradient_loss | -2.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.03e+09  |\n",
      "---------------------------------------\n",
      "Episode 883  finished with cumulative reward: -6323000.0 and \n",
      "with an average reward of: -2528.1887245101957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2205882\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2529.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1078      |\n",
      "|    time_elapsed         | 29949     |\n",
      "|    total_timesteps      | 2207744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0228    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.51e+09  |\n",
      "|    n_updates            | 10770     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "Episode 884  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2208383\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1079      |\n",
      "|    time_elapsed         | 29976     |\n",
      "|    total_timesteps      | 2209792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0242    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.74e+09  |\n",
      "|    n_updates            | 10780     |\n",
      "|    policy_gradient_loss | -6.04e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "Episode 885  finished with cumulative reward: -9561500.0 and \n",
      "with an average reward of: -3823.0707716913234\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2210884\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3824.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1080      |\n",
      "|    time_elapsed         | 30004     |\n",
      "|    total_timesteps      | 2211840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0229    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.39e+09  |\n",
      "|    n_updates            | 10790     |\n",
      "|    policy_gradient_loss | -1.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 886  finished with cumulative reward: -3416000.0 and \n",
      "with an average reward of: -1365.8536585365853\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2213385\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1366.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1081      |\n",
      "|    time_elapsed         | 30032     |\n",
      "|    total_timesteps      | 2213888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0261    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.98e+09  |\n",
      "|    n_updates            | 10800     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.74e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 887  finished with cumulative reward: -5226500.0 and \n",
      "with an average reward of: -2089.764094362255\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2215886\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2090.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1082      |\n",
      "|    time_elapsed         | 30060     |\n",
      "|    total_timesteps      | 2215936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0286    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.65e+09  |\n",
      "|    n_updates            | 10810     |\n",
      "|    policy_gradient_loss | -1.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.56e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1083      |\n",
      "|    time_elapsed         | 30081     |\n",
      "|    total_timesteps      | 2217984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.99e+09  |\n",
      "|    n_updates            | 10820     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 888  finished with cumulative reward: -6450500.0 and \n",
      "with an average reward of: -2579.168332666933\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2218387\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2580.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1084      |\n",
      "|    time_elapsed         | 30110     |\n",
      "|    total_timesteps      | 2220032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0207    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.41e+09  |\n",
      "|    n_updates            | 10830     |\n",
      "|    policy_gradient_loss | -2.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.33e+10  |\n",
      "---------------------------------------\n",
      "Episode 889  finished with cumulative reward: -3620000.0 and \n",
      "with an average reward of: -1447.421031587365\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2220888\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1448.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1085      |\n",
      "|    time_elapsed         | 30138     |\n",
      "|    total_timesteps      | 2222080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0262    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.51e+09  |\n",
      "|    n_updates            | 10840     |\n",
      "|    policy_gradient_loss | -8.98e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 890  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2223389\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1086      |\n",
      "|    time_elapsed         | 30167     |\n",
      "|    total_timesteps      | 2224128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0389    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.4e+09   |\n",
      "|    n_updates            | 10850     |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.07e+09  |\n",
      "---------------------------------------\n",
      "Episode 891  finished with cumulative reward: -6654500.0 and \n",
      "with an average reward of: -2660.735705717713\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2225890\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2661.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1087      |\n",
      "|    time_elapsed         | 30194     |\n",
      "|    total_timesteps      | 2226176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.024     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.18e+09  |\n",
      "|    n_updates            | 10860     |\n",
      "|    policy_gradient_loss | -1.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.57e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.11e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1088          |\n",
      "|    time_elapsed         | 30216         |\n",
      "|    total_timesteps      | 2228224       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4610504e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0298        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.27e+09      |\n",
      "|    n_updates            | 10870         |\n",
      "|    policy_gradient_loss | -1.1e-05      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.86e+09      |\n",
      "-------------------------------------------\n",
      "Episode 892  finished with cumulative reward: -6986000.0 and \n",
      "with an average reward of: -2793.28268692523\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2228391\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2794.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.15e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1089      |\n",
      "|    time_elapsed         | 30244     |\n",
      "|    total_timesteps      | 2230272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.03e+09  |\n",
      "|    n_updates            | 10880     |\n",
      "|    policy_gradient_loss | -2.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.73e+10  |\n",
      "---------------------------------------\n",
      "Episode 893  finished with cumulative reward: -6960500.0 and \n",
      "with an average reward of: -2783.0867652938823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2230892\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2784.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.17e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1090          |\n",
      "|    time_elapsed         | 30271         |\n",
      "|    total_timesteps      | 2232320       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8230716e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0266        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.22e+09      |\n",
      "|    n_updates            | 10890         |\n",
      "|    policy_gradient_loss | -8.35e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.8e+09       |\n",
      "-------------------------------------------\n",
      "Episode 894  finished with cumulative reward: -3416000.0 and \n",
      "with an average reward of: -1365.8536585365853\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2233393\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1366.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1091      |\n",
      "|    time_elapsed         | 30300     |\n",
      "|    total_timesteps      | 2234368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.019     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.43e+09  |\n",
      "|    n_updates            | 10900     |\n",
      "|    policy_gradient_loss | -9.98e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.91e+10  |\n",
      "---------------------------------------\n",
      "Episode 895  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2235894\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.13e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1092          |\n",
      "|    time_elapsed         | 30328         |\n",
      "|    total_timesteps      | 2236416       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0314        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.29e+08      |\n",
      "|    n_updates            | 10910         |\n",
      "|    policy_gradient_loss | -5.03e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.69e+09      |\n",
      "-------------------------------------------\n",
      "Episode 896  finished with cumulative reward: -6960500.0 and \n",
      "with an average reward of: -2783.0867652938823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2238395\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2784.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1093      |\n",
      "|    time_elapsed         | 30356     |\n",
      "|    total_timesteps      | 2238464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.026     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.59e+10  |\n",
      "|    n_updates            | 10920     |\n",
      "|    policy_gradient_loss | -4.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.16e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.13e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1094          |\n",
      "|    time_elapsed         | 30376         |\n",
      "|    total_timesteps      | 2240512       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0332        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.98e+09      |\n",
      "|    n_updates            | 10930         |\n",
      "|    policy_gradient_loss | -3.86e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.02e+09      |\n",
      "-------------------------------------------\n",
      "Episode 897  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2240896\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1095      |\n",
      "|    time_elapsed         | 30405     |\n",
      "|    total_timesteps      | 2242560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0249    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.89e+09  |\n",
      "|    n_updates            | 10940     |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 898  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2243397\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1096      |\n",
      "|    time_elapsed         | 30433     |\n",
      "|    total_timesteps      | 2244608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.023     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.2e+09   |\n",
      "|    n_updates            | 10950     |\n",
      "|    policy_gradient_loss | -9.71e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 899  finished with cumulative reward: -5813000.0 and \n",
      "with an average reward of: -2324.2702918832465\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2245898\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2325.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1097         |\n",
      "|    time_elapsed         | 30460        |\n",
      "|    total_timesteps      | 2246656      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 1.717126e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0289       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.84e+09     |\n",
      "|    n_updates            | 10960        |\n",
      "|    policy_gradient_loss | -1.03e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.7e+09      |\n",
      "------------------------------------------\n",
      "Episode 900  finished with cumulative reward: -7827500.0 and \n",
      "with an average reward of: -3129.748100759696\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2248399\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3131.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1098      |\n",
      "|    time_elapsed         | 30488     |\n",
      "|    total_timesteps      | 2248704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0278    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.82e+09  |\n",
      "|    n_updates            | 10970     |\n",
      "|    policy_gradient_loss | -1.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.15e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1099      |\n",
      "|    time_elapsed         | 30509     |\n",
      "|    total_timesteps      | 2250752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0217    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.55e+10  |\n",
      "|    n_updates            | 10980     |\n",
      "|    policy_gradient_loss | -9.26e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.42e+10  |\n",
      "---------------------------------------\n",
      "Episode 901  finished with cumulative reward: -10301000.0 and \n",
      "with an average reward of: -4118.752499000399\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2250900\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4120.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1100      |\n",
      "|    time_elapsed         | 30537     |\n",
      "|    total_timesteps      | 2252800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.02      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.98e+09  |\n",
      "|    n_updates            | 10990     |\n",
      "|    policy_gradient_loss | -9.46e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 902  finished with cumulative reward: -10122500.0 and \n",
      "with an average reward of: -4047.381047580968\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2253401\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4049.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1101      |\n",
      "|    time_elapsed         | 30565     |\n",
      "|    total_timesteps      | 2254848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.02      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.26e+10  |\n",
      "|    n_updates            | 11000     |\n",
      "|    policy_gradient_loss | -2.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.58e+10  |\n",
      "---------------------------------------\n",
      "Episode 903  finished with cumulative reward: -2931500.0 and \n",
      "with an average reward of: -1172.1311475409836\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2255902\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1172.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1102      |\n",
      "|    time_elapsed         | 30593     |\n",
      "|    total_timesteps      | 2256896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0417    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.57e+09  |\n",
      "|    n_updates            | 11010     |\n",
      "|    policy_gradient_loss | -2.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.8e+09   |\n",
      "---------------------------------------\n",
      "Episode 904  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2258403\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1103      |\n",
      "|    time_elapsed         | 30622     |\n",
      "|    total_timesteps      | 2258944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.52e+09  |\n",
      "|    n_updates            | 11020     |\n",
      "|    policy_gradient_loss | -3.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 905  finished with cumulative reward: -9306500.0 and \n",
      "with an average reward of: -3721.111555377849\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2260904\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3722.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1104      |\n",
      "|    time_elapsed         | 30649     |\n",
      "|    total_timesteps      | 2260992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0311    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.34e+09  |\n",
      "|    n_updates            | 11030     |\n",
      "|    policy_gradient_loss | -2.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.3e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1105      |\n",
      "|    time_elapsed         | 30670     |\n",
      "|    total_timesteps      | 2263040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0253    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+10  |\n",
      "|    n_updates            | 11040     |\n",
      "|    policy_gradient_loss | -1.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.51e+10  |\n",
      "---------------------------------------\n",
      "Episode 906  finished with cumulative reward: -7419500.0 and \n",
      "with an average reward of: -2966.613354658137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2263405\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2967.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1106      |\n",
      "|    time_elapsed         | 30700     |\n",
      "|    total_timesteps      | 2265088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0216    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.52e+09  |\n",
      "|    n_updates            | 11050     |\n",
      "|    policy_gradient_loss | -1.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 907  finished with cumulative reward: -2013500.0 and \n",
      "with an average reward of: -805.077968812475\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2265906\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -805.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1107      |\n",
      "|    time_elapsed         | 30727     |\n",
      "|    total_timesteps      | 2267136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0296    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.48e+09  |\n",
      "|    n_updates            | 11060     |\n",
      "|    policy_gradient_loss | -1.96e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 908  finished with cumulative reward: -4512500.0 and \n",
      "with an average reward of: -1804.2782886845262\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2268407\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1805.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1108      |\n",
      "|    time_elapsed         | 30755     |\n",
      "|    total_timesteps      | 2269184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.45e+09  |\n",
      "|    n_updates            | 11070     |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+09  |\n",
      "---------------------------------------\n",
      "Episode 909  finished with cumulative reward: -5277500.0 and \n",
      "with an average reward of: -2110.15593762495\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2270908\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2111.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1109      |\n",
      "|    time_elapsed         | 30783     |\n",
      "|    total_timesteps      | 2271232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0257    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.62e+09  |\n",
      "|    n_updates            | 11080     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1110      |\n",
      "|    time_elapsed         | 30804     |\n",
      "|    total_timesteps      | 2273280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0277    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.19e+09  |\n",
      "|    n_updates            | 11090     |\n",
      "|    policy_gradient_loss | -2.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.44e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 910  finished with cumulative reward: -4130000.0 and \n",
      "with an average reward of: -1651.3394642143144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2273409\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1652.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1111      |\n",
      "|    time_elapsed         | 30832     |\n",
      "|    total_timesteps      | 2275328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0248    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.18e+09  |\n",
      "|    n_updates            | 11100     |\n",
      "|    policy_gradient_loss | -2.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.34e+10  |\n",
      "---------------------------------------\n",
      "Episode 911  finished with cumulative reward: -6501500.0 and \n",
      "with an average reward of: -2599.5601759296283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2275910\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2600.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1112      |\n",
      "|    time_elapsed         | 30860     |\n",
      "|    total_timesteps      | 2277376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0239    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.04e+09  |\n",
      "|    n_updates            | 11110     |\n",
      "|    policy_gradient_loss | -1.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 912  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2278411\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1113      |\n",
      "|    time_elapsed         | 30887     |\n",
      "|    total_timesteps      | 2279424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0268    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.62e+09  |\n",
      "|    n_updates            | 11120     |\n",
      "|    policy_gradient_loss | -9.81e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.71e+09  |\n",
      "---------------------------------------\n",
      "Episode 913  finished with cumulative reward: -7062500.0 and \n",
      "with an average reward of: -2823.870451819272\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2280912\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2825.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1114      |\n",
      "|    time_elapsed         | 30915     |\n",
      "|    total_timesteps      | 2281472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.03      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.49e+09  |\n",
      "|    n_updates            | 11130     |\n",
      "|    policy_gradient_loss | -1.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.41e+09  |\n",
      "---------------------------------------\n",
      "Episode 914  finished with cumulative reward: -9765500.0 and \n",
      "with an average reward of: -3904.638144742103\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2283413\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3906.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1115      |\n",
      "|    time_elapsed         | 30942     |\n",
      "|    total_timesteps      | 2283520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0253    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.3e+10   |\n",
      "|    n_updates            | 11140     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.82e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1116      |\n",
      "|    time_elapsed         | 30963     |\n",
      "|    total_timesteps      | 2285568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0233    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e+10  |\n",
      "|    n_updates            | 11150     |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.86e+10  |\n",
      "---------------------------------------\n",
      "Episode 915  finished with cumulative reward: -1809500.0 and \n",
      "with an average reward of: -723.5105957616953\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2285914\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -723.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1117      |\n",
      "|    time_elapsed         | 30992     |\n",
      "|    total_timesteps      | 2287616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.034     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.5e+09   |\n",
      "|    n_updates            | 11160     |\n",
      "|    policy_gradient_loss | -2.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.71e+09  |\n",
      "---------------------------------------\n",
      "Episode 916  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2288415\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1118      |\n",
      "|    time_elapsed         | 31020     |\n",
      "|    total_timesteps      | 2289664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0312    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.02e+09  |\n",
      "|    n_updates            | 11170     |\n",
      "|    policy_gradient_loss | -4.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 917  finished with cumulative reward: -6297500.0 and \n",
      "with an average reward of: -2517.9928028788486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2290916\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2519.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1119      |\n",
      "|    time_elapsed         | 31048     |\n",
      "|    total_timesteps      | 2291712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0246    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.65e+09  |\n",
      "|    n_updates            | 11180     |\n",
      "|    policy_gradient_loss | -2.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 918  finished with cumulative reward: -6425000.0 and \n",
      "with an average reward of: -2568.972411035586\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2293417\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2570.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1120      |\n",
      "|    time_elapsed         | 31077     |\n",
      "|    total_timesteps      | 2293760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0304    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.36e+09  |\n",
      "|    n_updates            | 11190     |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.25e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1121      |\n",
      "|    time_elapsed         | 31098     |\n",
      "|    total_timesteps      | 2295808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.2e+09   |\n",
      "|    n_updates            | 11200     |\n",
      "|    policy_gradient_loss | -1.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 919  finished with cumulative reward: -8414000.0 and \n",
      "with an average reward of: -3364.2542982806876\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2295918\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3365.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1122      |\n",
      "|    time_elapsed         | 31126     |\n",
      "|    total_timesteps      | 2297856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0299    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.97e+09  |\n",
      "|    n_updates            | 11210     |\n",
      "|    policy_gradient_loss | -1.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 920  finished with cumulative reward: -7037000.0 and \n",
      "with an average reward of: -2813.674530187925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2298419\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2814.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.15e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1123      |\n",
      "|    time_elapsed         | 31154     |\n",
      "|    total_timesteps      | 2299904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.07e+09  |\n",
      "|    n_updates            | 11220     |\n",
      "|    policy_gradient_loss | -1.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "Episode 921  finished with cumulative reward: -3008000.0 and \n",
      "with an average reward of: -1202.718912435026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2300920\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1203.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1124      |\n",
      "|    time_elapsed         | 31182     |\n",
      "|    total_timesteps      | 2301952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0411    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.65e+08  |\n",
      "|    n_updates            | 11230     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.12e+09  |\n",
      "---------------------------------------\n",
      "Episode 922  finished with cumulative reward: -10479500.0 and \n",
      "with an average reward of: -4190.123950419832\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2303421\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4191.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1125      |\n",
      "|    time_elapsed         | 31210     |\n",
      "|    total_timesteps      | 2304000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.26e+08  |\n",
      "|    n_updates            | 11240     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.01e+09  |\n",
      "---------------------------------------\n",
      "Episode 923  finished with cumulative reward: -10887500.0 and \n",
      "with an average reward of: -4353.258696521391\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2305922\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4355.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1126      |\n",
      "|    time_elapsed         | 31239     |\n",
      "|    total_timesteps      | 2306048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0147    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.68e+10  |\n",
      "|    n_updates            | 11250     |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.12e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1127      |\n",
      "|    time_elapsed         | 31259     |\n",
      "|    total_timesteps      | 2308096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.28e+09  |\n",
      "|    n_updates            | 11260     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.86e+10  |\n",
      "---------------------------------------\n",
      "Episode 924  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2308423\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1128      |\n",
      "|    time_elapsed         | 31288     |\n",
      "|    total_timesteps      | 2310144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.03      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.51e+09  |\n",
      "|    n_updates            | 11270     |\n",
      "|    policy_gradient_loss | -2.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.61e+09  |\n",
      "---------------------------------------\n",
      "Episode 925  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2310924\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1129      |\n",
      "|    time_elapsed         | 31316     |\n",
      "|    total_timesteps      | 2312192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0245    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.85e+09  |\n",
      "|    n_updates            | 11280     |\n",
      "|    policy_gradient_loss | -2.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.42e+09  |\n",
      "---------------------------------------\n",
      "Episode 926  finished with cumulative reward: -8414000.0 and \n",
      "with an average reward of: -3364.2542982806876\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2313425\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3365.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1130      |\n",
      "|    time_elapsed         | 31345     |\n",
      "|    total_timesteps      | 2314240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0236    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.39e+09  |\n",
      "|    n_updates            | 11290     |\n",
      "|    policy_gradient_loss | -1.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.5e+10   |\n",
      "---------------------------------------\n",
      "Episode 927  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2315926\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1131      |\n",
      "|    time_elapsed         | 31373     |\n",
      "|    total_timesteps      | 2316288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.31e+09  |\n",
      "|    n_updates            | 11300     |\n",
      "|    policy_gradient_loss | -3.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -6.1e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 73       |\n",
      "|    iterations           | 1132     |\n",
      "|    time_elapsed         | 31394    |\n",
      "|    total_timesteps      | 2318336  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0262   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 3.55e+09 |\n",
      "|    n_updates            | 11310    |\n",
      "|    policy_gradient_loss | -2.1e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.52e+10 |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 928  finished with cumulative reward: -6578000.0 and \n",
      "with an average reward of: -2630.1479408236705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2318427\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2631.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1133      |\n",
      "|    time_elapsed         | 31422     |\n",
      "|    total_timesteps      | 2320384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.022     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.98e+09  |\n",
      "|    n_updates            | 11320     |\n",
      "|    policy_gradient_loss | -2.31e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.25e+10  |\n",
      "---------------------------------------\n",
      "Episode 929  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2320928\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1134      |\n",
      "|    time_elapsed         | 31450     |\n",
      "|    total_timesteps      | 2322432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.023     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.07e+09  |\n",
      "|    n_updates            | 11330     |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.4e+09   |\n",
      "---------------------------------------\n",
      "Episode 930  finished with cumulative reward: -10454000.0 and \n",
      "with an average reward of: -4179.928028788485\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2323429\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4181.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1135      |\n",
      "|    time_elapsed         | 31478     |\n",
      "|    total_timesteps      | 2324480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0179    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.89e+09  |\n",
      "|    n_updates            | 11340     |\n",
      "|    policy_gradient_loss | -2.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.8e+10   |\n",
      "---------------------------------------\n",
      "Episode 931  finished with cumulative reward: -4308500.0 and \n",
      "with an average reward of: -1722.7109156337465\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2325930\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1723.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1136      |\n",
      "|    time_elapsed         | 31505     |\n",
      "|    total_timesteps      | 2326528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.92e+09  |\n",
      "|    n_updates            | 11350     |\n",
      "|    policy_gradient_loss | -2.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 932  finished with cumulative reward: -4767500.0 and \n",
      "with an average reward of: -1906.2375049980008\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2328431\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1907.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1137      |\n",
      "|    time_elapsed         | 31533     |\n",
      "|    total_timesteps      | 2328576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0396    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.05e+09  |\n",
      "|    n_updates            | 11360     |\n",
      "|    policy_gradient_loss | -3.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.95e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1138      |\n",
      "|    time_elapsed         | 31554     |\n",
      "|    total_timesteps      | 2330624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.69e+09  |\n",
      "|    n_updates            | 11370     |\n",
      "|    policy_gradient_loss | -2.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 933  finished with cumulative reward: -2880500.0 and \n",
      "with an average reward of: -1151.7393042782887\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2330932\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1152.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1139      |\n",
      "|    time_elapsed         | 31582     |\n",
      "|    total_timesteps      | 2332672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0294    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.98e+09  |\n",
      "|    n_updates            | 11380     |\n",
      "|    policy_gradient_loss | -2.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.67e+09  |\n",
      "---------------------------------------\n",
      "Episode 934  finished with cumulative reward: -7190000.0 and \n",
      "with an average reward of: -2874.8500599760096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2333433\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2876.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1140      |\n",
      "|    time_elapsed         | 31610     |\n",
      "|    total_timesteps      | 2334720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0266    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.29e+10  |\n",
      "|    n_updates            | 11390     |\n",
      "|    policy_gradient_loss | -1.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n",
      "Episode 935  finished with cumulative reward: -2574500.0 and \n",
      "with an average reward of: -1029.3882447021192\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2335934\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1029.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1141      |\n",
      "|    time_elapsed         | 31638     |\n",
      "|    total_timesteps      | 2336768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0316    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.52e+09  |\n",
      "|    n_updates            | 11400     |\n",
      "|    policy_gradient_loss | -1.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n",
      "Episode 936  finished with cumulative reward: -6858500.0 and \n",
      "with an average reward of: -2742.3030787684925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2338435\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2743.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.17e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1142          |\n",
      "|    time_elapsed         | 31666         |\n",
      "|    total_timesteps      | 2338816       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0253        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.69e+08      |\n",
      "|    n_updates            | 11410         |\n",
      "|    policy_gradient_loss | -3.43e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.17e+09      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1143      |\n",
      "|    time_elapsed         | 31687     |\n",
      "|    total_timesteps      | 2340864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0256    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.93e+09  |\n",
      "|    n_updates            | 11420     |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 937  finished with cumulative reward: -7547000.0 and \n",
      "with an average reward of: -3017.592962814874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2340936\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3018.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1144      |\n",
      "|    time_elapsed         | 31715     |\n",
      "|    total_timesteps      | 2342912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.41e+09  |\n",
      "|    n_updates            | 11430     |\n",
      "|    policy_gradient_loss | -1.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.4e+10   |\n",
      "---------------------------------------\n",
      "Episode 938  finished with cumulative reward: -7598000.0 and \n",
      "with an average reward of: -3037.984806077569\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2343437\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3039.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1145      |\n",
      "|    time_elapsed         | 31744     |\n",
      "|    total_timesteps      | 2344960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0218    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.74e+09  |\n",
      "|    n_updates            | 11440     |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n",
      "Episode 939  finished with cumulative reward: -585500.0 and \n",
      "with an average reward of: -234.1063574570172\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2345938\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -234.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.02e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1146         |\n",
      "|    time_elapsed         | 31772        |\n",
      "|    total_timesteps      | 2347008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0322       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.69e+09     |\n",
      "|    n_updates            | 11450        |\n",
      "|    policy_gradient_loss | -5.22e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.96e+09     |\n",
      "------------------------------------------\n",
      "Episode 940  finished with cumulative reward: -7445000.0 and \n",
      "with an average reward of: -2976.8092762894844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2348439\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2978.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1147      |\n",
      "|    time_elapsed         | 31800     |\n",
      "|    total_timesteps      | 2349056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0413    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.58e+09  |\n",
      "|    n_updates            | 11460     |\n",
      "|    policy_gradient_loss | -1.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.61e+09  |\n",
      "---------------------------------------\n",
      "Episode 941  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2350940\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1148      |\n",
      "|    time_elapsed         | 31828     |\n",
      "|    total_timesteps      | 2351104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.95e+09  |\n",
      "|    n_updates            | 11470     |\n",
      "|    policy_gradient_loss | -3.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.59e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1149      |\n",
      "|    time_elapsed         | 31849     |\n",
      "|    total_timesteps      | 2353152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.11e+09  |\n",
      "|    n_updates            | 11480     |\n",
      "|    policy_gradient_loss | -3.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n",
      "Episode 942  finished with cumulative reward: -5685500.0 and \n",
      "with an average reward of: -2273.2906837265095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2353441\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2274.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1150      |\n",
      "|    time_elapsed         | 31877     |\n",
      "|    total_timesteps      | 2355200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0271    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.11e+09  |\n",
      "|    n_updates            | 11490     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 943  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2355942\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1151      |\n",
      "|    time_elapsed         | 31904     |\n",
      "|    total_timesteps      | 2357248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0356    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.29e+09  |\n",
      "|    n_updates            | 11500     |\n",
      "|    policy_gradient_loss | -1.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.88e+09  |\n",
      "---------------------------------------\n",
      "Episode 944  finished with cumulative reward: -7190000.0 and \n",
      "with an average reward of: -2874.8500599760096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2358443\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2876.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1152      |\n",
      "|    time_elapsed         | 31933     |\n",
      "|    total_timesteps      | 2359296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1e+10     |\n",
      "|    n_updates            | 11510     |\n",
      "|    policy_gradient_loss | -7.91e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.64e+10  |\n",
      "---------------------------------------\n",
      "Episode 945  finished with cumulative reward: -1988000.0 and \n",
      "with an average reward of: -794.8820471811275\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2360944\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -795.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1153      |\n",
      "|    time_elapsed         | 31961     |\n",
      "|    total_timesteps      | 2361344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0221    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.88e+09  |\n",
      "|    n_updates            | 11520     |\n",
      "|    policy_gradient_loss | -2.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1154      |\n",
      "|    time_elapsed         | 31982     |\n",
      "|    total_timesteps      | 2363392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0286    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.2e+09   |\n",
      "|    n_updates            | 11530     |\n",
      "|    policy_gradient_loss | -2.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.85e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 946  finished with cumulative reward: -4691000.0 and \n",
      "with an average reward of: -1875.6497401039585\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2363445\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1876.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1155      |\n",
      "|    time_elapsed         | 32010     |\n",
      "|    total_timesteps      | 2365440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.13e+09  |\n",
      "|    n_updates            | 11540     |\n",
      "|    policy_gradient_loss | -1.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 947  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2365946\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1156      |\n",
      "|    time_elapsed         | 32038     |\n",
      "|    total_timesteps      | 2367488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.2e+09   |\n",
      "|    n_updates            | 11550     |\n",
      "|    policy_gradient_loss | -2.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 948  finished with cumulative reward: -713000.0 and \n",
      "with an average reward of: -285.0859656137545\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2368447\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -285.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.11e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1157         |\n",
      "|    time_elapsed         | 32065        |\n",
      "|    total_timesteps      | 2369536      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0321       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.26e+09     |\n",
      "|    n_updates            | 11560        |\n",
      "|    policy_gradient_loss | -5.22e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.28e+09     |\n",
      "------------------------------------------\n",
      "Episode 949  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2370948\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1158      |\n",
      "|    time_elapsed         | 32093     |\n",
      "|    total_timesteps      | 2371584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0228    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.63e+09  |\n",
      "|    n_updates            | 11570     |\n",
      "|    policy_gradient_loss | -2.57e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.12e+09  |\n",
      "---------------------------------------\n",
      "Episode 950  finished with cumulative reward: -6119000.0 and \n",
      "with an average reward of: -2446.621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2373449\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2447.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1159      |\n",
      "|    time_elapsed         | 32120     |\n",
      "|    total_timesteps      | 2373632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0309    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.03e+10  |\n",
      "|    n_updates            | 11580     |\n",
      "|    policy_gradient_loss | -5.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1160      |\n",
      "|    time_elapsed         | 32142     |\n",
      "|    total_timesteps      | 2375680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.11e+09  |\n",
      "|    n_updates            | 11590     |\n",
      "|    policy_gradient_loss | -1.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.37e+09  |\n",
      "---------------------------------------\n",
      "Episode 951  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2375950\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1161      |\n",
      "|    time_elapsed         | 32171     |\n",
      "|    total_timesteps      | 2377728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0194    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.93e+09  |\n",
      "|    n_updates            | 11600     |\n",
      "|    policy_gradient_loss | -1.58e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 952  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2378451\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1162      |\n",
      "|    time_elapsed         | 32200     |\n",
      "|    total_timesteps      | 2379776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0313    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.01e+09  |\n",
      "|    n_updates            | 11610     |\n",
      "|    policy_gradient_loss | -2.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.21e+10  |\n",
      "---------------------------------------\n",
      "Episode 953  finished with cumulative reward: -9638000.0 and \n",
      "with an average reward of: -3853.6585365853657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2380952\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3855.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1163      |\n",
      "|    time_elapsed         | 32228     |\n",
      "|    total_timesteps      | 2381824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.024     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.48e+10  |\n",
      "|    n_updates            | 11620     |\n",
      "|    policy_gradient_loss | -3.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.8e+10   |\n",
      "---------------------------------------\n",
      "Episode 954  finished with cumulative reward: -483500.0 and \n",
      "with an average reward of: -193.32267093162736\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2383453\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -193.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1164      |\n",
      "|    time_elapsed         | 32256     |\n",
      "|    total_timesteps      | 2383872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.03      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.48e+09  |\n",
      "|    n_updates            | 11630     |\n",
      "|    policy_gradient_loss | -1.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.17e+09  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.06e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1165         |\n",
      "|    time_elapsed         | 32277        |\n",
      "|    total_timesteps      | 2385920      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0331       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.32e+09     |\n",
      "|    n_updates            | 11640        |\n",
      "|    policy_gradient_loss | -4.82e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.33e+09     |\n",
      "------------------------------------------\n",
      "Episode 955  finished with cumulative reward: -8363000.0 and \n",
      "with an average reward of: -3343.862455017993\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2385954\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3345.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1166      |\n",
      "|    time_elapsed         | 32304     |\n",
      "|    total_timesteps      | 2387968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0285    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.3e+09   |\n",
      "|    n_updates            | 11650     |\n",
      "|    policy_gradient_loss | -2.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.95e+10  |\n",
      "---------------------------------------\n",
      "Episode 956  finished with cumulative reward: -3416000.0 and \n",
      "with an average reward of: -1365.8536585365853\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2388455\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1366.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1167      |\n",
      "|    time_elapsed         | 32332     |\n",
      "|    total_timesteps      | 2390016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0232    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.07e+09  |\n",
      "|    n_updates            | 11660     |\n",
      "|    policy_gradient_loss | -1.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 957  finished with cumulative reward: -3441500.0 and \n",
      "with an average reward of: -1376.0495801679328\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2390956\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1376.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1168      |\n",
      "|    time_elapsed         | 32360     |\n",
      "|    total_timesteps      | 2392064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0229    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.04e+09  |\n",
      "|    n_updates            | 11670     |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.49e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 958  finished with cumulative reward: -1503500.0 and \n",
      "with an average reward of: -601.1595361855258\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2393457\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -601.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1169      |\n",
      "|    time_elapsed         | 32388     |\n",
      "|    total_timesteps      | 2394112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.22e+09  |\n",
      "|    n_updates            | 11680     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.01e+09  |\n",
      "---------------------------------------\n",
      "Episode 959  finished with cumulative reward: -5685500.0 and \n",
      "with an average reward of: -2273.2906837265095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2395958\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2274.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.96e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1170          |\n",
      "|    time_elapsed         | 32417         |\n",
      "|    total_timesteps      | 2396160       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0424        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.71e+09      |\n",
      "|    n_updates            | 11690         |\n",
      "|    policy_gradient_loss | -5.96e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.71e+09      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1171      |\n",
      "|    time_elapsed         | 32438     |\n",
      "|    total_timesteps      | 2398208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0309    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.77e+09  |\n",
      "|    n_updates            | 11700     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 960  finished with cumulative reward: -3314000.0 and \n",
      "with an average reward of: -1325.0699720111954\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2398459\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1325.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1172      |\n",
      "|    time_elapsed         | 32466     |\n",
      "|    total_timesteps      | 2400256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.57e+09  |\n",
      "|    n_updates            | 11710     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.33e+09  |\n",
      "---------------------------------------\n",
      "Episode 961  finished with cumulative reward: -4920500.0 and \n",
      "with an average reward of: -1967.4130347860855\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2400960\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1968.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1173      |\n",
      "|    time_elapsed         | 32494     |\n",
      "|    total_timesteps      | 2402304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0255    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.12e+10  |\n",
      "|    n_updates            | 11720     |\n",
      "|    policy_gradient_loss | -7.45e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.67e+10  |\n",
      "---------------------------------------\n",
      "Episode 962  finished with cumulative reward: -2600000.0 and \n",
      "with an average reward of: -1039.5841663334666\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2403461\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1040.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.89e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1174          |\n",
      "|    time_elapsed         | 32522         |\n",
      "|    total_timesteps      | 2404352       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0291        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.53e+08      |\n",
      "|    n_updates            | 11730         |\n",
      "|    policy_gradient_loss | -5.27e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.86e+09      |\n",
      "-------------------------------------------\n",
      "Episode 963  finished with cumulative reward: -4844000.0 and \n",
      "with an average reward of: -1936.8252698920433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2405962\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1937.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1175      |\n",
      "|    time_elapsed         | 32550     |\n",
      "|    total_timesteps      | 2406400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.88e+09  |\n",
      "|    n_updates            | 11740     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.32e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.85e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1176         |\n",
      "|    time_elapsed         | 32571        |\n",
      "|    total_timesteps      | 2408448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.124618e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.035        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.02e+09     |\n",
      "|    n_updates            | 11750        |\n",
      "|    policy_gradient_loss | -1.18e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.29e+09     |\n",
      "------------------------------------------\n",
      "Episode 964  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2408463\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.82e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1177      |\n",
      "|    time_elapsed         | 32600     |\n",
      "|    total_timesteps      | 2410496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0313    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.35e+09  |\n",
      "|    n_updates            | 11760     |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.8e+09   |\n",
      "---------------------------------------\n",
      "Episode 965  finished with cumulative reward: -2370500.0 and \n",
      "with an average reward of: -947.8208716513394\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2410964\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -948.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.81e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1178      |\n",
      "|    time_elapsed         | 32628     |\n",
      "|    total_timesteps      | 2412544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.4e+09   |\n",
      "|    n_updates            | 11770     |\n",
      "|    policy_gradient_loss | -1.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 966  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2413465\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1179      |\n",
      "|    time_elapsed         | 32657     |\n",
      "|    total_timesteps      | 2414592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0279    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.88e+09  |\n",
      "|    n_updates            | 11780     |\n",
      "|    policy_gradient_loss | -1.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.4e+09   |\n",
      "---------------------------------------\n",
      "Episode 967  finished with cumulative reward: -5456000.0 and \n",
      "with an average reward of: -2181.5273890443823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2415966\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2182.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.77e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1180      |\n",
      "|    time_elapsed         | 32685     |\n",
      "|    total_timesteps      | 2416640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0205    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.42e+09  |\n",
      "|    n_updates            | 11790     |\n",
      "|    policy_gradient_loss | -6.31e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 968  finished with cumulative reward: -7241000.0 and \n",
      "with an average reward of: -2895.2419032387047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2418467\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2896.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1181      |\n",
      "|    time_elapsed         | 32713     |\n",
      "|    total_timesteps      | 2418688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0292    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.32e+09  |\n",
      "|    n_updates            | 11800     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.21e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.7e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 73            |\n",
      "|    iterations           | 1182          |\n",
      "|    time_elapsed         | 32735         |\n",
      "|    total_timesteps      | 2420736       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.5774657e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0273        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.69e+09      |\n",
      "|    n_updates            | 11810         |\n",
      "|    policy_gradient_loss | -1.65e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.18e+10      |\n",
      "-------------------------------------------\n",
      "Episode 969  finished with cumulative reward: -6297500.0 and \n",
      "with an average reward of: -2517.9928028788486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2420968\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2519.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1183      |\n",
      "|    time_elapsed         | 32763     |\n",
      "|    total_timesteps      | 2422784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0319    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.53e+09  |\n",
      "|    n_updates            | 11820     |\n",
      "|    policy_gradient_loss | -2.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n",
      "Episode 970  finished with cumulative reward: -2166500.0 and \n",
      "with an average reward of: -866.2534986005597\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2423469\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -866.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1184      |\n",
      "|    time_elapsed         | 32791     |\n",
      "|    total_timesteps      | 2424832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0301    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.84e+09  |\n",
      "|    n_updates            | 11830     |\n",
      "|    policy_gradient_loss | -2.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.47e+09  |\n",
      "---------------------------------------\n",
      "Episode 971  finished with cumulative reward: -1835000.0 and \n",
      "with an average reward of: -733.7065173930428\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2425970\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -734.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 73           |\n",
      "|    iterations           | 1185         |\n",
      "|    time_elapsed         | 32819        |\n",
      "|    total_timesteps      | 2426880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0514       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.99e+07     |\n",
      "|    n_updates            | 11840        |\n",
      "|    policy_gradient_loss | -4.58e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.47e+08     |\n",
      "------------------------------------------\n",
      "Episode 972  finished with cumulative reward: -4818500.0 and \n",
      "with an average reward of: -1926.6293482606957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2428471\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1927.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1186      |\n",
      "|    time_elapsed         | 32847     |\n",
      "|    total_timesteps      | 2428928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.028     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.39e+09  |\n",
      "|    n_updates            | 11850     |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 973  finished with cumulative reward: -9689000.0 and \n",
      "with an average reward of: -3874.050379848061\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2430972\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3875.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1187      |\n",
      "|    time_elapsed         | 32876     |\n",
      "|    total_timesteps      | 2430976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0192    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.49e+10  |\n",
      "|    n_updates            | 11860     |\n",
      "|    policy_gradient_loss | -6.69e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1188      |\n",
      "|    time_elapsed         | 32897     |\n",
      "|    total_timesteps      | 2433024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.08e+09  |\n",
      "|    n_updates            | 11870     |\n",
      "|    policy_gradient_loss | -2.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+10   |\n",
      "---------------------------------------\n",
      "Episode 974  finished with cumulative reward: -2804000.0 and \n",
      "with an average reward of: -1121.1515393842462\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2433473\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1121.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1189      |\n",
      "|    time_elapsed         | 32925     |\n",
      "|    total_timesteps      | 2435072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0409    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.82e+09  |\n",
      "|    n_updates            | 11880     |\n",
      "|    policy_gradient_loss | -2.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 975  finished with cumulative reward: -5048000.0 and \n",
      "with an average reward of: -2018.392642942823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2435974\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2019.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1190      |\n",
      "|    time_elapsed         | 32954     |\n",
      "|    total_timesteps      | 2437120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.57e+09  |\n",
      "|    n_updates            | 11890     |\n",
      "|    policy_gradient_loss | -8.26e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.24e+09  |\n",
      "---------------------------------------\n",
      "Episode 976  finished with cumulative reward: -8439500.0 and \n",
      "with an average reward of: -3374.450219912035\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2438475\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3375.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1191      |\n",
      "|    time_elapsed         | 32981     |\n",
      "|    total_timesteps      | 2439168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.9e+09   |\n",
      "|    n_updates            | 11900     |\n",
      "|    policy_gradient_loss | -2.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.49e+09  |\n",
      "---------------------------------------\n",
      "Episode 977  finished with cumulative reward: -2574500.0 and \n",
      "with an average reward of: -1029.3882447021192\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2440976\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1029.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1192      |\n",
      "|    time_elapsed         | 33009     |\n",
      "|    total_timesteps      | 2441216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1e+10     |\n",
      "|    n_updates            | 11910     |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.79e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1193      |\n",
      "|    time_elapsed         | 33030     |\n",
      "|    total_timesteps      | 2443264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0377    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.29e+09  |\n",
      "|    n_updates            | 11920     |\n",
      "|    policy_gradient_loss | -2.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.11e+09  |\n",
      "---------------------------------------\n",
      "Episode 978  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2443477\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1194      |\n",
      "|    time_elapsed         | 33058     |\n",
      "|    total_timesteps      | 2445312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.78e+09  |\n",
      "|    n_updates            | 11930     |\n",
      "|    policy_gradient_loss | -2.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1e+10     |\n",
      "---------------------------------------\n",
      "Episode 979  finished with cumulative reward: -5736500.0 and \n",
      "with an average reward of: -2293.6825269892042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2445978\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2294.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1195      |\n",
      "|    time_elapsed         | 33085     |\n",
      "|    total_timesteps      | 2447360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0246    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.16e+09  |\n",
      "|    n_updates            | 11940     |\n",
      "|    policy_gradient_loss | -2.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 980  finished with cumulative reward: -4385000.0 and \n",
      "with an average reward of: -1753.298680527789\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2448479\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1754.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1196      |\n",
      "|    time_elapsed         | 33113     |\n",
      "|    total_timesteps      | 2449408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.2e+09   |\n",
      "|    n_updates            | 11950     |\n",
      "|    policy_gradient_loss | -1.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 981  finished with cumulative reward: -8541500.0 and \n",
      "with an average reward of: -3415.233906437425\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2450980\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3416.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1197      |\n",
      "|    time_elapsed         | 33142     |\n",
      "|    total_timesteps      | 2451456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0251    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.62e+09  |\n",
      "|    n_updates            | 11960     |\n",
      "|    policy_gradient_loss | -2.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 982  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2453481\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1198      |\n",
      "|    time_elapsed         | 33169     |\n",
      "|    total_timesteps      | 2453504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.026     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.56e+10  |\n",
      "|    n_updates            | 11970     |\n",
      "|    policy_gradient_loss | -1.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.77e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1199      |\n",
      "|    time_elapsed         | 33191     |\n",
      "|    total_timesteps      | 2455552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0228    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.96e+09  |\n",
      "|    n_updates            | 11980     |\n",
      "|    policy_gradient_loss | -1.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 983  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2455982\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1200      |\n",
      "|    time_elapsed         | 33219     |\n",
      "|    total_timesteps      | 2457600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0315    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.94e+09  |\n",
      "|    n_updates            | 11990     |\n",
      "|    policy_gradient_loss | -3.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 984  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2458483\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1201      |\n",
      "|    time_elapsed         | 33248     |\n",
      "|    total_timesteps      | 2459648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0293    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.62e+09  |\n",
      "|    n_updates            | 12000     |\n",
      "|    policy_gradient_loss | -8.84e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.79e+09  |\n",
      "---------------------------------------\n",
      "Episode 985  finished with cumulative reward: -7572500.0 and \n",
      "with an average reward of: -3027.7888844462213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2460984\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3029.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.54e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1202      |\n",
      "|    time_elapsed         | 33277     |\n",
      "|    total_timesteps      | 2461696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.41e+09  |\n",
      "|    n_updates            | 12010     |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 986  finished with cumulative reward: -9357500.0 and \n",
      "with an average reward of: -3741.5033986405438\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2463485\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3743.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.6e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 73       |\n",
      "|    iterations           | 1203     |\n",
      "|    time_elapsed         | 33305    |\n",
      "|    total_timesteps      | 2463744  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0282   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 7.89e+09 |\n",
      "|    n_updates            | 12020    |\n",
      "|    policy_gradient_loss | -1.8e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.95e+10 |\n",
      "--------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1204      |\n",
      "|    time_elapsed         | 33326     |\n",
      "|    total_timesteps      | 2465792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0156    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.52e+10  |\n",
      "|    n_updates            | 12030     |\n",
      "|    policy_gradient_loss | -1.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.36e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 987  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2465986\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1205      |\n",
      "|    time_elapsed         | 33354     |\n",
      "|    total_timesteps      | 2467840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.54e+09  |\n",
      "|    n_updates            | 12040     |\n",
      "|    policy_gradient_loss | -2.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.6e+10   |\n",
      "---------------------------------------\n",
      "Episode 988  finished with cumulative reward: -2192000.0 and \n",
      "with an average reward of: -876.4494202319072\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2468487\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -876.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1206      |\n",
      "|    time_elapsed         | 33382     |\n",
      "|    total_timesteps      | 2469888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.73e+09  |\n",
      "|    n_updates            | 12050     |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.95e+09  |\n",
      "---------------------------------------\n",
      "Episode 989  finished with cumulative reward: -2472500.0 and \n",
      "with an average reward of: -988.6045581767293\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2470988\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -989.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1207      |\n",
      "|    time_elapsed         | 33411     |\n",
      "|    total_timesteps      | 2471936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0413    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.91e+08  |\n",
      "|    n_updates            | 12060     |\n",
      "|    policy_gradient_loss | -2.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+09  |\n",
      "---------------------------------------\n",
      "Episode 990  finished with cumulative reward: -9893000.0 and \n",
      "with an average reward of: -3955.6177528988405\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2473489\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3957.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1208      |\n",
      "|    time_elapsed         | 33440     |\n",
      "|    total_timesteps      | 2473984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.38e+10  |\n",
      "|    n_updates            | 12070     |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 991  finished with cumulative reward: -4461500.0 and \n",
      "with an average reward of: -1783.8864454218312\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2475990\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1784.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1209      |\n",
      "|    time_elapsed         | 33468     |\n",
      "|    total_timesteps      | 2476032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.24e+09  |\n",
      "|    n_updates            | 12080     |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1210      |\n",
      "|    time_elapsed         | 33488     |\n",
      "|    total_timesteps      | 2478080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.77e+09  |\n",
      "|    n_updates            | 12090     |\n",
      "|    policy_gradient_loss | -2.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 992  finished with cumulative reward: -4818500.0 and \n",
      "with an average reward of: -1926.6293482606957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2478491\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1927.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1211      |\n",
      "|    time_elapsed         | 33517     |\n",
      "|    total_timesteps      | 2480128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0321    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.77e+09  |\n",
      "|    n_updates            | 12100     |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 993  finished with cumulative reward: -6782000.0 and \n",
      "with an average reward of: -2711.71531387445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2480992\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2712.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1212      |\n",
      "|    time_elapsed         | 33544     |\n",
      "|    total_timesteps      | 2482176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0268    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.89e+09  |\n",
      "|    n_updates            | 12110     |\n",
      "|    policy_gradient_loss | -4.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 994  finished with cumulative reward: -2447000.0 and \n",
      "with an average reward of: -978.4086365453818\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2483493\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -978.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1213      |\n",
      "|    time_elapsed         | 33573     |\n",
      "|    total_timesteps      | 2484224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0348    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.36e+09  |\n",
      "|    n_updates            | 12120     |\n",
      "|    policy_gradient_loss | -1.5e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.47e+09  |\n",
      "---------------------------------------\n",
      "Episode 995  finished with cumulative reward: -2574500.0 and \n",
      "with an average reward of: -1029.3882447021192\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2485994\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1029.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 73        |\n",
      "|    iterations           | 1214      |\n",
      "|    time_elapsed         | 33602     |\n",
      "|    total_timesteps      | 2486272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0316    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.97e+09  |\n",
      "|    n_updates            | 12130     |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.1e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1215      |\n",
      "|    time_elapsed         | 33623     |\n",
      "|    total_timesteps      | 2488320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0341    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.06e+09  |\n",
      "|    n_updates            | 12140     |\n",
      "|    policy_gradient_loss | -3e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.92e+09  |\n",
      "---------------------------------------\n",
      "Episode 996  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2488495\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1216      |\n",
      "|    time_elapsed         | 33651     |\n",
      "|    total_timesteps      | 2490368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.04e+09  |\n",
      "|    n_updates            | 12150     |\n",
      "|    policy_gradient_loss | -1.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.36e+09  |\n",
      "---------------------------------------\n",
      "Episode 997  finished with cumulative reward: -3492500.0 and \n",
      "with an average reward of: -1396.4414234306278\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2490996\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1397.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1217      |\n",
      "|    time_elapsed         | 33679     |\n",
      "|    total_timesteps      | 2492416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0374    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.02e+09  |\n",
      "|    n_updates            | 12160     |\n",
      "|    policy_gradient_loss | -1.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.84e+09  |\n",
      "---------------------------------------\n",
      "Episode 998  finished with cumulative reward: -6246500.0 and \n",
      "with an average reward of: -2497.6009596161534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2493497\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2498.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1218      |\n",
      "|    time_elapsed         | 33707     |\n",
      "|    total_timesteps      | 2494464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0373    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.45e+09  |\n",
      "|    n_updates            | 12170     |\n",
      "|    policy_gradient_loss | -1.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.25e+09  |\n",
      "---------------------------------------\n",
      "Episode 999  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2495998\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1219      |\n",
      "|    time_elapsed         | 33735     |\n",
      "|    total_timesteps      | 2496512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0225    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.04e+10  |\n",
      "|    n_updates            | 12180     |\n",
      "|    policy_gradient_loss | -1.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.91e+10  |\n",
      "---------------------------------------\n",
      "Episode 1000  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2498499\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1220      |\n",
      "|    time_elapsed         | 33763     |\n",
      "|    total_timesteps      | 2498560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.29e+09  |\n",
      "|    n_updates            | 12190     |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.3e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1221      |\n",
      "|    time_elapsed         | 33784     |\n",
      "|    total_timesteps      | 2500608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0287    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.71e+09  |\n",
      "|    n_updates            | 12200     |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 1001  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2501000\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1222      |\n",
      "|    time_elapsed         | 33811     |\n",
      "|    total_timesteps      | 2502656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0375    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.92e+09  |\n",
      "|    n_updates            | 12210     |\n",
      "|    policy_gradient_loss | -2.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.44e+09  |\n",
      "---------------------------------------\n",
      "Episode 1002  finished with cumulative reward: -2549000.0 and \n",
      "with an average reward of: -1019.1923230707716\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2503501\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1019.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1223      |\n",
      "|    time_elapsed         | 33839     |\n",
      "|    total_timesteps      | 2504704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0235    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.3e+09   |\n",
      "|    n_updates            | 12220     |\n",
      "|    policy_gradient_loss | -2.96e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.25e+10  |\n",
      "---------------------------------------\n",
      "Episode 1003  finished with cumulative reward: -9995000.0 and \n",
      "with an average reward of: -3996.4014394242304\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2506002\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3998.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1224      |\n",
      "|    time_elapsed         | 33868     |\n",
      "|    total_timesteps      | 2506752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.21e+09  |\n",
      "|    n_updates            | 12230     |\n",
      "|    policy_gradient_loss | -3.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 1004  finished with cumulative reward: -3263000.0 and \n",
      "with an average reward of: -1304.6781287485005\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2508503\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1305.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1225      |\n",
      "|    time_elapsed         | 33896     |\n",
      "|    total_timesteps      | 2508800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0259    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.31e+10  |\n",
      "|    n_updates            | 12240     |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.9e+10   |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.41e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1226          |\n",
      "|    time_elapsed         | 33918         |\n",
      "|    total_timesteps      | 2510848       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0382        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.01e+08      |\n",
      "|    n_updates            | 12250         |\n",
      "|    policy_gradient_loss | -5.56e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.13e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1005  finished with cumulative reward: -6782000.0 and \n",
      "with an average reward of: -2711.71531387445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2511004\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2712.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1227      |\n",
      "|    time_elapsed         | 33947     |\n",
      "|    total_timesteps      | 2512896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.017     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.02e+09  |\n",
      "|    n_updates            | 12260     |\n",
      "|    policy_gradient_loss | -1.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n",
      "Episode 1006  finished with cumulative reward: -3900500.0 and \n",
      "with an average reward of: -1559.5761695321871\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2513505\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1560.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1228      |\n",
      "|    time_elapsed         | 33975     |\n",
      "|    total_timesteps      | 2514944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0305    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.66e+09  |\n",
      "|    n_updates            | 12270     |\n",
      "|    policy_gradient_loss | -3.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.92e+09  |\n",
      "---------------------------------------\n",
      "Episode 1007  finished with cumulative reward: -7445000.0 and \n",
      "with an average reward of: -2976.8092762894844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2516006\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2978.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1229      |\n",
      "|    time_elapsed         | 34002     |\n",
      "|    total_timesteps      | 2516992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0436    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.27e+09  |\n",
      "|    n_updates            | 12280     |\n",
      "|    policy_gradient_loss | -1.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.76e+09  |\n",
      "---------------------------------------\n",
      "Episode 1008  finished with cumulative reward: -3849500.0 and \n",
      "with an average reward of: -1539.1843262694922\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2518507\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1539.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1230      |\n",
      "|    time_elapsed         | 34030     |\n",
      "|    total_timesteps      | 2519040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0257    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.8e+09   |\n",
      "|    n_updates            | 12290     |\n",
      "|    policy_gradient_loss | -2.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1009  finished with cumulative reward: -6654500.0 and \n",
      "with an average reward of: -2660.735705717713\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2521008\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2661.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1231      |\n",
      "|    time_elapsed         | 34058     |\n",
      "|    total_timesteps      | 2521088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0292    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.11e+09  |\n",
      "|    n_updates            | 12300     |\n",
      "|    policy_gradient_loss | -2.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1232      |\n",
      "|    time_elapsed         | 34078     |\n",
      "|    total_timesteps      | 2523136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.31e+09  |\n",
      "|    n_updates            | 12310     |\n",
      "|    policy_gradient_loss | -2.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.42e+09  |\n",
      "---------------------------------------\n",
      "Episode 1010  finished with cumulative reward: -5762000.0 and \n",
      "with an average reward of: -2303.878448620552\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2523509\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2304.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1233      |\n",
      "|    time_elapsed         | 34106     |\n",
      "|    total_timesteps      | 2525184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.032     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.32e+09  |\n",
      "|    n_updates            | 12320     |\n",
      "|    policy_gradient_loss | -2.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 1011  finished with cumulative reward: -7394000.0 and \n",
      "with an average reward of: -2956.4174330267892\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2526010\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2957.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1234      |\n",
      "|    time_elapsed         | 34134     |\n",
      "|    total_timesteps      | 2527232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 12330     |\n",
      "|    policy_gradient_loss | -1.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.9e+10   |\n",
      "---------------------------------------\n",
      "Episode 1012  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2528511\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.42e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1235          |\n",
      "|    time_elapsed         | 34163         |\n",
      "|    total_timesteps      | 2529280       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0337        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.08e+08      |\n",
      "|    n_updates            | 12340         |\n",
      "|    policy_gradient_loss | -5.81e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.61e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1013  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2531012\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.38e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1236      |\n",
      "|    time_elapsed         | 34190     |\n",
      "|    total_timesteps      | 2531328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0247    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.12e+09  |\n",
      "|    n_updates            | 12350     |\n",
      "|    policy_gradient_loss | -1.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.3e+09   |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.38e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1237          |\n",
      "|    time_elapsed         | 34211         |\n",
      "|    total_timesteps      | 2533376       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2386895e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0298        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.54e+09      |\n",
      "|    n_updates            | 12360         |\n",
      "|    policy_gradient_loss | -8.43e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.34e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1014  finished with cumulative reward: -2268500.0 and \n",
      "with an average reward of: -907.0371851259496\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2533513\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -907.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.31e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1238          |\n",
      "|    time_elapsed         | 34239         |\n",
      "|    total_timesteps      | 2535424       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0308        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.84e+09      |\n",
      "|    n_updates            | 12370         |\n",
      "|    policy_gradient_loss | -7.65e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.52e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1015  finished with cumulative reward: -9230000.0 and \n",
      "with an average reward of: -3690.5237904838064\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2536014\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3692.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.38e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1239      |\n",
      "|    time_elapsed         | 34267     |\n",
      "|    total_timesteps      | 2537472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0232    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.4e+09   |\n",
      "|    n_updates            | 12380     |\n",
      "|    policy_gradient_loss | -8.67e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+10  |\n",
      "---------------------------------------\n",
      "Episode 1016  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2538515\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.37e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1240      |\n",
      "|    time_elapsed         | 34296     |\n",
      "|    total_timesteps      | 2539520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0259    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.86e+09  |\n",
      "|    n_updates            | 12390     |\n",
      "|    policy_gradient_loss | -1.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.64e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1017  finished with cumulative reward: -4614500.0 and \n",
      "with an average reward of: -1845.061975209916\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2541016\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1845.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1241      |\n",
      "|    time_elapsed         | 34324     |\n",
      "|    total_timesteps      | 2541568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0303    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.8e+09   |\n",
      "|    n_updates            | 12400     |\n",
      "|    policy_gradient_loss | -2.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 1018  finished with cumulative reward: -2447000.0 and \n",
      "with an average reward of: -978.4086365453818\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2543517\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -978.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.32e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1242         |\n",
      "|    time_elapsed         | 34352        |\n",
      "|    total_timesteps      | 2543616      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.029        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+09     |\n",
      "|    n_updates            | 12410        |\n",
      "|    policy_gradient_loss | -5.56e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.54e+09     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.32e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1243         |\n",
      "|    time_elapsed         | 34373        |\n",
      "|    total_timesteps      | 2545664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0193       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.05e+09     |\n",
      "|    n_updates            | 12420        |\n",
      "|    policy_gradient_loss | -4.1e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.59e+09     |\n",
      "------------------------------------------\n",
      "Episode 1019  finished with cumulative reward: -3875000.0 and \n",
      "with an average reward of: -1549.3802479008398\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2546018\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1550.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1244      |\n",
      "|    time_elapsed         | 34401     |\n",
      "|    total_timesteps      | 2547712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0248    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.63e+09  |\n",
      "|    n_updates            | 12430     |\n",
      "|    policy_gradient_loss | -1.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.41e+09  |\n",
      "---------------------------------------\n",
      "Episode 1020  finished with cumulative reward: -4946000.0 and \n",
      "with an average reward of: -1977.608956417433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2548519\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1978.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1245      |\n",
      "|    time_elapsed         | 34429     |\n",
      "|    total_timesteps      | 2549760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.41e+09  |\n",
      "|    n_updates            | 12440     |\n",
      "|    policy_gradient_loss | -2.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.07e+10  |\n",
      "---------------------------------------\n",
      "Episode 1021  finished with cumulative reward: -7521500.0 and \n",
      "with an average reward of: -3007.3970411835267\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2551020\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3008.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.3e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1246      |\n",
      "|    time_elapsed         | 34457     |\n",
      "|    total_timesteps      | 2551808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0149    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.39e+09  |\n",
      "|    n_updates            | 12450     |\n",
      "|    policy_gradient_loss | -1.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.58e+10  |\n",
      "---------------------------------------\n",
      "Episode 1022  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2553521\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.26e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1247      |\n",
      "|    time_elapsed         | 34486     |\n",
      "|    total_timesteps      | 2553856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0358    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.32e+09  |\n",
      "|    n_updates            | 12460     |\n",
      "|    policy_gradient_loss | -1.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.18e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.26e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1248      |\n",
      "|    time_elapsed         | 34506     |\n",
      "|    total_timesteps      | 2555904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.7e+09   |\n",
      "|    n_updates            | 12470     |\n",
      "|    policy_gradient_loss | -1.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.29e+10  |\n",
      "---------------------------------------\n",
      "Episode 1023  finished with cumulative reward: -5226500.0 and \n",
      "with an average reward of: -2089.764094362255\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2556022\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2090.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1249      |\n",
      "|    time_elapsed         | 34535     |\n",
      "|    total_timesteps      | 2557952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0377    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.34e+09  |\n",
      "|    n_updates            | 12480     |\n",
      "|    policy_gradient_loss | -3.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.03e+09  |\n",
      "---------------------------------------\n",
      "Episode 1024  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2558523\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1250      |\n",
      "|    time_elapsed         | 34563     |\n",
      "|    total_timesteps      | 2560000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.036     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.9e+09   |\n",
      "|    n_updates            | 12490     |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 1025  finished with cumulative reward: -6552500.0 and \n",
      "with an average reward of: -2619.952019192323\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2561024\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2621.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1251      |\n",
      "|    time_elapsed         | 34591     |\n",
      "|    total_timesteps      | 2562048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.21e+09  |\n",
      "|    n_updates            | 12500     |\n",
      "|    policy_gradient_loss | -1.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.15e+09  |\n",
      "---------------------------------------\n",
      "Episode 1026  finished with cumulative reward: -10071500.0 and \n",
      "with an average reward of: -4026.9892043182726\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2563525\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4028.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1252      |\n",
      "|    time_elapsed         | 34619     |\n",
      "|    total_timesteps      | 2564096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0274    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.04e+10  |\n",
      "|    n_updates            | 12510     |\n",
      "|    policy_gradient_loss | -9.18e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n",
      "Episode 1027  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2566026\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1253      |\n",
      "|    time_elapsed         | 34648     |\n",
      "|    total_timesteps      | 2566144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.68e+09  |\n",
      "|    n_updates            | 12520     |\n",
      "|    policy_gradient_loss | -9.07e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.8e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1254      |\n",
      "|    time_elapsed         | 34668     |\n",
      "|    total_timesteps      | 2568192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0336    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.17e+09  |\n",
      "|    n_updates            | 12530     |\n",
      "|    policy_gradient_loss | -2e-06    |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.35e+09  |\n",
      "---------------------------------------\n",
      "Episode 1028  finished with cumulative reward: -6450500.0 and \n",
      "with an average reward of: -2579.168332666933\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2568527\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2580.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1255      |\n",
      "|    time_elapsed         | 34696     |\n",
      "|    total_timesteps      | 2570240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0239    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.49e+09  |\n",
      "|    n_updates            | 12540     |\n",
      "|    policy_gradient_loss | -4.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1029  finished with cumulative reward: -3722000.0 and \n",
      "with an average reward of: -1488.2047181127548\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2571028\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1488.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1256      |\n",
      "|    time_elapsed         | 34725     |\n",
      "|    total_timesteps      | 2572288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0261    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.16e+09  |\n",
      "|    n_updates            | 12550     |\n",
      "|    policy_gradient_loss | -9.69e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.8e+09   |\n",
      "---------------------------------------\n",
      "Episode 1030  finished with cumulative reward: -6348500.0 and \n",
      "with an average reward of: -2538.3846461415433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2573529\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2539.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1257      |\n",
      "|    time_elapsed         | 34753     |\n",
      "|    total_timesteps      | 2574336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0233    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.46e+09  |\n",
      "|    n_updates            | 12560     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n",
      "Episode 1031  finished with cumulative reward: -9230000.0 and \n",
      "with an average reward of: -3690.5237904838064\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2576030\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3692.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1258      |\n",
      "|    time_elapsed         | 34782     |\n",
      "|    total_timesteps      | 2576384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0344    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.92e+08  |\n",
      "|    n_updates            | 12570     |\n",
      "|    policy_gradient_loss | -3.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.92e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1259      |\n",
      "|    time_elapsed         | 34803     |\n",
      "|    total_timesteps      | 2578432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0252    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.09e+10  |\n",
      "|    n_updates            | 12580     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.94e+10  |\n",
      "---------------------------------------\n",
      "Episode 1032  finished with cumulative reward: -6552500.0 and \n",
      "with an average reward of: -2619.952019192323\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2578531\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2621.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1260      |\n",
      "|    time_elapsed         | 34831     |\n",
      "|    total_timesteps      | 2580480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0223    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.7e+09   |\n",
      "|    n_updates            | 12590     |\n",
      "|    policy_gradient_loss | -1.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 1033  finished with cumulative reward: -4946000.0 and \n",
      "with an average reward of: -1977.608956417433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2581032\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1978.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1261      |\n",
      "|    time_elapsed         | 34859     |\n",
      "|    total_timesteps      | 2582528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0252    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.43e+09  |\n",
      "|    n_updates            | 12600     |\n",
      "|    policy_gradient_loss | -1.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 1034  finished with cumulative reward: -840500.0 and \n",
      "with an average reward of: -336.0655737704918\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2583533\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -336.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.18e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1262          |\n",
      "|    time_elapsed         | 34888         |\n",
      "|    total_timesteps      | 2584576       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0552        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.9e+08       |\n",
      "|    n_updates            | 12610         |\n",
      "|    policy_gradient_loss | -7.13e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.14e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1035  finished with cumulative reward: -8720000.0 and \n",
      "with an average reward of: -3486.605357856857\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2586034\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3488.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1263      |\n",
      "|    time_elapsed         | 34915     |\n",
      "|    total_timesteps      | 2586624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.82e+09  |\n",
      "|    n_updates            | 12620     |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.54e+10  |\n",
      "---------------------------------------\n",
      "Episode 1036  finished with cumulative reward: -4359500.0 and \n",
      "with an average reward of: -1743.1027588964414\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2588535\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1743.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1264      |\n",
      "|    time_elapsed         | 34943     |\n",
      "|    total_timesteps      | 2588672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0266    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.82e+09  |\n",
      "|    n_updates            | 12630     |\n",
      "|    policy_gradient_loss | -1.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1265      |\n",
      "|    time_elapsed         | 34964     |\n",
      "|    total_timesteps      | 2590720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.035     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.5e+09   |\n",
      "|    n_updates            | 12640     |\n",
      "|    policy_gradient_loss | -2.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.49e+09  |\n",
      "---------------------------------------\n",
      "Episode 1037  finished with cumulative reward: -6935000.0 and \n",
      "with an average reward of: -2772.890843662535\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2591036\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2774.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.21e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1266          |\n",
      "|    time_elapsed         | 34992         |\n",
      "|    total_timesteps      | 2592768       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0273        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.08e+10      |\n",
      "|    n_updates            | 12650         |\n",
      "|    policy_gradient_loss | -6.67e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.88e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1038  finished with cumulative reward: -8694500.0 and \n",
      "with an average reward of: -3476.4094362255096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2593537\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3477.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1267      |\n",
      "|    time_elapsed         | 35020     |\n",
      "|    total_timesteps      | 2594816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0312    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.41e+09  |\n",
      "|    n_updates            | 12660     |\n",
      "|    policy_gradient_loss | -4.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 1039  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2596038\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.28e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1268      |\n",
      "|    time_elapsed         | 35048     |\n",
      "|    total_timesteps      | 2596864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0261    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.31e+10  |\n",
      "|    n_updates            | 12670     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 1040  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2598539\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.27e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1269          |\n",
      "|    time_elapsed         | 35077         |\n",
      "|    total_timesteps      | 2598912       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0296        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.72e+09      |\n",
      "|    n_updates            | 12680         |\n",
      "|    policy_gradient_loss | -2.83e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.99e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1270      |\n",
      "|    time_elapsed         | 35098     |\n",
      "|    total_timesteps      | 2600960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0283    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.36e+10  |\n",
      "|    n_updates            | 12690     |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 1041  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2601040\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1271      |\n",
      "|    time_elapsed         | 35127     |\n",
      "|    total_timesteps      | 2603008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.028     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.18e+09  |\n",
      "|    n_updates            | 12700     |\n",
      "|    policy_gradient_loss | -9.7e-07  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1e+10     |\n",
      "---------------------------------------\n",
      "Episode 1042  finished with cumulative reward: -8082500.0 and \n",
      "with an average reward of: -3231.7073170731705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2603541\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3233.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1272      |\n",
      "|    time_elapsed         | 35155     |\n",
      "|    total_timesteps      | 2605056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.7e+09   |\n",
      "|    n_updates            | 12710     |\n",
      "|    policy_gradient_loss | -1.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 1043  finished with cumulative reward: -8286500.0 and \n",
      "with an average reward of: -3313.27469012395\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2606042\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3314.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.31e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1273      |\n",
      "|    time_elapsed         | 35183     |\n",
      "|    total_timesteps      | 2607104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0248    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.25e+09  |\n",
      "|    n_updates            | 12720     |\n",
      "|    policy_gradient_loss | -7.97e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 1044  finished with cumulative reward: -1248500.0 and \n",
      "with an average reward of: -499.2003198720512\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2608543\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -499.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.26e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1274      |\n",
      "|    time_elapsed         | 35211     |\n",
      "|    total_timesteps      | 2609152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0363    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.87e+09  |\n",
      "|    n_updates            | 12730     |\n",
      "|    policy_gradient_loss | -3.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.16e+09  |\n",
      "---------------------------------------\n",
      "Episode 1045  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2611044\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1275      |\n",
      "|    time_elapsed         | 35239     |\n",
      "|    total_timesteps      | 2611200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0304    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.82e+09  |\n",
      "|    n_updates            | 12740     |\n",
      "|    policy_gradient_loss | -1.96e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1276      |\n",
      "|    time_elapsed         | 35260     |\n",
      "|    total_timesteps      | 2613248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0298    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.06e+09  |\n",
      "|    n_updates            | 12750     |\n",
      "|    policy_gradient_loss | -1.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.75e+09  |\n",
      "---------------------------------------\n",
      "Episode 1046  finished with cumulative reward: -3645500.0 and \n",
      "with an average reward of: -1457.6169532187125\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2613545\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1458.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.28e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1277      |\n",
      "|    time_elapsed         | 35288     |\n",
      "|    total_timesteps      | 2615296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0291    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.85e+09  |\n",
      "|    n_updates            | 12760     |\n",
      "|    policy_gradient_loss | -1.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.72e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1047  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2616046\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1278      |\n",
      "|    time_elapsed         | 35316     |\n",
      "|    total_timesteps      | 2617344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0306    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.52e+09  |\n",
      "|    n_updates            | 12770     |\n",
      "|    policy_gradient_loss | -2.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.51e+09  |\n",
      "---------------------------------------\n",
      "Episode 1048  finished with cumulative reward: -9459500.0 and \n",
      "with an average reward of: -3782.2870851659336\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2618547\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3783.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1279      |\n",
      "|    time_elapsed         | 35345     |\n",
      "|    total_timesteps      | 2619392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0372    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.14e+09  |\n",
      "|    n_updates            | 12780     |\n",
      "|    policy_gradient_loss | -2.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 1049  finished with cumulative reward: -8465000.0 and \n",
      "with an average reward of: -3384.646141543383\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2621048\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3386.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.37e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1280      |\n",
      "|    time_elapsed         | 35373     |\n",
      "|    total_timesteps      | 2621440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0259    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.35e+09  |\n",
      "|    n_updates            | 12790     |\n",
      "|    policy_gradient_loss | -1.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.37e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1281      |\n",
      "|    time_elapsed         | 35394     |\n",
      "|    total_timesteps      | 2623488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0279    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.27e+10  |\n",
      "|    n_updates            | 12800     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 1050  finished with cumulative reward: -9893000.0 and \n",
      "with an average reward of: -3955.6177528988405\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2623549\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3957.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1282      |\n",
      "|    time_elapsed         | 35422     |\n",
      "|    total_timesteps      | 2625536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0236    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.21e+10  |\n",
      "|    n_updates            | 12810     |\n",
      "|    policy_gradient_loss | -4.4e-07  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.33e+10  |\n",
      "---------------------------------------\n",
      "Episode 1051  finished with cumulative reward: -7623500.0 and \n",
      "with an average reward of: -3048.1807277089165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2626050\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3049.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1283      |\n",
      "|    time_elapsed         | 35450     |\n",
      "|    total_timesteps      | 2627584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0222    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.44e+09  |\n",
      "|    n_updates            | 12820     |\n",
      "|    policy_gradient_loss | -5.85e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.83e+10  |\n",
      "---------------------------------------\n",
      "Episode 1052  finished with cumulative reward: -4844000.0 and \n",
      "with an average reward of: -1936.8252698920433\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2628551\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1937.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.4e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1284          |\n",
      "|    time_elapsed         | 35478         |\n",
      "|    total_timesteps      | 2629632       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.024         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.25e+09      |\n",
      "|    n_updates            | 12830         |\n",
      "|    policy_gradient_loss | -4.16e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.18e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1053  finished with cumulative reward: -5150000.0 and \n",
      "with an average reward of: -2059.1763294682128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2631052\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2060.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1285      |\n",
      "|    time_elapsed         | 35507     |\n",
      "|    total_timesteps      | 2631680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0282    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.67e+09  |\n",
      "|    n_updates            | 12840     |\n",
      "|    policy_gradient_loss | -1.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.33e+09  |\n",
      "---------------------------------------\n",
      "Episode 1054  finished with cumulative reward: -6272000.0 and \n",
      "with an average reward of: -2507.796881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2633553\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2508.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1286      |\n",
      "|    time_elapsed         | 35535     |\n",
      "|    total_timesteps      | 2633728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.1e+09   |\n",
      "|    n_updates            | 12850     |\n",
      "|    policy_gradient_loss | -2.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1287      |\n",
      "|    time_elapsed         | 35556     |\n",
      "|    total_timesteps      | 2635776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.13e+09  |\n",
      "|    n_updates            | 12860     |\n",
      "|    policy_gradient_loss | -2.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 1055  finished with cumulative reward: -11142500.0 and \n",
      "with an average reward of: -4455.217912834866\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2636054\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4457.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1288      |\n",
      "|    time_elapsed         | 35584     |\n",
      "|    total_timesteps      | 2637824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0224    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.98e+10  |\n",
      "|    n_updates            | 12870     |\n",
      "|    policy_gradient_loss | -9.59e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 1056  finished with cumulative reward: -11142500.0 and \n",
      "with an average reward of: -4455.217912834866\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2638555\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4457.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1289      |\n",
      "|    time_elapsed         | 35613     |\n",
      "|    total_timesteps      | 2639872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0247    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+10  |\n",
      "|    n_updates            | 12880     |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.59e+10  |\n",
      "---------------------------------------\n",
      "Episode 1057  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2641056\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1290      |\n",
      "|    time_elapsed         | 35641     |\n",
      "|    total_timesteps      | 2641920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.82e+09  |\n",
      "|    n_updates            | 12890     |\n",
      "|    policy_gradient_loss | -1.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.98e+10  |\n",
      "---------------------------------------\n",
      "Episode 1058  finished with cumulative reward: -7496000.0 and \n",
      "with an average reward of: -2997.201119552179\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2643557\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2998.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1291      |\n",
      "|    time_elapsed         | 35669     |\n",
      "|    total_timesteps      | 2643968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0207    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.93e+09  |\n",
      "|    n_updates            | 12900     |\n",
      "|    policy_gradient_loss | -7.03e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.51e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1292      |\n",
      "|    time_elapsed         | 35690     |\n",
      "|    total_timesteps      | 2646016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.47e+09  |\n",
      "|    n_updates            | 12910     |\n",
      "|    policy_gradient_loss | -9.91e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.16e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1059  finished with cumulative reward: -7623500.0 and \n",
      "with an average reward of: -3048.1807277089165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2646058\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3049.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1293      |\n",
      "|    time_elapsed         | 35718     |\n",
      "|    total_timesteps      | 2648064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0213    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.6e+09   |\n",
      "|    n_updates            | 12920     |\n",
      "|    policy_gradient_loss | -2.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.84e+09  |\n",
      "---------------------------------------\n",
      "Episode 1060  finished with cumulative reward: -3900500.0 and \n",
      "with an average reward of: -1559.5761695321871\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2648559\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1560.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1294          |\n",
      "|    time_elapsed         | 35747         |\n",
      "|    total_timesteps      | 2650112       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0334        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.25e+09      |\n",
      "|    n_updates            | 12930         |\n",
      "|    policy_gradient_loss | -5.17e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.49e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1061  finished with cumulative reward: -5532500.0 and \n",
      "with an average reward of: -2212.1151539384246\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2651060\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2213.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1295      |\n",
      "|    time_elapsed         | 35775     |\n",
      "|    total_timesteps      | 2652160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0251    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.39e+09  |\n",
      "|    n_updates            | 12940     |\n",
      "|    policy_gradient_loss | -1.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 1062  finished with cumulative reward: -4691000.0 and \n",
      "with an average reward of: -1875.6497401039585\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2653561\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1876.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1296      |\n",
      "|    time_elapsed         | 35803     |\n",
      "|    total_timesteps      | 2654208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0349    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.19e+09  |\n",
      "|    n_updates            | 12950     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.99e+09  |\n",
      "---------------------------------------\n",
      "Episode 1063  finished with cumulative reward: -8082500.0 and \n",
      "with an average reward of: -3231.7073170731705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2656062\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3233.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1297      |\n",
      "|    time_elapsed         | 35831     |\n",
      "|    total_timesteps      | 2656256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0274    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.53e+09  |\n",
      "|    n_updates            | 12960     |\n",
      "|    policy_gradient_loss | -1.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1298      |\n",
      "|    time_elapsed         | 35853     |\n",
      "|    total_timesteps      | 2658304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.92e+09  |\n",
      "|    n_updates            | 12970     |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 1064  finished with cumulative reward: -9842000.0 and \n",
      "with an average reward of: -3935.2259096361454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2658563\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3936.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.74e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1299          |\n",
      "|    time_elapsed         | 35881         |\n",
      "|    total_timesteps      | 2660352       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0768417e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0272        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.01e+10      |\n",
      "|    n_updates            | 12980         |\n",
      "|    policy_gradient_loss | -7.46e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.04e+10      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1065  finished with cumulative reward: -7700000.0 and \n",
      "with an average reward of: -3078.7684926029588\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2661064\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3080.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.79e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1300      |\n",
      "|    time_elapsed         | 35909     |\n",
      "|    total_timesteps      | 2662400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0318    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.13e+10  |\n",
      "|    n_updates            | 12990     |\n",
      "|    policy_gradient_loss | -2.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.97e+10  |\n",
      "---------------------------------------\n",
      "Episode 1066  finished with cumulative reward: -6450500.0 and \n",
      "with an average reward of: -2579.168332666933\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2663565\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2580.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.81e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1301      |\n",
      "|    time_elapsed         | 35938     |\n",
      "|    total_timesteps      | 2664448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.93e+09  |\n",
      "|    n_updates            | 13000     |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.06e+10  |\n",
      "---------------------------------------\n",
      "Episode 1067  finished with cumulative reward: -2268500.0 and \n",
      "with an average reward of: -907.0371851259496\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2666066\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -907.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1302      |\n",
      "|    time_elapsed         | 35966     |\n",
      "|    total_timesteps      | 2666496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0268    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.93e+09  |\n",
      "|    n_updates            | 13010     |\n",
      "|    policy_gradient_loss | -1.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.23e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1303      |\n",
      "|    time_elapsed         | 35987     |\n",
      "|    total_timesteps      | 2668544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0339    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.08e+09  |\n",
      "|    n_updates            | 13020     |\n",
      "|    policy_gradient_loss | -2.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.19e+09  |\n",
      "---------------------------------------\n",
      "Episode 1068  finished with cumulative reward: -5532500.0 and \n",
      "with an average reward of: -2212.1151539384246\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2668567\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2213.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.76e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1304      |\n",
      "|    time_elapsed         | 36015     |\n",
      "|    total_timesteps      | 2670592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.96e+09  |\n",
      "|    n_updates            | 13030     |\n",
      "|    policy_gradient_loss | -2.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 1069  finished with cumulative reward: -1656500.0 and \n",
      "with an average reward of: -662.3350659736105\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2671068\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -662.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.71e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1305         |\n",
      "|    time_elapsed         | 36043        |\n",
      "|    total_timesteps      | 2672640      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0405       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.01e+09     |\n",
      "|    n_updates            | 13040        |\n",
      "|    policy_gradient_loss | -4.46e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.15e+09     |\n",
      "------------------------------------------\n",
      "Episode 1070  finished with cumulative reward: -5328500.0 and \n",
      "with an average reward of: -2130.547780887645\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2673569\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2131.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1306      |\n",
      "|    time_elapsed         | 36071     |\n",
      "|    total_timesteps      | 2674688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.034     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.13e+09  |\n",
      "|    n_updates            | 13050     |\n",
      "|    policy_gradient_loss | -2.31e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.75e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1071  finished with cumulative reward: -8516000.0 and \n",
      "with an average reward of: -3405.0379848060775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2676070\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3406.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.81e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1307      |\n",
      "|    time_elapsed         | 36099     |\n",
      "|    total_timesteps      | 2676736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0246    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.13e+10  |\n",
      "|    n_updates            | 13060     |\n",
      "|    policy_gradient_loss | -2.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 1072  finished with cumulative reward: -6858500.0 and \n",
      "with an average reward of: -2742.3030787684925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2678571\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2743.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1308      |\n",
      "|    time_elapsed         | 36127     |\n",
      "|    total_timesteps      | 2678784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.85e+09  |\n",
      "|    n_updates            | 13070     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1309      |\n",
      "|    time_elapsed         | 36148     |\n",
      "|    total_timesteps      | 2680832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.92e+09  |\n",
      "|    n_updates            | 13080     |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 1073  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2681072\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.82e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1310      |\n",
      "|    time_elapsed         | 36176     |\n",
      "|    total_timesteps      | 2682880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 13090     |\n",
      "|    policy_gradient_loss | -2.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.9e+10   |\n",
      "---------------------------------------\n",
      "Episode 1074  finished with cumulative reward: -11270000.0 and \n",
      "with an average reward of: -4506.197520991604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2683573\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4508.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1311      |\n",
      "|    time_elapsed         | 36204     |\n",
      "|    total_timesteps      | 2684928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0291    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.64e+09  |\n",
      "|    n_updates            | 13100     |\n",
      "|    policy_gradient_loss | -2.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.96e+10  |\n",
      "---------------------------------------\n",
      "Episode 1075  finished with cumulative reward: -9332000.0 and \n",
      "with an average reward of: -3731.307477009196\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2686074\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3732.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.94e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1312         |\n",
      "|    time_elapsed         | 36232        |\n",
      "|    total_timesteps      | 2686976      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.240995e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.029        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.21e+09     |\n",
      "|    n_updates            | 13110        |\n",
      "|    policy_gradient_loss | -1.29e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.03e+10     |\n",
      "------------------------------------------\n",
      "Episode 1076  finished with cumulative reward: -8133500.0 and \n",
      "with an average reward of: -3252.0991603358657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2688575\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3253.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1313      |\n",
      "|    time_elapsed         | 36260     |\n",
      "|    total_timesteps      | 2689024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28e+10  |\n",
      "|    n_updates            | 13120     |\n",
      "|    policy_gradient_loss | -1.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.16e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1314      |\n",
      "|    time_elapsed         | 36281     |\n",
      "|    total_timesteps      | 2691072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0228    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+10  |\n",
      "|    n_updates            | 13130     |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.49e+10  |\n",
      "---------------------------------------\n",
      "Episode 1077  finished with cumulative reward: -9230000.0 and \n",
      "with an average reward of: -3690.5237904838064\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2691076\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3692.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1315      |\n",
      "|    time_elapsed         | 36310     |\n",
      "|    total_timesteps      | 2693120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.026     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.54e+09  |\n",
      "|    n_updates            | 13140     |\n",
      "|    policy_gradient_loss | -1.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.21e+10  |\n",
      "---------------------------------------\n",
      "Episode 1078  finished with cumulative reward: -4512500.0 and \n",
      "with an average reward of: -1804.2782886845262\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2693577\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1805.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1316      |\n",
      "|    time_elapsed         | 36338     |\n",
      "|    total_timesteps      | 2695168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0294    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.69e+09  |\n",
      "|    n_updates            | 13150     |\n",
      "|    policy_gradient_loss | -1.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 1079  finished with cumulative reward: -4895000.0 and \n",
      "with an average reward of: -1957.2171131547382\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2696078\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1958.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6e+06       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1317         |\n",
      "|    time_elapsed         | 36366        |\n",
      "|    total_timesteps      | 2697216      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0514       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.93e+09     |\n",
      "|    n_updates            | 13160        |\n",
      "|    policy_gradient_loss | -3.34e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.41e+09     |\n",
      "------------------------------------------\n",
      "Episode 1080  finished with cumulative reward: -4742000.0 and \n",
      "with an average reward of: -1896.0415833666534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2698579\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1896.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1318      |\n",
      "|    time_elapsed         | 36394     |\n",
      "|    total_timesteps      | 2699264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0331    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.02e+09  |\n",
      "|    n_updates            | 13170     |\n",
      "|    policy_gradient_loss | -3.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.08e+10  |\n",
      "---------------------------------------\n",
      "Episode 1081  finished with cumulative reward: -10046000.0 and \n",
      "with an average reward of: -4016.793282686925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2701080\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4018.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1319      |\n",
      "|    time_elapsed         | 36423     |\n",
      "|    total_timesteps      | 2701312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0224    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.54e+10  |\n",
      "|    n_updates            | 13180     |\n",
      "|    policy_gradient_loss | -1.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.53e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1320      |\n",
      "|    time_elapsed         | 36443     |\n",
      "|    total_timesteps      | 2703360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.87e+09  |\n",
      "|    n_updates            | 13190     |\n",
      "|    policy_gradient_loss | -2.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.02e+09  |\n",
      "---------------------------------------\n",
      "Episode 1082  finished with cumulative reward: -2345000.0 and \n",
      "with an average reward of: -937.624950019992\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2703581\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -938.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1321      |\n",
      "|    time_elapsed         | 36471     |\n",
      "|    total_timesteps      | 2705408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.01e+08  |\n",
      "|    n_updates            | 13200     |\n",
      "|    policy_gradient_loss | -2.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.22e+09  |\n",
      "---------------------------------------\n",
      "Episode 1083  finished with cumulative reward: -5660000.0 and \n",
      "with an average reward of: -2263.094762095162\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2706082\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2264.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1322      |\n",
      "|    time_elapsed         | 36499     |\n",
      "|    total_timesteps      | 2707456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.027     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.02e+09  |\n",
      "|    n_updates            | 13210     |\n",
      "|    policy_gradient_loss | -1.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 1084  finished with cumulative reward: -4895000.0 and \n",
      "with an average reward of: -1957.2171131547382\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2708583\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1958.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1323      |\n",
      "|    time_elapsed         | 36527     |\n",
      "|    total_timesteps      | 2709504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0209    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.63e+09  |\n",
      "|    n_updates            | 13220     |\n",
      "|    policy_gradient_loss | -1.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 1085  finished with cumulative reward: -7878500.0 and \n",
      "with an average reward of: -3150.139944022391\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2711084\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3151.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1324      |\n",
      "|    time_elapsed         | 36555     |\n",
      "|    total_timesteps      | 2711552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0292    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3e+09     |\n",
      "|    n_updates            | 13230     |\n",
      "|    policy_gradient_loss | -2.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.16e+09  |\n",
      "---------------------------------------\n",
      "Episode 1086  finished with cumulative reward: -2447000.0 and \n",
      "with an average reward of: -978.4086365453818\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2713585\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -978.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.94e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1325         |\n",
      "|    time_elapsed         | 36583        |\n",
      "|    total_timesteps      | 2713600      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0287       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.4e+09      |\n",
      "|    n_updates            | 13240        |\n",
      "|    policy_gradient_loss | -3.73e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.48e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1326      |\n",
      "|    time_elapsed         | 36604     |\n",
      "|    total_timesteps      | 2715648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0331    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.3e+09   |\n",
      "|    n_updates            | 13250     |\n",
      "|    policy_gradient_loss | -1.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 1087  finished with cumulative reward: -2727500.0 and \n",
      "with an average reward of: -1090.563774490204\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2716086\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1091.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1327      |\n",
      "|    time_elapsed         | 36634     |\n",
      "|    total_timesteps      | 2717696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0376    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.72e+09  |\n",
      "|    n_updates            | 13260     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.58e+09  |\n",
      "---------------------------------------\n",
      "Episode 1088  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2718587\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1328      |\n",
      "|    time_elapsed         | 36662     |\n",
      "|    total_timesteps      | 2719744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.14e+09  |\n",
      "|    n_updates            | 13270     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.23e+09  |\n",
      "---------------------------------------\n",
      "Episode 1089  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2721088\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1329      |\n",
      "|    time_elapsed         | 36691     |\n",
      "|    total_timesteps      | 2721792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0298    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.8e+09   |\n",
      "|    n_updates            | 13280     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 1090  finished with cumulative reward: -10199000.0 and \n",
      "with an average reward of: -4077.96881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2723589\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4079.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1330      |\n",
      "|    time_elapsed         | 36718     |\n",
      "|    total_timesteps      | 2723840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0371    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.64e+09  |\n",
      "|    n_updates            | 13290     |\n",
      "|    policy_gradient_loss | -1.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.13e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1331      |\n",
      "|    time_elapsed         | 36739     |\n",
      "|    total_timesteps      | 2725888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.18e+09  |\n",
      "|    n_updates            | 13300     |\n",
      "|    policy_gradient_loss | -3.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.99e+10  |\n",
      "---------------------------------------\n",
      "Episode 1091  finished with cumulative reward: -8643500.0 and \n",
      "with an average reward of: -3456.017592962815\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2726090\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3457.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1332      |\n",
      "|    time_elapsed         | 36768     |\n",
      "|    total_timesteps      | 2727936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0252    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e+10  |\n",
      "|    n_updates            | 13310     |\n",
      "|    policy_gradient_loss | -2.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.45e+10  |\n",
      "---------------------------------------\n",
      "Episode 1092  finished with cumulative reward: -11295500.0 and \n",
      "with an average reward of: -4516.393442622951\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2728591\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4518.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1333      |\n",
      "|    time_elapsed         | 36796     |\n",
      "|    total_timesteps      | 2729984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0254    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.81e+10  |\n",
      "|    n_updates            | 13320     |\n",
      "|    policy_gradient_loss | -8.81e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 1093  finished with cumulative reward: -7751000.0 and \n",
      "with an average reward of: -3099.160335865654\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2731092\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3100.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1334      |\n",
      "|    time_elapsed         | 36824     |\n",
      "|    total_timesteps      | 2732032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0268    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 13330     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.89e+10  |\n",
      "---------------------------------------\n",
      "Episode 1094  finished with cumulative reward: -13182500.0 and \n",
      "with an average reward of: -5270.891643342663\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2733593\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5273.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1335      |\n",
      "|    time_elapsed         | 36852     |\n",
      "|    total_timesteps      | 2734080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0283    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.62e+09  |\n",
      "|    n_updates            | 13340     |\n",
      "|    policy_gradient_loss | -2.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1095  finished with cumulative reward: -4538000.0 and \n",
      "with an average reward of: -1814.4742103158737\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2736094\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1815.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1336      |\n",
      "|    time_elapsed         | 36881     |\n",
      "|    total_timesteps      | 2736128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0287    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.71e+10  |\n",
      "|    n_updates            | 13350     |\n",
      "|    policy_gradient_loss | -1.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.21e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1337      |\n",
      "|    time_elapsed         | 36902     |\n",
      "|    total_timesteps      | 2738176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0277    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.81e+09  |\n",
      "|    n_updates            | 13360     |\n",
      "|    policy_gradient_loss | -3.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 1096  finished with cumulative reward: -5022500.0 and \n",
      "with an average reward of: -2008.1967213114754\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2738595\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2009.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1338      |\n",
      "|    time_elapsed         | 36929     |\n",
      "|    total_timesteps      | 2740224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.98e+09  |\n",
      "|    n_updates            | 13370     |\n",
      "|    policy_gradient_loss | -1.99e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 1097  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2741096\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.2e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1339          |\n",
      "|    time_elapsed         | 36958         |\n",
      "|    total_timesteps      | 2742272       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.039         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.09e+09      |\n",
      "|    n_updates            | 13380         |\n",
      "|    policy_gradient_loss | -8.57e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.03e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1098  finished with cumulative reward: -3875000.0 and \n",
      "with an average reward of: -1549.3802479008398\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2743597\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1550.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1340      |\n",
      "|    time_elapsed         | 36986     |\n",
      "|    total_timesteps      | 2744320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0244    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.3e+09   |\n",
      "|    n_updates            | 13390     |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1099  finished with cumulative reward: -9255500.0 and \n",
      "with an average reward of: -3700.719712115154\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2746098\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3702.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.21e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1341         |\n",
      "|    time_elapsed         | 37015        |\n",
      "|    total_timesteps      | 2746368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0401       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.29e+09     |\n",
      "|    n_updates            | 13400        |\n",
      "|    policy_gradient_loss | -3.78e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.69e+09     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.21e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1342         |\n",
      "|    time_elapsed         | 37036        |\n",
      "|    total_timesteps      | 2748416      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0271       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.4e+10      |\n",
      "|    n_updates            | 13410        |\n",
      "|    policy_gradient_loss | -2.28e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.7e+10      |\n",
      "------------------------------------------\n",
      "Episode 1100  finished with cumulative reward: -1299500.0 and \n",
      "with an average reward of: -519.5921631347461\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2748599\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -519.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1343      |\n",
      "|    time_elapsed         | 37064     |\n",
      "|    total_timesteps      | 2750464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0317    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.54e+09  |\n",
      "|    n_updates            | 13420     |\n",
      "|    policy_gradient_loss | -1.58e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.29e+09  |\n",
      "---------------------------------------\n",
      "Episode 1101  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2751100\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1344      |\n",
      "|    time_elapsed         | 37091     |\n",
      "|    total_timesteps      | 2752512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0368    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.44e+09  |\n",
      "|    n_updates            | 13430     |\n",
      "|    policy_gradient_loss | -2.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 1102  finished with cumulative reward: -12009500.0 and \n",
      "with an average reward of: -4801.87924830068\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2753601\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4803.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1345      |\n",
      "|    time_elapsed         | 37119     |\n",
      "|    total_timesteps      | 2754560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.7e+10   |\n",
      "|    n_updates            | 13440     |\n",
      "|    policy_gradient_loss | -8.96e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.7e+10   |\n",
      "---------------------------------------\n",
      "Episode 1103  finished with cumulative reward: -7368500.0 and \n",
      "with an average reward of: -2946.2215113954417\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2756102\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2947.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1346      |\n",
      "|    time_elapsed         | 37147     |\n",
      "|    total_timesteps      | 2756608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0319    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.59e+09  |\n",
      "|    n_updates            | 13450     |\n",
      "|    policy_gradient_loss | -1.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 1104  finished with cumulative reward: -4359500.0 and \n",
      "with an average reward of: -1743.1027588964414\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2758603\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1743.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1347      |\n",
      "|    time_elapsed         | 37175     |\n",
      "|    total_timesteps      | 2758656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0208    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.05e+10  |\n",
      "|    n_updates            | 13460     |\n",
      "|    policy_gradient_loss | -2.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.87e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.25e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1348         |\n",
      "|    time_elapsed         | 37195        |\n",
      "|    total_timesteps      | 2760704      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.037        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.39e+09     |\n",
      "|    n_updates            | 13470        |\n",
      "|    policy_gradient_loss | -4.54e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.54e+09     |\n",
      "------------------------------------------\n",
      "Episode 1105  finished with cumulative reward: -2778500.0 and \n",
      "with an average reward of: -1110.955617752899\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2761104\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1111.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1349      |\n",
      "|    time_elapsed         | 37224     |\n",
      "|    total_timesteps      | 2762752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.031     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.91e+09  |\n",
      "|    n_updates            | 13480     |\n",
      "|    policy_gradient_loss | -1.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.49e+09  |\n",
      "---------------------------------------\n",
      "Episode 1106  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2763605\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1350      |\n",
      "|    time_elapsed         | 37252     |\n",
      "|    total_timesteps      | 2764800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.93e+09  |\n",
      "|    n_updates            | 13490     |\n",
      "|    policy_gradient_loss | -2.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1e+10     |\n",
      "---------------------------------------\n",
      "Episode 1107  finished with cumulative reward: -11423000.0 and \n",
      "with an average reward of: -4567.373050779688\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2766106\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4569.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1351      |\n",
      "|    time_elapsed         | 37280     |\n",
      "|    total_timesteps      | 2766848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0251    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.39e+09  |\n",
      "|    n_updates            | 13500     |\n",
      "|    policy_gradient_loss | -1.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.99e+10  |\n",
      "---------------------------------------\n",
      "Episode 1108  finished with cumulative reward: -4589000.0 and \n",
      "with an average reward of: -1834.8660535785687\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2768607\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1835.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.25e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1352         |\n",
      "|    time_elapsed         | 37308        |\n",
      "|    total_timesteps      | 2768896      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0279       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.07e+10     |\n",
      "|    n_updates            | 13510        |\n",
      "|    policy_gradient_loss | -3.59e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.59e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1353      |\n",
      "|    time_elapsed         | 37330     |\n",
      "|    total_timesteps      | 2770944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.43e+09  |\n",
      "|    n_updates            | 13520     |\n",
      "|    policy_gradient_loss | -1.96e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.77e+09  |\n",
      "---------------------------------------\n",
      "Episode 1109  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2771108\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1354      |\n",
      "|    time_elapsed         | 37358     |\n",
      "|    total_timesteps      | 2772992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0236    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.72e+09  |\n",
      "|    n_updates            | 13530     |\n",
      "|    policy_gradient_loss | -1.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.45e+09  |\n",
      "---------------------------------------\n",
      "Episode 1110  finished with cumulative reward: -8541500.0 and \n",
      "with an average reward of: -3415.233906437425\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2773609\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3416.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1355      |\n",
      "|    time_elapsed         | 37386     |\n",
      "|    total_timesteps      | 2775040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0313    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.22e+09  |\n",
      "|    n_updates            | 13540     |\n",
      "|    policy_gradient_loss | -2.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.82e+10  |\n",
      "---------------------------------------\n",
      "Episode 1111  finished with cumulative reward: -5660000.0 and \n",
      "with an average reward of: -2263.094762095162\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2776110\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2264.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1356      |\n",
      "|    time_elapsed         | 37414     |\n",
      "|    total_timesteps      | 2777088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.3e+09   |\n",
      "|    n_updates            | 13550     |\n",
      "|    policy_gradient_loss | -3.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.8e+10   |\n",
      "---------------------------------------\n",
      "Episode 1112  finished with cumulative reward: -3416000.0 and \n",
      "with an average reward of: -1365.8536585365853\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2778611\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1366.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.23e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1357          |\n",
      "|    time_elapsed         | 37443         |\n",
      "|    total_timesteps      | 2779136       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.1118044e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0393        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 9.57e+08      |\n",
      "|    n_updates            | 13560         |\n",
      "|    policy_gradient_loss | -7.69e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.13e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1113  finished with cumulative reward: -3237500.0 and \n",
      "with an average reward of: -1294.4822071171532\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2781112\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1295.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1358      |\n",
      "|    time_elapsed         | 37471     |\n",
      "|    total_timesteps      | 2781184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.82e+09  |\n",
      "|    n_updates            | 13570     |\n",
      "|    policy_gradient_loss | -1.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.23e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1359         |\n",
      "|    time_elapsed         | 37492        |\n",
      "|    total_timesteps      | 2783232      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0361       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.2e+08      |\n",
      "|    n_updates            | 13580        |\n",
      "|    policy_gradient_loss | -1.82e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.8e+09      |\n",
      "------------------------------------------\n",
      "Episode 1114  finished with cumulative reward: -2625500.0 and \n",
      "with an average reward of: -1049.7800879648141\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2783613\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1050.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1360      |\n",
      "|    time_elapsed         | 37521     |\n",
      "|    total_timesteps      | 2785280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0349    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.56e+09  |\n",
      "|    n_updates            | 13590     |\n",
      "|    policy_gradient_loss | -1.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.07e+09  |\n",
      "---------------------------------------\n",
      "Episode 1115  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2786114\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1361      |\n",
      "|    time_elapsed         | 37549     |\n",
      "|    total_timesteps      | 2787328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0266    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.02e+09  |\n",
      "|    n_updates            | 13600     |\n",
      "|    policy_gradient_loss | -1.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 1116  finished with cumulative reward: -1758500.0 and \n",
      "with an average reward of: -703.1187524990004\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2788615\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -703.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.18e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1362          |\n",
      "|    time_elapsed         | 37578         |\n",
      "|    total_timesteps      | 2789376       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.3655746e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0439        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.02e+08      |\n",
      "|    n_updates            | 13610         |\n",
      "|    policy_gradient_loss | -3.72e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.61e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1117  finished with cumulative reward: -5456000.0 and \n",
      "with an average reward of: -2181.5273890443823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2791116\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2182.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1363      |\n",
      "|    time_elapsed         | 37606     |\n",
      "|    total_timesteps      | 2791424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0318    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.09e+09  |\n",
      "|    n_updates            | 13620     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.55e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.19e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1364         |\n",
      "|    time_elapsed         | 37627        |\n",
      "|    total_timesteps      | 2793472      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0299       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.25e+09     |\n",
      "|    n_updates            | 13630        |\n",
      "|    policy_gradient_loss | -1.88e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.48e+09     |\n",
      "------------------------------------------\n",
      "Episode 1118  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2793617\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1365      |\n",
      "|    time_elapsed         | 37654     |\n",
      "|    total_timesteps      | 2795520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0324    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.07e+09  |\n",
      "|    n_updates            | 13640     |\n",
      "|    policy_gradient_loss | -3.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.08e+09  |\n",
      "---------------------------------------\n",
      "Episode 1119  finished with cumulative reward: -6170000.0 and \n",
      "with an average reward of: -2467.013194722111\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2796118\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2468.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1366      |\n",
      "|    time_elapsed         | 37682     |\n",
      "|    total_timesteps      | 2797568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0243    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.24e+09  |\n",
      "|    n_updates            | 13650     |\n",
      "|    policy_gradient_loss | -1.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 1120  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2798619\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1367      |\n",
      "|    time_elapsed         | 37710     |\n",
      "|    total_timesteps      | 2799616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0329    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.37e+09  |\n",
      "|    n_updates            | 13660     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.72e+09  |\n",
      "---------------------------------------\n",
      "Episode 1121  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2801120\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1368      |\n",
      "|    time_elapsed         | 37738     |\n",
      "|    total_timesteps      | 2801664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0277    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.5e+09   |\n",
      "|    n_updates            | 13670     |\n",
      "|    policy_gradient_loss | -1.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n",
      "Episode 1122  finished with cumulative reward: -4563500.0 and \n",
      "with an average reward of: -1824.670131947221\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2803621\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1825.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1369      |\n",
      "|    time_elapsed         | 37766     |\n",
      "|    total_timesteps      | 2803712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.13e+09  |\n",
      "|    n_updates            | 13680     |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1370      |\n",
      "|    time_elapsed         | 37787     |\n",
      "|    total_timesteps      | 2805760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0381    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.52e+09  |\n",
      "|    n_updates            | 13690     |\n",
      "|    policy_gradient_loss | -9.16e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.27e+09  |\n",
      "---------------------------------------\n",
      "Episode 1123  finished with cumulative reward: -6272000.0 and \n",
      "with an average reward of: -2507.796881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2806122\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2508.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1371      |\n",
      "|    time_elapsed         | 37815     |\n",
      "|    total_timesteps      | 2807808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0302    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.56e+09  |\n",
      "|    n_updates            | 13700     |\n",
      "|    policy_gradient_loss | -1.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 1124  finished with cumulative reward: -14381000.0 and \n",
      "with an average reward of: -5750.099960015994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2808623\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5752.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1372      |\n",
      "|    time_elapsed         | 37843     |\n",
      "|    total_timesteps      | 2809856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.65e+10  |\n",
      "|    n_updates            | 13710     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 1125  finished with cumulative reward: -7190000.0 and \n",
      "with an average reward of: -2874.8500599760096\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2811124\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2876.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -6.3e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1373     |\n",
      "|    time_elapsed         | 37872    |\n",
      "|    total_timesteps      | 2811904  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0307   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.04e+10 |\n",
      "|    n_updates            | 13720    |\n",
      "|    policy_gradient_loss | -7.8e-07 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.89e+10 |\n",
      "--------------------------------------\n",
      "Episode 1126  finished with cumulative reward: -6578000.0 and \n",
      "with an average reward of: -2630.1479408236705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2813625\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2631.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.26e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1374         |\n",
      "|    time_elapsed         | 37901        |\n",
      "|    total_timesteps      | 2813952      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.683411e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0324       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.24e+09     |\n",
      "|    n_updates            | 13730        |\n",
      "|    policy_gradient_loss | -2.04e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.23e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.26e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1375      |\n",
      "|    time_elapsed         | 37923     |\n",
      "|    total_timesteps      | 2816000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0219    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.24e+09  |\n",
      "|    n_updates            | 13740     |\n",
      "|    policy_gradient_loss | -6.57e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 1127  finished with cumulative reward: -4920500.0 and \n",
      "with an average reward of: -1967.4130347860855\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2816126\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1968.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.28e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1376      |\n",
      "|    time_elapsed         | 37951     |\n",
      "|    total_timesteps      | 2818048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0301    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.57e+09  |\n",
      "|    n_updates            | 13750     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 1128  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2818627\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1377      |\n",
      "|    time_elapsed         | 37978     |\n",
      "|    total_timesteps      | 2820096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0414    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.58e+09  |\n",
      "|    n_updates            | 13760     |\n",
      "|    policy_gradient_loss | -1.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.29e+09  |\n",
      "---------------------------------------\n",
      "Episode 1129  finished with cumulative reward: -2880500.0 and \n",
      "with an average reward of: -1151.7393042782887\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2821128\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1152.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1378      |\n",
      "|    time_elapsed         | 38006     |\n",
      "|    total_timesteps      | 2822144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0314    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.89e+09  |\n",
      "|    n_updates            | 13770     |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.5e+09   |\n",
      "---------------------------------------\n",
      "Episode 1130  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2823629\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1379      |\n",
      "|    time_elapsed         | 38034     |\n",
      "|    total_timesteps      | 2824192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0238    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.41e+09  |\n",
      "|    n_updates            | 13780     |\n",
      "|    policy_gradient_loss | -7.64e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1131  finished with cumulative reward: -5634500.0 and \n",
      "with an average reward of: -2252.8988404638144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2826130\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2253.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1380      |\n",
      "|    time_elapsed         | 38062     |\n",
      "|    total_timesteps      | 2826240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0482    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.01e+09  |\n",
      "|    n_updates            | 13790     |\n",
      "|    policy_gradient_loss | -3.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.39e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1381      |\n",
      "|    time_elapsed         | 38083     |\n",
      "|    total_timesteps      | 2828288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0271    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.56e+09  |\n",
      "|    n_updates            | 13800     |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 1132  finished with cumulative reward: -1631000.0 and \n",
      "with an average reward of: -652.1391443422631\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2828631\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -652.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1382      |\n",
      "|    time_elapsed         | 38111     |\n",
      "|    total_timesteps      | 2830336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.04      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.17e+09  |\n",
      "|    n_updates            | 13810     |\n",
      "|    policy_gradient_loss | -1.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.35e+09  |\n",
      "---------------------------------------\n",
      "Episode 1133  finished with cumulative reward: -6476000.0 and \n",
      "with an average reward of: -2589.3642542982807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2831132\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2590.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1383      |\n",
      "|    time_elapsed         | 38139     |\n",
      "|    total_timesteps      | 2832384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0231    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.47e+09  |\n",
      "|    n_updates            | 13820     |\n",
      "|    policy_gradient_loss | -1.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1134  finished with cumulative reward: -5456000.0 and \n",
      "with an average reward of: -2181.5273890443823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2833633\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2182.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1384      |\n",
      "|    time_elapsed         | 38167     |\n",
      "|    total_timesteps      | 2834432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0347    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.93e+09  |\n",
      "|    n_updates            | 13830     |\n",
      "|    policy_gradient_loss | -2.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 1135  finished with cumulative reward: -5940500.0 and \n",
      "with an average reward of: -2375.249900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2836134\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2376.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1385      |\n",
      "|    time_elapsed         | 38195     |\n",
      "|    total_timesteps      | 2836480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.02e+09  |\n",
      "|    n_updates            | 13840     |\n",
      "|    policy_gradient_loss | -2.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.93e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1386      |\n",
      "|    time_elapsed         | 38216     |\n",
      "|    total_timesteps      | 2838528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.026     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.8e+09   |\n",
      "|    n_updates            | 13850     |\n",
      "|    policy_gradient_loss | -3.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.42e+10  |\n",
      "---------------------------------------\n",
      "Episode 1136  finished with cumulative reward: -2931500.0 and \n",
      "with an average reward of: -1172.1311475409836\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2838635\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1172.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1387      |\n",
      "|    time_elapsed         | 38245     |\n",
      "|    total_timesteps      | 2840576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.67e+09  |\n",
      "|    n_updates            | 13860     |\n",
      "|    policy_gradient_loss | -1.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.3e+09   |\n",
      "---------------------------------------\n",
      "Episode 1137  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2841136\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1388      |\n",
      "|    time_elapsed         | 38273     |\n",
      "|    total_timesteps      | 2842624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0304    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.79e+09  |\n",
      "|    n_updates            | 13870     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n",
      "Episode 1138  finished with cumulative reward: -6017000.0 and \n",
      "with an average reward of: -2405.837664934026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2843637\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2406.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1389      |\n",
      "|    time_elapsed         | 38301     |\n",
      "|    total_timesteps      | 2844672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0375    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.43e+09  |\n",
      "|    n_updates            | 13880     |\n",
      "|    policy_gradient_loss | -2.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 1139  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2846138\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1390      |\n",
      "|    time_elapsed         | 38329     |\n",
      "|    total_timesteps      | 2846720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0323    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.67e+09  |\n",
      "|    n_updates            | 13890     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 1140  finished with cumulative reward: -2906000.0 and \n",
      "with an average reward of: -1161.935225909636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2848639\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1162.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1391      |\n",
      "|    time_elapsed         | 38358     |\n",
      "|    total_timesteps      | 2848768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0306    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.73e+09  |\n",
      "|    n_updates            | 13900     |\n",
      "|    policy_gradient_loss | -2.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1392      |\n",
      "|    time_elapsed         | 38378     |\n",
      "|    total_timesteps      | 2850816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0333    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.2e+09   |\n",
      "|    n_updates            | 13910     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 1141  finished with cumulative reward: -8720000.0 and \n",
      "with an average reward of: -3486.605357856857\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2851140\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3488.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1393      |\n",
      "|    time_elapsed         | 38407     |\n",
      "|    total_timesteps      | 2852864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0242    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.34e+09  |\n",
      "|    n_updates            | 13920     |\n",
      "|    policy_gradient_loss | -4.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.59e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1142  finished with cumulative reward: -4971500.0 and \n",
      "with an average reward of: -1987.8048780487804\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2853641\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1988.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1394      |\n",
      "|    time_elapsed         | 38435     |\n",
      "|    total_timesteps      | 2854912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0405    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.47e+09  |\n",
      "|    n_updates            | 13930     |\n",
      "|    policy_gradient_loss | -2.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.35e+09  |\n",
      "---------------------------------------\n",
      "Episode 1143  finished with cumulative reward: -7317500.0 and \n",
      "with an average reward of: -2925.829668132747\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2856142\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2927.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1395      |\n",
      "|    time_elapsed         | 38463     |\n",
      "|    total_timesteps      | 2856960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0397    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.37e+09  |\n",
      "|    n_updates            | 13940     |\n",
      "|    policy_gradient_loss | -1.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.12e+09  |\n",
      "---------------------------------------\n",
      "Episode 1144  finished with cumulative reward: 26500.0 and \n",
      "with an average reward of: 10.59576169532187\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2858643\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: 10.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1396      |\n",
      "|    time_elapsed         | 38491     |\n",
      "|    total_timesteps      | 2859008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0304    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.6e+09   |\n",
      "|    n_updates            | 13950     |\n",
      "|    policy_gradient_loss | -1.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1397         |\n",
      "|    time_elapsed         | 38513        |\n",
      "|    total_timesteps      | 2861056      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0353       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.49e+09     |\n",
      "|    n_updates            | 13960        |\n",
      "|    policy_gradient_loss | -4.7e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.79e+09     |\n",
      "------------------------------------------\n",
      "Episode 1145  finished with cumulative reward: -5481500.0 and \n",
      "with an average reward of: -2191.72331067573\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2861144\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2192.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1398      |\n",
      "|    time_elapsed         | 38541     |\n",
      "|    total_timesteps      | 2863104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.54e+09  |\n",
      "|    n_updates            | 13970     |\n",
      "|    policy_gradient_loss | -2.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.85e+09  |\n",
      "---------------------------------------\n",
      "Episode 1146  finished with cumulative reward: -789500.0 and \n",
      "with an average reward of: -315.6737305077969\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2863645\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -315.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1399      |\n",
      "|    time_elapsed         | 38570     |\n",
      "|    total_timesteps      | 2865152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0517    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.18e+08  |\n",
      "|    n_updates            | 13980     |\n",
      "|    policy_gradient_loss | -1.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.88e+09  |\n",
      "---------------------------------------\n",
      "Episode 1147  finished with cumulative reward: -2243000.0 and \n",
      "with an average reward of: -896.8412634946021\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2866146\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -897.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.04e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1400          |\n",
      "|    time_elapsed         | 38598         |\n",
      "|    total_timesteps      | 2867200       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 7.3341653e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0589        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.31e+09      |\n",
      "|    n_updates            | 13990         |\n",
      "|    policy_gradient_loss | -1.02e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.7e+09       |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1148  finished with cumulative reward: -1044500.0 and \n",
      "with an average reward of: -417.6329468212715\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2868647\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -417.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1401      |\n",
      "|    time_elapsed         | 38626     |\n",
      "|    total_timesteps      | 2869248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0381    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.78e+09  |\n",
      "|    n_updates            | 14000     |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.81e+09  |\n",
      "---------------------------------------\n",
      "Episode 1149  finished with cumulative reward: -6374000.0 and \n",
      "with an average reward of: -2548.580567772891\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2871148\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2549.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1402      |\n",
      "|    time_elapsed         | 38654     |\n",
      "|    total_timesteps      | 2871296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.83e+09  |\n",
      "|    n_updates            | 14010     |\n",
      "|    policy_gradient_loss | -3.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.45e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1403      |\n",
      "|    time_elapsed         | 38675     |\n",
      "|    total_timesteps      | 2873344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0337    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.81e+09  |\n",
      "|    n_updates            | 14020     |\n",
      "|    policy_gradient_loss | -1.14e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.29e+09  |\n",
      "---------------------------------------\n",
      "Episode 1150  finished with cumulative reward: -7725500.0 and \n",
      "with an average reward of: -3088.9644142343063\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2873649\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3090.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1404      |\n",
      "|    time_elapsed         | 38703     |\n",
      "|    total_timesteps      | 2875392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.79e+09  |\n",
      "|    n_updates            | 14030     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 1151  finished with cumulative reward: -7674500.0 and \n",
      "with an average reward of: -3068.572570971611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2876150\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3069.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1405      |\n",
      "|    time_elapsed         | 38732     |\n",
      "|    total_timesteps      | 2877440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0309    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.4e+09   |\n",
      "|    n_updates            | 14040     |\n",
      "|    policy_gradient_loss | -3.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.15e+09  |\n",
      "---------------------------------------\n",
      "Episode 1152  finished with cumulative reward: -5150000.0 and \n",
      "with an average reward of: -2059.1763294682128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2878651\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2060.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1406      |\n",
      "|    time_elapsed         | 38760     |\n",
      "|    total_timesteps      | 2879488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0308    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.02e+09  |\n",
      "|    n_updates            | 14050     |\n",
      "|    policy_gradient_loss | -2.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.59e+10  |\n",
      "---------------------------------------\n",
      "Episode 1153  finished with cumulative reward: -1733000.0 and \n",
      "with an average reward of: -692.9228308676529\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2881152\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -693.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1407      |\n",
      "|    time_elapsed         | 38788     |\n",
      "|    total_timesteps      | 2881536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0316    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.3e+09   |\n",
      "|    n_updates            | 14060     |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.45e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1408      |\n",
      "|    time_elapsed         | 38809     |\n",
      "|    total_timesteps      | 2883584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0376    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4e+09     |\n",
      "|    n_updates            | 14070     |\n",
      "|    policy_gradient_loss | -7.72e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.81e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1154  finished with cumulative reward: -7343000.0 and \n",
      "with an average reward of: -2936.0255897640945\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2883653\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2937.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1409      |\n",
      "|    time_elapsed         | 38837     |\n",
      "|    total_timesteps      | 2885632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.73e+09  |\n",
      "|    n_updates            | 14080     |\n",
      "|    policy_gradient_loss | -1.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1155  finished with cumulative reward: -9357500.0 and \n",
      "with an average reward of: -3741.5033986405438\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2886154\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3743.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1410      |\n",
      "|    time_elapsed         | 38866     |\n",
      "|    total_timesteps      | 2887680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.03      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.15e+10  |\n",
      "|    n_updates            | 14090     |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 1156  finished with cumulative reward: -7368500.0 and \n",
      "with an average reward of: -2946.2215113954417\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2888655\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2947.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1411      |\n",
      "|    time_elapsed         | 38895     |\n",
      "|    total_timesteps      | 2889728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+10  |\n",
      "|    n_updates            | 14100     |\n",
      "|    policy_gradient_loss | -1.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.26e+10  |\n",
      "---------------------------------------\n",
      "Episode 1157  finished with cumulative reward: -1503500.0 and \n",
      "with an average reward of: -601.1595361855258\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2891156\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -601.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1412      |\n",
      "|    time_elapsed         | 38923     |\n",
      "|    total_timesteps      | 2891776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0332    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.02e+09  |\n",
      "|    n_updates            | 14110     |\n",
      "|    policy_gradient_loss | -1.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 1158  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2893657\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1413      |\n",
      "|    time_elapsed         | 38951     |\n",
      "|    total_timesteps      | 2893824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0364    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.31e+09  |\n",
      "|    n_updates            | 14120     |\n",
      "|    policy_gradient_loss | -1.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.52e+09  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.8e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1414          |\n",
      "|    time_elapsed         | 38972         |\n",
      "|    total_timesteps      | 2895872       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.029         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.28e+10      |\n",
      "|    n_updates            | 14130         |\n",
      "|    policy_gradient_loss | -5.22e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.99e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1159  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2896158\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.76e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1415      |\n",
      "|    time_elapsed         | 39000     |\n",
      "|    total_timesteps      | 2897920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0321    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.84e+09  |\n",
      "|    n_updates            | 14140     |\n",
      "|    policy_gradient_loss | -1.28e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.6e+09   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1160  finished with cumulative reward: -5991500.0 and \n",
      "with an average reward of: -2395.641743302679\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2898659\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2396.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.78e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1416         |\n",
      "|    time_elapsed         | 39027        |\n",
      "|    total_timesteps      | 2899968      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0315       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.9e+09      |\n",
      "|    n_updates            | 14150        |\n",
      "|    policy_gradient_loss | -4.27e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.95e+09     |\n",
      "------------------------------------------\n",
      "Episode 1161  finished with cumulative reward: -2039000.0 and \n",
      "with an average reward of: -815.2738904438224\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2901160\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -815.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.74e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1417          |\n",
      "|    time_elapsed         | 39055         |\n",
      "|    total_timesteps      | 2902016       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.2491592e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0368        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.01e+09      |\n",
      "|    n_updates            | 14160         |\n",
      "|    policy_gradient_loss | -5.57e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.15e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1162  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2903661\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.74e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1418      |\n",
      "|    time_elapsed         | 39083     |\n",
      "|    total_timesteps      | 2904064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0353    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.02e+09  |\n",
      "|    n_updates            | 14170     |\n",
      "|    policy_gradient_loss | -1.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.76e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.74e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1419      |\n",
      "|    time_elapsed         | 39104     |\n",
      "|    total_timesteps      | 2906112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0325    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.36e+09  |\n",
      "|    n_updates            | 14180     |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.25e+09  |\n",
      "---------------------------------------\n",
      "Episode 1163  finished with cumulative reward: -4232000.0 and \n",
      "with an average reward of: -1692.1231507397042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2906162\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1692.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1420      |\n",
      "|    time_elapsed         | 39133     |\n",
      "|    total_timesteps      | 2908160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.74e+09  |\n",
      "|    n_updates            | 14190     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 1164  finished with cumulative reward: -4589000.0 and \n",
      "with an average reward of: -1834.8660535785687\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2908663\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1835.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1421      |\n",
      "|    time_elapsed         | 39161     |\n",
      "|    total_timesteps      | 2910208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0333    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.95e+09  |\n",
      "|    n_updates            | 14200     |\n",
      "|    policy_gradient_loss | -3.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.8e+09   |\n",
      "---------------------------------------\n",
      "Episode 1165  finished with cumulative reward: -2090000.0 and \n",
      "with an average reward of: -835.6657337065174\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2911164\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -836.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1422      |\n",
      "|    time_elapsed         | 39189     |\n",
      "|    total_timesteps      | 2912256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.83e+08  |\n",
      "|    n_updates            | 14210     |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.74e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1166  finished with cumulative reward: -6323000.0 and \n",
      "with an average reward of: -2528.1887245101957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2913665\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2529.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.6e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1423     |\n",
      "|    time_elapsed         | 39217    |\n",
      "|    total_timesteps      | 2914304  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0318   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 3.87e+09 |\n",
      "|    n_updates            | 14220    |\n",
      "|    policy_gradient_loss | -1.3e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 8.23e+09 |\n",
      "--------------------------------------\n",
      "Episode 1167  finished with cumulative reward: -5583500.0 and \n",
      "with an average reward of: -2232.5069972011197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2916166\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2233.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.63e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1424          |\n",
      "|    time_elapsed         | 39245         |\n",
      "|    total_timesteps      | 2916352       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0194        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.56e+09      |\n",
      "|    n_updates            | 14230         |\n",
      "|    policy_gradient_loss | -1.87e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.05e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1425      |\n",
      "|    time_elapsed         | 39266     |\n",
      "|    total_timesteps      | 2918400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.22e+09  |\n",
      "|    n_updates            | 14240     |\n",
      "|    policy_gradient_loss | -1.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.58e+10  |\n",
      "---------------------------------------\n",
      "Episode 1168  finished with cumulative reward: -6654500.0 and \n",
      "with an average reward of: -2660.735705717713\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2918667\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2661.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1426      |\n",
      "|    time_elapsed         | 39294     |\n",
      "|    total_timesteps      | 2920448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0279    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.95e+09  |\n",
      "|    n_updates            | 14250     |\n",
      "|    policy_gradient_loss | -1.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.87e+10  |\n",
      "---------------------------------------\n",
      "Episode 1169  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2921168\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1427      |\n",
      "|    time_elapsed         | 39322     |\n",
      "|    total_timesteps      | 2922496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0275    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.03e+09  |\n",
      "|    n_updates            | 14260     |\n",
      "|    policy_gradient_loss | -1.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.5e+10   |\n",
      "---------------------------------------\n",
      "Episode 1170  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2923669\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1428      |\n",
      "|    time_elapsed         | 39350     |\n",
      "|    total_timesteps      | 2924544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0351    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.37e+09  |\n",
      "|    n_updates            | 14270     |\n",
      "|    policy_gradient_loss | -1.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.22e+09  |\n",
      "---------------------------------------\n",
      "Episode 1171  finished with cumulative reward: -2447000.0 and \n",
      "with an average reward of: -978.4086365453818\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2926170\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -978.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1429      |\n",
      "|    time_elapsed         | 39379     |\n",
      "|    total_timesteps      | 2926592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0375    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.45e+09  |\n",
      "|    n_updates            | 14280     |\n",
      "|    policy_gradient_loss | -1.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.74e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.61e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1430          |\n",
      "|    time_elapsed         | 39400         |\n",
      "|    total_timesteps      | 2928640       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.8335413e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0606        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.45e+08      |\n",
      "|    n_updates            | 14290         |\n",
      "|    policy_gradient_loss | -4.78e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.11e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1172  finished with cumulative reward: -8592500.0 and \n",
      "with an average reward of: -3435.6257497001197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2928671\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3437.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1431      |\n",
      "|    time_elapsed         | 39429     |\n",
      "|    total_timesteps      | 2930688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+10  |\n",
      "|    n_updates            | 14300     |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.42e+10  |\n",
      "---------------------------------------\n",
      "Episode 1173  finished with cumulative reward: -3926000.0 and \n",
      "with an average reward of: -1569.7720911635347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2931172\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1570.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1432      |\n",
      "|    time_elapsed         | 39457     |\n",
      "|    total_timesteps      | 2932736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0404    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.11e+09  |\n",
      "|    n_updates            | 14310     |\n",
      "|    policy_gradient_loss | -2.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.86e+09  |\n",
      "---------------------------------------\n",
      "Episode 1174  finished with cumulative reward: -2090000.0 and \n",
      "with an average reward of: -835.6657337065174\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2933673\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -836.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.49e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1433      |\n",
      "|    time_elapsed         | 39485     |\n",
      "|    total_timesteps      | 2934784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0432    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.38e+09  |\n",
      "|    n_updates            | 14320     |\n",
      "|    policy_gradient_loss | -2.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 1175  finished with cumulative reward: -5711000.0 and \n",
      "with an average reward of: -2283.4866053578567\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2936174\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2284.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.46e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1434      |\n",
      "|    time_elapsed         | 39513     |\n",
      "|    total_timesteps      | 2936832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.21e+09  |\n",
      "|    n_updates            | 14330     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.26e+09  |\n",
      "---------------------------------------\n",
      "Episode 1176  finished with cumulative reward: -3747500.0 and \n",
      "with an average reward of: -1498.4006397441024\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2938675\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1499.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.41e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1435         |\n",
      "|    time_elapsed         | 39541        |\n",
      "|    total_timesteps      | 2938880      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0353       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.46e+09     |\n",
      "|    n_updates            | 14340        |\n",
      "|    policy_gradient_loss | -6.62e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.16e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1436      |\n",
      "|    time_elapsed         | 39562     |\n",
      "|    total_timesteps      | 2940928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.032     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.97e+09  |\n",
      "|    n_updates            | 14350     |\n",
      "|    policy_gradient_loss | -1.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 1177  finished with cumulative reward: -1809500.0 and \n",
      "with an average reward of: -723.5105957616953\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2941176\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -723.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1437      |\n",
      "|    time_elapsed         | 39590     |\n",
      "|    total_timesteps      | 2942976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0493    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.25e+08  |\n",
      "|    n_updates            | 14360     |\n",
      "|    policy_gradient_loss | -1.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.03e+09  |\n",
      "---------------------------------------\n",
      "Episode 1178  finished with cumulative reward: -4614500.0 and \n",
      "with an average reward of: -1845.061975209916\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2943677\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1845.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1438      |\n",
      "|    time_elapsed         | 39618     |\n",
      "|    total_timesteps      | 2945024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0354    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.89e+09  |\n",
      "|    n_updates            | 14370     |\n",
      "|    policy_gradient_loss | -3.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.79e+09  |\n",
      "---------------------------------------\n",
      "Episode 1179  finished with cumulative reward: -7547000.0 and \n",
      "with an average reward of: -3017.592962814874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2946178\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3018.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.37e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1439      |\n",
      "|    time_elapsed         | 39647     |\n",
      "|    total_timesteps      | 2947072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0202    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.44e+09  |\n",
      "|    n_updates            | 14380     |\n",
      "|    policy_gradient_loss | -9.13e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "Episode 1180  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2948679\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1440      |\n",
      "|    time_elapsed         | 39675     |\n",
      "|    total_timesteps      | 2949120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.6e+09   |\n",
      "|    n_updates            | 14390     |\n",
      "|    policy_gradient_loss | -2.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1441      |\n",
      "|    time_elapsed         | 39696     |\n",
      "|    total_timesteps      | 2951168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0314    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.68e+09  |\n",
      "|    n_updates            | 14400     |\n",
      "|    policy_gradient_loss | -1.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "Episode 1181  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2951180\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.33e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1442      |\n",
      "|    time_elapsed         | 39724     |\n",
      "|    total_timesteps      | 2953216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0362    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.73e+09  |\n",
      "|    n_updates            | 14410     |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.07e+09  |\n",
      "---------------------------------------\n",
      "Episode 1182  finished with cumulative reward: -15197000.0 and \n",
      "with an average reward of: -6076.369452219113\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2953681\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -6078.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.45e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1443      |\n",
      "|    time_elapsed         | 39753     |\n",
      "|    total_timesteps      | 2955264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.61e+10  |\n",
      "|    n_updates            | 14420     |\n",
      "|    policy_gradient_loss | -8.15e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.94e+10  |\n",
      "---------------------------------------\n",
      "Episode 1183  finished with cumulative reward: -8159000.0 and \n",
      "with an average reward of: -3262.2950819672133\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2956182\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3263.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.48e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1444         |\n",
      "|    time_elapsed         | 39781        |\n",
      "|    total_timesteps      | 2957312      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0234       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.67e+09     |\n",
      "|    n_updates            | 14430        |\n",
      "|    policy_gradient_loss | -1.38e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.09e+10     |\n",
      "------------------------------------------\n",
      "Episode 1184  finished with cumulative reward: -7470500.0 and \n",
      "with an average reward of: -2987.0051979208315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2958683\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2988.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1445      |\n",
      "|    time_elapsed         | 39809     |\n",
      "|    total_timesteps      | 2959360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0274    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.18e+10  |\n",
      "|    n_updates            | 14440     |\n",
      "|    policy_gradient_loss | -1.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 1185  finished with cumulative reward: -917000.0 and \n",
      "with an average reward of: -366.6533386645342\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2961184\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -366.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1446      |\n",
      "|    time_elapsed         | 39836     |\n",
      "|    total_timesteps      | 2961408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0313    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.59e+09  |\n",
      "|    n_updates            | 14450     |\n",
      "|    policy_gradient_loss | -8.93e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.29e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1447      |\n",
      "|    time_elapsed         | 39857     |\n",
      "|    total_timesteps      | 2963456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0379    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.84e+09  |\n",
      "|    n_updates            | 14460     |\n",
      "|    policy_gradient_loss | -3.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.05e+09  |\n",
      "---------------------------------------\n",
      "Episode 1186  finished with cumulative reward: -3263000.0 and \n",
      "with an average reward of: -1304.6781287485005\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2963685\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1305.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1448      |\n",
      "|    time_elapsed         | 39886     |\n",
      "|    total_timesteps      | 2965504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0408    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.5e+09   |\n",
      "|    n_updates            | 14470     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.89e+09  |\n",
      "---------------------------------------\n",
      "Episode 1187  finished with cumulative reward: -5150000.0 and \n",
      "with an average reward of: -2059.1763294682128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2966186\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2060.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1449      |\n",
      "|    time_elapsed         | 39914     |\n",
      "|    total_timesteps      | 2967552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0239    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.68e+09  |\n",
      "|    n_updates            | 14480     |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 1188  finished with cumulative reward: -7547000.0 and \n",
      "with an average reward of: -3017.592962814874\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2968687\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3018.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.49e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1450      |\n",
      "|    time_elapsed         | 39942     |\n",
      "|    total_timesteps      | 2969600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.82e+09  |\n",
      "|    n_updates            | 14490     |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.72e+10  |\n",
      "---------------------------------------\n",
      "Episode 1189  finished with cumulative reward: -509000.0 and \n",
      "with an average reward of: -203.51859256297482\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2971188\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -203.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.45e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1451          |\n",
      "|    time_elapsed         | 39971         |\n",
      "|    total_timesteps      | 2971648       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.7252903e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0368        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.57e+09      |\n",
      "|    n_updates            | 14500         |\n",
      "|    policy_gradient_loss | -1.02e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.02e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1190  finished with cumulative reward: -4640000.0 and \n",
      "with an average reward of: -1855.2578968412636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2973689\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1856.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.4e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1452          |\n",
      "|    time_elapsed         | 39999         |\n",
      "|    total_timesteps      | 2973696       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9849193e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0437        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.62e+09      |\n",
      "|    n_updates            | 14510         |\n",
      "|    policy_gradient_loss | -9.49e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.16e+09      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1453      |\n",
      "|    time_elapsed         | 40020     |\n",
      "|    total_timesteps      | 2975744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0318    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.92e+09  |\n",
      "|    n_updates            | 14520     |\n",
      "|    policy_gradient_loss | -1.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.86e+09  |\n",
      "---------------------------------------\n",
      "Episode 1191  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2976190\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.37e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1454      |\n",
      "|    time_elapsed         | 40048     |\n",
      "|    total_timesteps      | 2977792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0284    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.31e+09  |\n",
      "|    n_updates            | 14530     |\n",
      "|    policy_gradient_loss | -1.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.47e+10  |\n",
      "---------------------------------------\n",
      "Episode 1192  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2978691\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.31e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1455      |\n",
      "|    time_elapsed         | 40077     |\n",
      "|    total_timesteps      | 2979840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0411    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.35e+09  |\n",
      "|    n_updates            | 14540     |\n",
      "|    policy_gradient_loss | -3.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 1193  finished with cumulative reward: -2268500.0 and \n",
      "with an average reward of: -907.0371851259496\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2981192\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -907.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1456      |\n",
      "|    time_elapsed         | 40105     |\n",
      "|    total_timesteps      | 2981888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0337    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.57e+09  |\n",
      "|    n_updates            | 14550     |\n",
      "|    policy_gradient_loss | -2.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.17e+09  |\n",
      "---------------------------------------\n",
      "Episode 1194  finished with cumulative reward: -4436000.0 and \n",
      "with an average reward of: -1773.690523790484\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2983693\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1774.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1457      |\n",
      "|    time_elapsed         | 40133     |\n",
      "|    total_timesteps      | 2983936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0412    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.99e+09  |\n",
      "|    n_updates            | 14560     |\n",
      "|    policy_gradient_loss | -1.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.11e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1458      |\n",
      "|    time_elapsed         | 40155     |\n",
      "|    total_timesteps      | 2985984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.035     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.77e+09  |\n",
      "|    n_updates            | 14570     |\n",
      "|    policy_gradient_loss | -8.51e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.06e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1195  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2986194\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1459      |\n",
      "|    time_elapsed         | 40183     |\n",
      "|    total_timesteps      | 2988032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.032     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.13e+10  |\n",
      "|    n_updates            | 14580     |\n",
      "|    policy_gradient_loss | -2.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.89e+10  |\n",
      "---------------------------------------\n",
      "Episode 1196  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2988695\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1460      |\n",
      "|    time_elapsed         | 40211     |\n",
      "|    total_timesteps      | 2990080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0294    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.81e+09  |\n",
      "|    n_updates            | 14590     |\n",
      "|    policy_gradient_loss | -2.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.17e+09  |\n",
      "---------------------------------------\n",
      "Episode 1197  finished with cumulative reward: -9230000.0 and \n",
      "with an average reward of: -3690.5237904838064\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2991196\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3692.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1461      |\n",
      "|    time_elapsed         | 40239     |\n",
      "|    total_timesteps      | 2992128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0221    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e+10  |\n",
      "|    n_updates            | 14600     |\n",
      "|    policy_gradient_loss | -2.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 1198  finished with cumulative reward: -8924000.0 and \n",
      "with an average reward of: -3568.172730907637\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2993697\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3569.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.32e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1462      |\n",
      "|    time_elapsed         | 40267     |\n",
      "|    total_timesteps      | 2994176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0298    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.11e+09  |\n",
      "|    n_updates            | 14610     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 1199  finished with cumulative reward: -10020500.0 and \n",
      "with an average reward of: -4006.597361055578\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2996198\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4008.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.33e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1463      |\n",
      "|    time_elapsed         | 40295     |\n",
      "|    total_timesteps      | 2996224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0252    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.48e+10  |\n",
      "|    n_updates            | 14620     |\n",
      "|    policy_gradient_loss | -1.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.48e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.33e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1464      |\n",
      "|    time_elapsed         | 40317     |\n",
      "|    total_timesteps      | 2998272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0256    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.43e+09  |\n",
      "|    n_updates            | 14630     |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.95e+10  |\n",
      "---------------------------------------\n",
      "Episode 1200  finished with cumulative reward: -3110000.0 and \n",
      "with an average reward of: -1243.5025989604158\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 2998699\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1244.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1465      |\n",
      "|    time_elapsed         | 40345     |\n",
      "|    total_timesteps      | 3000320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0346    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.79e+09  |\n",
      "|    n_updates            | 14640     |\n",
      "|    policy_gradient_loss | -1.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 1201  finished with cumulative reward: -2600000.0 and \n",
      "with an average reward of: -1039.5841663334666\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3001200\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1040.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.31e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1466         |\n",
      "|    time_elapsed         | 40373        |\n",
      "|    total_timesteps      | 3002368      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0343       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.17e+10     |\n",
      "|    n_updates            | 14650        |\n",
      "|    policy_gradient_loss | -4.53e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.08e+10     |\n",
      "------------------------------------------\n",
      "Episode 1202  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3003701\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1467      |\n",
      "|    time_elapsed         | 40402     |\n",
      "|    total_timesteps      | 3004416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0696    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.43e+08  |\n",
      "|    n_updates            | 14660     |\n",
      "|    policy_gradient_loss | -1.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+09  |\n",
      "---------------------------------------\n",
      "Episode 1203  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3006202\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1468      |\n",
      "|    time_elapsed         | 40430     |\n",
      "|    total_timesteps      | 3006464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.34e+09  |\n",
      "|    n_updates            | 14670     |\n",
      "|    policy_gradient_loss | -2.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.98e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1469      |\n",
      "|    time_elapsed         | 40451     |\n",
      "|    total_timesteps      | 3008512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0166    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.7e+09   |\n",
      "|    n_updates            | 14680     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.81e+09  |\n",
      "---------------------------------------\n",
      "Episode 1204  finished with cumulative reward: -4512500.0 and \n",
      "with an average reward of: -1804.2782886845262\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3008703\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1805.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1470      |\n",
      "|    time_elapsed         | 40480     |\n",
      "|    total_timesteps      | 3010560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0304    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.92e+09  |\n",
      "|    n_updates            | 14690     |\n",
      "|    policy_gradient_loss | -1.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 1205  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3011204\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1471      |\n",
      "|    time_elapsed         | 40508     |\n",
      "|    total_timesteps      | 3012608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0314    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.2e+09   |\n",
      "|    n_updates            | 14700     |\n",
      "|    policy_gradient_loss | -1.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.17e+10  |\n",
      "---------------------------------------\n",
      "Episode 1206  finished with cumulative reward: -8031500.0 and \n",
      "with an average reward of: -3211.315473810476\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3013705\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3212.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1472      |\n",
      "|    time_elapsed         | 40536     |\n",
      "|    total_timesteps      | 3014656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0284    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.01e+09  |\n",
      "|    n_updates            | 14710     |\n",
      "|    policy_gradient_loss | -1.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "Episode 1207  finished with cumulative reward: -10479500.0 and \n",
      "with an average reward of: -4190.123950419832\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3016206\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4191.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.26e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1473          |\n",
      "|    time_elapsed         | 40564         |\n",
      "|    total_timesteps      | 3016704       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0405        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.68e+09      |\n",
      "|    n_updates            | 14720         |\n",
      "|    policy_gradient_loss | -6.71e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.2e+10       |\n",
      "-------------------------------------------\n",
      "Episode 1208  finished with cumulative reward: -7088000.0 and \n",
      "with an average reward of: -2834.0663734506197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3018707\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2835.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1474      |\n",
      "|    time_elapsed         | 40592     |\n",
      "|    total_timesteps      | 3018752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+10  |\n",
      "|    n_updates            | 14730     |\n",
      "|    policy_gradient_loss | -7.71e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.68e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1475      |\n",
      "|    time_elapsed         | 40612     |\n",
      "|    total_timesteps      | 3020800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0255    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.67e+09  |\n",
      "|    n_updates            | 14740     |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+10   |\n",
      "---------------------------------------\n",
      "Episode 1209  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3021208\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1476      |\n",
      "|    time_elapsed         | 40641     |\n",
      "|    total_timesteps      | 3022848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0336    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.06e+09  |\n",
      "|    n_updates            | 14750     |\n",
      "|    policy_gradient_loss | -2.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 1210  finished with cumulative reward: -8720000.0 and \n",
      "with an average reward of: -3486.605357856857\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3023709\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3488.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.28e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1477      |\n",
      "|    time_elapsed         | 40669     |\n",
      "|    total_timesteps      | 3024896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0349    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.85e+09  |\n",
      "|    n_updates            | 14760     |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "Episode 1211  finished with cumulative reward: -1784000.0 and \n",
      "with an average reward of: -713.3146741303478\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3026210\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -713.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1478      |\n",
      "|    time_elapsed         | 40697     |\n",
      "|    total_timesteps      | 3026944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0277    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.8e+09   |\n",
      "|    n_updates            | 14770     |\n",
      "|    policy_gradient_loss | -2.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 1212  finished with cumulative reward: -5507000.0 and \n",
      "with an average reward of: -2201.919232307077\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3028711\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2202.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.26e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1479      |\n",
      "|    time_elapsed         | 40725     |\n",
      "|    total_timesteps      | 3028992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0389    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.79e+09  |\n",
      "|    n_updates            | 14780     |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.46e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.26e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1480      |\n",
      "|    time_elapsed         | 40746     |\n",
      "|    total_timesteps      | 3031040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0274    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.69e+09  |\n",
      "|    n_updates            | 14790     |\n",
      "|    policy_gradient_loss | -1.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.03e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1213  finished with cumulative reward: -2957000.0 and \n",
      "with an average reward of: -1182.327069172331\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3031212\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1182.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.25e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1481         |\n",
      "|    time_elapsed         | 40775        |\n",
      "|    total_timesteps      | 3033088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0346       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.75e+09     |\n",
      "|    n_updates            | 14800        |\n",
      "|    policy_gradient_loss | -2.67e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.13e+09     |\n",
      "------------------------------------------\n",
      "Episode 1214  finished with cumulative reward: -2447000.0 and \n",
      "with an average reward of: -978.4086365453818\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3033713\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -978.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1482      |\n",
      "|    time_elapsed         | 40803     |\n",
      "|    total_timesteps      | 3035136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.024     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.68e+09  |\n",
      "|    n_updates            | 14810     |\n",
      "|    policy_gradient_loss | -1.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.56e+09  |\n",
      "---------------------------------------\n",
      "Episode 1215  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3036214\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1483      |\n",
      "|    time_elapsed         | 40831     |\n",
      "|    total_timesteps      | 3037184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.033     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.25e+08  |\n",
      "|    n_updates            | 14820     |\n",
      "|    policy_gradient_loss | -2.57e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.74e+09  |\n",
      "---------------------------------------\n",
      "Episode 1216  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3038715\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1484      |\n",
      "|    time_elapsed         | 40859     |\n",
      "|    total_timesteps      | 3039232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0355    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.24e+09  |\n",
      "|    n_updates            | 14830     |\n",
      "|    policy_gradient_loss | -1.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.29e+10  |\n",
      "---------------------------------------\n",
      "Episode 1217  finished with cumulative reward: -2064500.0 and \n",
      "with an average reward of: -825.4698120751699\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3041216\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -825.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1485      |\n",
      "|    time_elapsed         | 40887     |\n",
      "|    total_timesteps      | 3041280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0322    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.06e+09  |\n",
      "|    n_updates            | 14840     |\n",
      "|    policy_gradient_loss | -9.49e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.94e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1486      |\n",
      "|    time_elapsed         | 40908     |\n",
      "|    total_timesteps      | 3043328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0336    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.01e+09  |\n",
      "|    n_updates            | 14850     |\n",
      "|    policy_gradient_loss | -2.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.78e+09  |\n",
      "---------------------------------------\n",
      "Episode 1218  finished with cumulative reward: -7419500.0 and \n",
      "with an average reward of: -2966.613354658137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3043717\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2967.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.28e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1487      |\n",
      "|    time_elapsed         | 40937     |\n",
      "|    total_timesteps      | 3045376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0371    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.39e+09  |\n",
      "|    n_updates            | 14860     |\n",
      "|    policy_gradient_loss | -2.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.42e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1219  finished with cumulative reward: -9587000.0 and \n",
      "with an average reward of: -3833.266693322671\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3046218\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3834.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.31e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1488      |\n",
      "|    time_elapsed         | 40964     |\n",
      "|    total_timesteps      | 3047424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0249    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.73e+09  |\n",
      "|    n_updates            | 14870     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1220  finished with cumulative reward: -4232000.0 and \n",
      "with an average reward of: -1692.1231507397042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3048719\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1692.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.3e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1489      |\n",
      "|    time_elapsed         | 40992     |\n",
      "|    total_timesteps      | 3049472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0355    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.04e+10  |\n",
      "|    n_updates            | 14880     |\n",
      "|    policy_gradient_loss | -1.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.58e+10  |\n",
      "---------------------------------------\n",
      "Episode 1221  finished with cumulative reward: -8618000.0 and \n",
      "with an average reward of: -3445.8216713314673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3051220\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3447.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.33e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1490          |\n",
      "|    time_elapsed         | 41020         |\n",
      "|    total_timesteps      | 3051520       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4028427e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0347        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.35e+09      |\n",
      "|    n_updates            | 14890         |\n",
      "|    policy_gradient_loss | -7.93e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.54e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.33e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1491      |\n",
      "|    time_elapsed         | 41042     |\n",
      "|    total_timesteps      | 3053568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0329    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.7e+09   |\n",
      "|    n_updates            | 14900     |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.13e+10  |\n",
      "---------------------------------------\n",
      "Episode 1222  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3053721\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.31e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1492      |\n",
      "|    time_elapsed         | 41070     |\n",
      "|    total_timesteps      | 3055616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.23e+09  |\n",
      "|    n_updates            | 14910     |\n",
      "|    policy_gradient_loss | -1.98e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.97e+09  |\n",
      "---------------------------------------\n",
      "Episode 1223  finished with cumulative reward: -8949500.0 and \n",
      "with an average reward of: -3578.3686525389844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3056222\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3579.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1493      |\n",
      "|    time_elapsed         | 41098     |\n",
      "|    total_timesteps      | 3057664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0327    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.54e+09  |\n",
      "|    n_updates            | 14920     |\n",
      "|    policy_gradient_loss | -1.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 1224  finished with cumulative reward: -7317500.0 and \n",
      "with an average reward of: -2925.829668132747\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3058723\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2927.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1494      |\n",
      "|    time_elapsed         | 41126     |\n",
      "|    total_timesteps      | 3059712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0305    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.57e+10  |\n",
      "|    n_updates            | 14930     |\n",
      "|    policy_gradient_loss | -1.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.87e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1225  finished with cumulative reward: -6476000.0 and \n",
      "with an average reward of: -2589.3642542982807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3061224\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2590.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.26e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1495      |\n",
      "|    time_elapsed         | 41154     |\n",
      "|    total_timesteps      | 3061760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0377    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.54e+09  |\n",
      "|    n_updates            | 14940     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.89e+09  |\n",
      "---------------------------------------\n",
      "Episode 1226  finished with cumulative reward: -2523500.0 and \n",
      "with an average reward of: -1008.9964014394242\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3063725\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1009.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1496      |\n",
      "|    time_elapsed         | 41181     |\n",
      "|    total_timesteps      | 3063808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0298    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.93e+09  |\n",
      "|    n_updates            | 14950     |\n",
      "|    policy_gradient_loss | -2.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1e+10     |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1497      |\n",
      "|    time_elapsed         | 41202     |\n",
      "|    total_timesteps      | 3065856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0448    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.79e+09  |\n",
      "|    n_updates            | 14960     |\n",
      "|    policy_gradient_loss | -2.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.29e+09  |\n",
      "---------------------------------------\n",
      "Episode 1227  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3066226\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1498      |\n",
      "|    time_elapsed         | 41230     |\n",
      "|    total_timesteps      | 3067904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0348    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.14e+09  |\n",
      "|    n_updates            | 14970     |\n",
      "|    policy_gradient_loss | -1.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 1228  finished with cumulative reward: -8363000.0 and \n",
      "with an average reward of: -3343.862455017993\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3068727\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3345.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.27e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1499      |\n",
      "|    time_elapsed         | 41258     |\n",
      "|    total_timesteps      | 3069952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0313    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.1e+09   |\n",
      "|    n_updates            | 14980     |\n",
      "|    policy_gradient_loss | -2.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 1229  finished with cumulative reward: -16701500.0 and \n",
      "with an average reward of: -6677.928828468613\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3071228\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -6680.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1500      |\n",
      "|    time_elapsed         | 41286     |\n",
      "|    total_timesteps      | 3072000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0306    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.67e+09  |\n",
      "|    n_updates            | 14990     |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 1230  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3073729\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1501      |\n",
      "|    time_elapsed         | 41315     |\n",
      "|    total_timesteps      | 3074048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.027     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.57e+10  |\n",
      "|    n_updates            | 15000     |\n",
      "|    policy_gradient_loss | -2.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.98e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1502      |\n",
      "|    time_elapsed         | 41336     |\n",
      "|    total_timesteps      | 3076096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0322    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.35e+10  |\n",
      "|    n_updates            | 15010     |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.07e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1231  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3076230\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1503      |\n",
      "|    time_elapsed         | 41364     |\n",
      "|    total_timesteps      | 3078144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0381    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.66e+09  |\n",
      "|    n_updates            | 15020     |\n",
      "|    policy_gradient_loss | -2.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 1232  finished with cumulative reward: -5201000.0 and \n",
      "with an average reward of: -2079.5681727309075\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3078731\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2080.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1504      |\n",
      "|    time_elapsed         | 41393     |\n",
      "|    total_timesteps      | 3080192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0301    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.49e+09  |\n",
      "|    n_updates            | 15030     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.09e+09  |\n",
      "---------------------------------------\n",
      "Episode 1233  finished with cumulative reward: -7598000.0 and \n",
      "with an average reward of: -3037.984806077569\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3081232\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3039.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.45e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1505         |\n",
      "|    time_elapsed         | 41421        |\n",
      "|    total_timesteps      | 3082240      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0332       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.8e+09      |\n",
      "|    n_updates            | 15040        |\n",
      "|    policy_gradient_loss | -3.18e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.24e+10     |\n",
      "------------------------------------------\n",
      "Episode 1234  finished with cumulative reward: -4436000.0 and \n",
      "with an average reward of: -1773.690523790484\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3083733\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1774.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.44e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1506          |\n",
      "|    time_elapsed         | 41449         |\n",
      "|    total_timesteps      | 3084288       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.2014214e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0249        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.05e+09      |\n",
      "|    n_updates            | 15050         |\n",
      "|    policy_gradient_loss | -6.58e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.43e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1235  finished with cumulative reward: -2498000.0 and \n",
      "with an average reward of: -998.8004798080767\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3086234\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -999.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1507      |\n",
      "|    time_elapsed         | 41478     |\n",
      "|    total_timesteps      | 3086336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0371    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.68e+09  |\n",
      "|    n_updates            | 15060     |\n",
      "|    policy_gradient_loss | -2.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.47e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1508      |\n",
      "|    time_elapsed         | 41498     |\n",
      "|    total_timesteps      | 3088384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0347    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.97e+09  |\n",
      "|    n_updates            | 15070     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.18e+09  |\n",
      "---------------------------------------\n",
      "Episode 1236  finished with cumulative reward: -4971500.0 and \n",
      "with an average reward of: -1987.8048780487804\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3088735\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1988.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1509      |\n",
      "|    time_elapsed         | 41526     |\n",
      "|    total_timesteps      | 3090432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0299    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.18e+09  |\n",
      "|    n_updates            | 15080     |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1237  finished with cumulative reward: -4869500.0 and \n",
      "with an average reward of: -1947.0211915233906\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3091236\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1947.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1510      |\n",
      "|    time_elapsed         | 41554     |\n",
      "|    total_timesteps      | 3092480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0241    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.85e+09  |\n",
      "|    n_updates            | 15090     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.21e+09  |\n",
      "---------------------------------------\n",
      "Episode 1238  finished with cumulative reward: -6884000.0 and \n",
      "with an average reward of: -2752.49900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3093737\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2753.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1511      |\n",
      "|    time_elapsed         | 41582     |\n",
      "|    total_timesteps      | 3094528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0303    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.04e+09  |\n",
      "|    n_updates            | 15100     |\n",
      "|    policy_gradient_loss | -2.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.85e+10  |\n",
      "---------------------------------------\n",
      "Episode 1239  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3096238\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1512      |\n",
      "|    time_elapsed         | 41611     |\n",
      "|    total_timesteps      | 3096576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0408    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.79e+09  |\n",
      "|    n_updates            | 15110     |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.55e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1513      |\n",
      "|    time_elapsed         | 41632     |\n",
      "|    total_timesteps      | 3098624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0235    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.05e+10  |\n",
      "|    n_updates            | 15120     |\n",
      "|    policy_gradient_loss | -2.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 1240  finished with cumulative reward: -8745500.0 and \n",
      "with an average reward of: -3496.8012794882047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3098739\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3498.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.45e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1514      |\n",
      "|    time_elapsed         | 41660     |\n",
      "|    total_timesteps      | 3100672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0353    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.5e+09   |\n",
      "|    n_updates            | 15130     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.29e+10  |\n",
      "---------------------------------------\n",
      "Episode 1241  finished with cumulative reward: -5022500.0 and \n",
      "with an average reward of: -2008.1967213114754\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3101240\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2009.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.41e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1515         |\n",
      "|    time_elapsed         | 41688        |\n",
      "|    total_timesteps      | 3102720      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0398       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.15e+09     |\n",
      "|    n_updates            | 15140        |\n",
      "|    policy_gradient_loss | -3.36e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.72e+09     |\n",
      "------------------------------------------\n",
      "Episode 1242  finished with cumulative reward: -5634500.0 and \n",
      "with an average reward of: -2252.8988404638144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3103741\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2253.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1516      |\n",
      "|    time_elapsed         | 41716     |\n",
      "|    total_timesteps      | 3104768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0279    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.25e+09  |\n",
      "|    n_updates            | 15150     |\n",
      "|    policy_gradient_loss | -2.75e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.77e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1243  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3106242\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.39e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1517      |\n",
      "|    time_elapsed         | 41744     |\n",
      "|    total_timesteps      | 3106816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0344    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.66e+09  |\n",
      "|    n_updates            | 15160     |\n",
      "|    policy_gradient_loss | -5.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.89e+09  |\n",
      "---------------------------------------\n",
      "Episode 1244  finished with cumulative reward: -4971500.0 and \n",
      "with an average reward of: -1987.8048780487804\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3108743\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1988.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1518      |\n",
      "|    time_elapsed         | 41773     |\n",
      "|    total_timesteps      | 3108864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0318    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.98e+09  |\n",
      "|    n_updates            | 15170     |\n",
      "|    policy_gradient_loss | -3.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1519      |\n",
      "|    time_elapsed         | 41794     |\n",
      "|    total_timesteps      | 3110912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.79e+09  |\n",
      "|    n_updates            | 15180     |\n",
      "|    policy_gradient_loss | -5.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.4e+10   |\n",
      "---------------------------------------\n",
      "Episode 1245  finished with cumulative reward: -2625500.0 and \n",
      "with an average reward of: -1049.7800879648141\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3111244\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1050.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.41e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1520      |\n",
      "|    time_elapsed         | 41823     |\n",
      "|    total_timesteps      | 3112960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0384    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.85e+09  |\n",
      "|    n_updates            | 15190     |\n",
      "|    policy_gradient_loss | -2.57e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.16e+09  |\n",
      "---------------------------------------\n",
      "Episode 1246  finished with cumulative reward: -1886000.0 and \n",
      "with an average reward of: -754.0983606557377\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3113745\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -754.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1521      |\n",
      "|    time_elapsed         | 41850     |\n",
      "|    total_timesteps      | 3115008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0532    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.05e+09  |\n",
      "|    n_updates            | 15200     |\n",
      "|    policy_gradient_loss | -3.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.19e+09  |\n",
      "---------------------------------------\n",
      "Episode 1247  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3116246\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.45e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1522          |\n",
      "|    time_elapsed         | 41878         |\n",
      "|    total_timesteps      | 3117056       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0365        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.91e+09      |\n",
      "|    n_updates            | 15210         |\n",
      "|    policy_gradient_loss | -4.73e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.15e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1248  finished with cumulative reward: -6731000.0 and \n",
      "with an average reward of: -2691.3234706117555\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3118747\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2692.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1523      |\n",
      "|    time_elapsed         | 41906     |\n",
      "|    total_timesteps      | 3119104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0389    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.59e+09  |\n",
      "|    n_updates            | 15220     |\n",
      "|    policy_gradient_loss | -1.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.69e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1524      |\n",
      "|    time_elapsed         | 41926     |\n",
      "|    total_timesteps      | 3121152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0305    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.58e+09  |\n",
      "|    n_updates            | 15230     |\n",
      "|    policy_gradient_loss | -1.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2e+10     |\n",
      "---------------------------------------\n",
      "Episode 1249  finished with cumulative reward: -6374000.0 and \n",
      "with an average reward of: -2548.580567772891\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3121248\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2549.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1525      |\n",
      "|    time_elapsed         | 41954     |\n",
      "|    total_timesteps      | 3123200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0321    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.73e+09  |\n",
      "|    n_updates            | 15240     |\n",
      "|    policy_gradient_loss | -9.58e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.07e+10  |\n",
      "---------------------------------------\n",
      "Episode 1250  finished with cumulative reward: -2676500.0 and \n",
      "with an average reward of: -1070.171931227509\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3123749\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1070.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.46e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1526          |\n",
      "|    time_elapsed         | 41982         |\n",
      "|    total_timesteps      | 3125248       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0422        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.06e+08      |\n",
      "|    n_updates            | 15250         |\n",
      "|    policy_gradient_loss | -9.24e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.01e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1251  finished with cumulative reward: -4614500.0 and \n",
      "with an average reward of: -1845.061975209916\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3126250\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1845.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1527      |\n",
      "|    time_elapsed         | 42011     |\n",
      "|    total_timesteps      | 3127296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0397    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.99e+09  |\n",
      "|    n_updates            | 15260     |\n",
      "|    policy_gradient_loss | -1.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.47e+09  |\n",
      "---------------------------------------\n",
      "Episode 1252  finished with cumulative reward: -4130000.0 and \n",
      "with an average reward of: -1651.3394642143144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3128751\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1652.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.42e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1528      |\n",
      "|    time_elapsed         | 42038     |\n",
      "|    total_timesteps      | 3129344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0269    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.09e+09  |\n",
      "|    n_updates            | 15270     |\n",
      "|    policy_gradient_loss | -1.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.73e+10  |\n",
      "---------------------------------------\n",
      "Episode 1253  finished with cumulative reward: -11168000.0 and \n",
      "with an average reward of: -4465.413834466213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3131252\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4467.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1529      |\n",
      "|    time_elapsed         | 42066     |\n",
      "|    total_timesteps      | 3131392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.27e+09  |\n",
      "|    n_updates            | 15280     |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.96e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1530      |\n",
      "|    time_elapsed         | 42087     |\n",
      "|    total_timesteps      | 3133440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0243    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.28e+10  |\n",
      "|    n_updates            | 15290     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 1254  finished with cumulative reward: -14712500.0 and \n",
      "with an average reward of: -5882.646941223511\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3133753\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5885.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1531      |\n",
      "|    time_elapsed         | 42115     |\n",
      "|    total_timesteps      | 3135488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0261    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.42e+09  |\n",
      "|    n_updates            | 15300     |\n",
      "|    policy_gradient_loss | -7.74e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.83e+10  |\n",
      "---------------------------------------\n",
      "Episode 1255  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3136254\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1532      |\n",
      "|    time_elapsed         | 42143     |\n",
      "|    total_timesteps      | 3137536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0301    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.48e+10  |\n",
      "|    n_updates            | 15310     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 1256  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3138755\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.54e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1533      |\n",
      "|    time_elapsed         | 42171     |\n",
      "|    total_timesteps      | 3139584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0303    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.32e+10  |\n",
      "|    n_updates            | 15320     |\n",
      "|    policy_gradient_loss | -2.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "Episode 1257  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3141256\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1534      |\n",
      "|    time_elapsed         | 42199     |\n",
      "|    total_timesteps      | 3141632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0331    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.11e+09  |\n",
      "|    n_updates            | 15330     |\n",
      "|    policy_gradient_loss | -1.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.26e+09  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.55e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1535         |\n",
      "|    time_elapsed         | 42221        |\n",
      "|    total_timesteps      | 3143680      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0464       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.45e+09     |\n",
      "|    n_updates            | 15340        |\n",
      "|    policy_gradient_loss | -2.76e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.6e+09      |\n",
      "------------------------------------------\n",
      "Episode 1258  finished with cumulative reward: -3875000.0 and \n",
      "with an average reward of: -1549.3802479008398\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3143757\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1550.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1536      |\n",
      "|    time_elapsed         | 42249     |\n",
      "|    total_timesteps      | 3145728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0402    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.81e+09  |\n",
      "|    n_updates            | 15350     |\n",
      "|    policy_gradient_loss | -3.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.8e+09   |\n",
      "---------------------------------------\n",
      "Episode 1259  finished with cumulative reward: -3492500.0 and \n",
      "with an average reward of: -1396.4414234306278\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3146258\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1397.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1537      |\n",
      "|    time_elapsed         | 42278     |\n",
      "|    total_timesteps      | 3147776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0362    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+09  |\n",
      "|    n_updates            | 15360     |\n",
      "|    policy_gradient_loss | -2.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.65e+09  |\n",
      "---------------------------------------\n",
      "Episode 1260  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3148759\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1538      |\n",
      "|    time_elapsed         | 42306     |\n",
      "|    total_timesteps      | 3149824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0299    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.35e+09  |\n",
      "|    n_updates            | 15370     |\n",
      "|    policy_gradient_loss | -1.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 1261  finished with cumulative reward: -6476000.0 and \n",
      "with an average reward of: -2589.3642542982807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3151260\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2590.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1539      |\n",
      "|    time_elapsed         | 42334     |\n",
      "|    total_timesteps      | 3151872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0352    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.05e+09  |\n",
      "|    n_updates            | 15380     |\n",
      "|    policy_gradient_loss | -1.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.48e+09  |\n",
      "---------------------------------------\n",
      "Episode 1262  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3153761\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1540      |\n",
      "|    time_elapsed         | 42362     |\n",
      "|    total_timesteps      | 3153920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0337    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.68e+09  |\n",
      "|    n_updates            | 15390     |\n",
      "|    policy_gradient_loss | -8.94e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.11e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.57e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1541         |\n",
      "|    time_elapsed         | 42382        |\n",
      "|    total_timesteps      | 3155968      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0436       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.38e+09     |\n",
      "|    n_updates            | 15400        |\n",
      "|    policy_gradient_loss | -5.21e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.34e+09     |\n",
      "------------------------------------------\n",
      "Episode 1263  finished with cumulative reward: -1988000.0 and \n",
      "with an average reward of: -794.8820471811275\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3156262\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -795.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1542      |\n",
      "|    time_elapsed         | 42410     |\n",
      "|    total_timesteps      | 3158016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0365    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.24e+09  |\n",
      "|    n_updates            | 15410     |\n",
      "|    policy_gradient_loss | -1.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.42e+09  |\n",
      "---------------------------------------\n",
      "Episode 1264  finished with cumulative reward: -8057000.0 and \n",
      "with an average reward of: -3221.5113954418234\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3158763\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3222.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1543      |\n",
      "|    time_elapsed         | 42438     |\n",
      "|    total_timesteps      | 3160064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0287    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.08e+10  |\n",
      "|    n_updates            | 15420     |\n",
      "|    policy_gradient_loss | -3.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 1265  finished with cumulative reward: -5813000.0 and \n",
      "with an average reward of: -2324.2702918832465\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3161264\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2325.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1544      |\n",
      "|    time_elapsed         | 42467     |\n",
      "|    total_timesteps      | 3162112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0285    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.21e+10  |\n",
      "|    n_updates            | 15430     |\n",
      "|    policy_gradient_loss | -1.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.99e+10  |\n",
      "---------------------------------------\n",
      "Episode 1266  finished with cumulative reward: -6144500.0 and \n",
      "with an average reward of: -2456.8172730907636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3163765\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2457.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1545      |\n",
      "|    time_elapsed         | 42495     |\n",
      "|    total_timesteps      | 3164160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0298    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.22e+09  |\n",
      "|    n_updates            | 15440     |\n",
      "|    policy_gradient_loss | -1.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.07e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1546      |\n",
      "|    time_elapsed         | 42515     |\n",
      "|    total_timesteps      | 3166208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.72e+09  |\n",
      "|    n_updates            | 15450     |\n",
      "|    policy_gradient_loss | -1.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.07e+10  |\n",
      "---------------------------------------\n",
      "Episode 1267  finished with cumulative reward: -6884000.0 and \n",
      "with an average reward of: -2752.49900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3166266\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2753.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.63e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1547          |\n",
      "|    time_elapsed         | 42543         |\n",
      "|    total_timesteps      | 3168256       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0323        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.2e+09       |\n",
      "|    n_updates            | 15460         |\n",
      "|    policy_gradient_loss | -4.63e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.34e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1268  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3168767\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1548      |\n",
      "|    time_elapsed         | 42571     |\n",
      "|    total_timesteps      | 3170304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.43e+09  |\n",
      "|    n_updates            | 15470     |\n",
      "|    policy_gradient_loss | -1.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.3e+09   |\n",
      "---------------------------------------\n",
      "Episode 1269  finished with cumulative reward: -4385000.0 and \n",
      "with an average reward of: -1753.298680527789\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3171268\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1754.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.57e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1549         |\n",
      "|    time_elapsed         | 42599        |\n",
      "|    total_timesteps      | 3172352      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0441       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.1e+08      |\n",
      "|    n_updates            | 15480        |\n",
      "|    policy_gradient_loss | -2.96e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.2e+09      |\n",
      "------------------------------------------\n",
      "Episode 1270  finished with cumulative reward: -4155500.0 and \n",
      "with an average reward of: -1661.5353858456617\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3173769\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1662.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1550      |\n",
      "|    time_elapsed         | 42626     |\n",
      "|    total_timesteps      | 3174400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.07e+09  |\n",
      "|    n_updates            | 15490     |\n",
      "|    policy_gradient_loss | -6.98e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 1271  finished with cumulative reward: -6501500.0 and \n",
      "with an average reward of: -2599.5601759296283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3176270\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2600.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1551         |\n",
      "|    time_elapsed         | 42654        |\n",
      "|    total_timesteps      | 3176448      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0255       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.67e+09     |\n",
      "|    n_updates            | 15500        |\n",
      "|    policy_gradient_loss | -1.8e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.11e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1552         |\n",
      "|    time_elapsed         | 42675        |\n",
      "|    total_timesteps      | 3178496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.04         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.17e+09     |\n",
      "|    n_updates            | 15510        |\n",
      "|    policy_gradient_loss | -3.65e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.33e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1272  finished with cumulative reward: -2039000.0 and \n",
      "with an average reward of: -815.2738904438224\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3178771\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -815.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1553      |\n",
      "|    time_elapsed         | 42703     |\n",
      "|    total_timesteps      | 3180544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.036     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.11e+09  |\n",
      "|    n_updates            | 15520     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.22e+09  |\n",
      "---------------------------------------\n",
      "Episode 1273  finished with cumulative reward: -6986000.0 and \n",
      "with an average reward of: -2793.28268692523\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3181272\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2794.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1554      |\n",
      "|    time_elapsed         | 42731     |\n",
      "|    total_timesteps      | 3182592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0393    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.55e+09  |\n",
      "|    n_updates            | 15530     |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.78e+09  |\n",
      "---------------------------------------\n",
      "Episode 1274  finished with cumulative reward: -5583500.0 and \n",
      "with an average reward of: -2232.5069972011197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3183773\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2233.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1555      |\n",
      "|    time_elapsed         | 42759     |\n",
      "|    total_timesteps      | 3184640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0297    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.25e+09  |\n",
      "|    n_updates            | 15540     |\n",
      "|    policy_gradient_loss | -2.5e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.83e+10  |\n",
      "---------------------------------------\n",
      "Episode 1275  finished with cumulative reward: -7776500.0 and \n",
      "with an average reward of: -3109.356257497001\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3186274\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3110.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1556      |\n",
      "|    time_elapsed         | 42787     |\n",
      "|    total_timesteps      | 3186688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0405    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.22e+09  |\n",
      "|    n_updates            | 15550     |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.08e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1557      |\n",
      "|    time_elapsed         | 42808     |\n",
      "|    total_timesteps      | 3188736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0305    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.85e+09  |\n",
      "|    n_updates            | 15560     |\n",
      "|    policy_gradient_loss | -1.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.87e+10  |\n",
      "---------------------------------------\n",
      "Episode 1276  finished with cumulative reward: -7088000.0 and \n",
      "with an average reward of: -2834.0663734506197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3188775\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2835.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1558      |\n",
      "|    time_elapsed         | 42837     |\n",
      "|    total_timesteps      | 3190784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0197    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.61e+10  |\n",
      "|    n_updates            | 15570     |\n",
      "|    policy_gradient_loss | -4.8e-07  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.86e+10  |\n",
      "---------------------------------------\n",
      "Episode 1277  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3191276\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1559      |\n",
      "|    time_elapsed         | 42865     |\n",
      "|    total_timesteps      | 3192832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0366    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.87e+09  |\n",
      "|    n_updates            | 15580     |\n",
      "|    policy_gradient_loss | -1.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.24e+09  |\n",
      "---------------------------------------\n",
      "Episode 1278  finished with cumulative reward: -6272000.0 and \n",
      "with an average reward of: -2507.796881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3193777\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2508.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.74e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1560      |\n",
      "|    time_elapsed         | 42893     |\n",
      "|    total_timesteps      | 3194880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.61e+09  |\n",
      "|    n_updates            | 15590     |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.1e+09   |\n",
      "---------------------------------------\n",
      "Episode 1279  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3196278\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1561          |\n",
      "|    time_elapsed         | 42922         |\n",
      "|    total_timesteps      | 3196928       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2386895e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0304        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.02e+10      |\n",
      "|    n_updates            | 15600         |\n",
      "|    policy_gradient_loss | -7.2e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.32e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1280  finished with cumulative reward: -5150000.0 and \n",
      "with an average reward of: -2059.1763294682128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3198779\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2060.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1562      |\n",
      "|    time_elapsed         | 42950     |\n",
      "|    total_timesteps      | 3198976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0386    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.37e+09  |\n",
      "|    n_updates            | 15610     |\n",
      "|    policy_gradient_loss | -9.98e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.65e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1563      |\n",
      "|    time_elapsed         | 42971     |\n",
      "|    total_timesteps      | 3201024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.91e+09  |\n",
      "|    n_updates            | 15620     |\n",
      "|    policy_gradient_loss | -1.03e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.54e+09  |\n",
      "---------------------------------------\n",
      "Episode 1281  finished with cumulative reward: -6552500.0 and \n",
      "with an average reward of: -2619.952019192323\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3201280\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2621.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1564      |\n",
      "|    time_elapsed         | 42999     |\n",
      "|    total_timesteps      | 3203072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0311    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.36e+09  |\n",
      "|    n_updates            | 15630     |\n",
      "|    policy_gradient_loss | -3.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 1282  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3203781\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.6e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1565          |\n",
      "|    time_elapsed         | 43027         |\n",
      "|    total_timesteps      | 3205120       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.7066562e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0461        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.92e+09      |\n",
      "|    n_updates            | 15640         |\n",
      "|    policy_gradient_loss | -7.68e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.62e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1283  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3206282\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1566      |\n",
      "|    time_elapsed         | 43055     |\n",
      "|    total_timesteps      | 3207168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0337    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.57e+09  |\n",
      "|    n_updates            | 15650     |\n",
      "|    policy_gradient_loss | -2.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 1284  finished with cumulative reward: -5915000.0 and \n",
      "with an average reward of: -2365.0539784086363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3208783\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2366.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1567      |\n",
      "|    time_elapsed         | 43082     |\n",
      "|    total_timesteps      | 3209216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0433    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.64e+09  |\n",
      "|    n_updates            | 15660     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.25e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1568      |\n",
      "|    time_elapsed         | 43103     |\n",
      "|    total_timesteps      | 3211264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0328    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.28e+09  |\n",
      "|    n_updates            | 15670     |\n",
      "|    policy_gradient_loss | -1.57e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.49e+10  |\n",
      "---------------------------------------\n",
      "Episode 1285  finished with cumulative reward: -4283000.0 and \n",
      "with an average reward of: -1712.5149940023991\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3211284\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1713.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1569      |\n",
      "|    time_elapsed         | 43131     |\n",
      "|    total_timesteps      | 3213312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0263    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.75e+09  |\n",
      "|    n_updates            | 15680     |\n",
      "|    policy_gradient_loss | -9.33e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.21e+10  |\n",
      "---------------------------------------\n",
      "Episode 1286  finished with cumulative reward: -5456000.0 and \n",
      "with an average reward of: -2181.5273890443823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3213785\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2182.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1570      |\n",
      "|    time_elapsed         | 43158     |\n",
      "|    total_timesteps      | 3215360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0251    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.27e+09  |\n",
      "|    n_updates            | 15690     |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 1287  finished with cumulative reward: 154000.0 and \n",
      "with an average reward of: 61.575369852059175\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3216286\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: 61.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.56e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1571          |\n",
      "|    time_elapsed         | 43187         |\n",
      "|    total_timesteps      | 3217408       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.9849193e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0384        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.17e+09      |\n",
      "|    n_updates            | 15700         |\n",
      "|    policy_gradient_loss | -7.41e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.41e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1288  finished with cumulative reward: -7674500.0 and \n",
      "with an average reward of: -3068.572570971611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3218787\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3069.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1572      |\n",
      "|    time_elapsed         | 43215     |\n",
      "|    total_timesteps      | 3219456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0349    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.41e+09  |\n",
      "|    n_updates            | 15710     |\n",
      "|    policy_gradient_loss | -1.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 1289  finished with cumulative reward: -2192000.0 and \n",
      "with an average reward of: -876.4494202319072\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3221288\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -876.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1573      |\n",
      "|    time_elapsed         | 43243     |\n",
      "|    total_timesteps      | 3221504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0452    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.01e+09  |\n",
      "|    n_updates            | 15720     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.09e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1574      |\n",
      "|    time_elapsed         | 43264     |\n",
      "|    total_timesteps      | 3223552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0287    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.16e+08  |\n",
      "|    n_updates            | 15730     |\n",
      "|    policy_gradient_loss | -1.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.64e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1290  finished with cumulative reward: -10224500.0 and \n",
      "with an average reward of: -4088.1647341063576\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3223789\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4089.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1575      |\n",
      "|    time_elapsed         | 43293     |\n",
      "|    total_timesteps      | 3225600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0367    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.87e+09  |\n",
      "|    n_updates            | 15740     |\n",
      "|    policy_gradient_loss | -4.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 1291  finished with cumulative reward: -8312000.0 and \n",
      "with an average reward of: -3323.470611755298\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3226290\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3324.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1576      |\n",
      "|    time_elapsed         | 43321     |\n",
      "|    total_timesteps      | 3227648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0308    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.09e+09  |\n",
      "|    n_updates            | 15750     |\n",
      "|    policy_gradient_loss | -9.16e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.36e+10  |\n",
      "---------------------------------------\n",
      "Episode 1292  finished with cumulative reward: -8796500.0 and \n",
      "with an average reward of: -3517.1931227508994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3228791\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3518.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1577      |\n",
      "|    time_elapsed         | 43348     |\n",
      "|    total_timesteps      | 3229696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0413    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.68e+09  |\n",
      "|    n_updates            | 15760     |\n",
      "|    policy_gradient_loss | -5.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 1293  finished with cumulative reward: -5022500.0 and \n",
      "with an average reward of: -2008.1967213114754\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3231292\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2009.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1578      |\n",
      "|    time_elapsed         | 43376     |\n",
      "|    total_timesteps      | 3231744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.75e+10  |\n",
      "|    n_updates            | 15770     |\n",
      "|    policy_gradient_loss | -3.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.18e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1579      |\n",
      "|    time_elapsed         | 43397     |\n",
      "|    total_timesteps      | 3233792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0378    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.22e+09  |\n",
      "|    n_updates            | 15780     |\n",
      "|    policy_gradient_loss | -1.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.44e+09  |\n",
      "---------------------------------------\n",
      "Episode 1294  finished with cumulative reward: -7955000.0 and \n",
      "with an average reward of: -3180.7277089164336\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3233793\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3182.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.76e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1580      |\n",
      "|    time_elapsed         | 43426     |\n",
      "|    total_timesteps      | 3235840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.4e+09   |\n",
      "|    n_updates            | 15790     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 1295  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3236294\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1581      |\n",
      "|    time_elapsed         | 43454     |\n",
      "|    total_timesteps      | 3237888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0377    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.61e+09  |\n",
      "|    n_updates            | 15800     |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.63e+09  |\n",
      "---------------------------------------\n",
      "Episode 1296  finished with cumulative reward: -2115500.0 and \n",
      "with an average reward of: -845.8616553378648\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3238795\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -846.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1582      |\n",
      "|    time_elapsed         | 43482     |\n",
      "|    total_timesteps      | 3239936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.032     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.98e+09  |\n",
      "|    n_updates            | 15810     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 1297  finished with cumulative reward: -279500.0 and \n",
      "with an average reward of: -111.75529788084766\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3241296\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -111.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.58e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1583          |\n",
      "|    time_elapsed         | 43511         |\n",
      "|    total_timesteps      | 3241984       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.2386895e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0549        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.4e+09       |\n",
      "|    n_updates            | 15820         |\n",
      "|    policy_gradient_loss | -8.58e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.3e+09       |\n",
      "-------------------------------------------\n",
      "Episode 1298  finished with cumulative reward: -917000.0 and \n",
      "with an average reward of: -366.6533386645342\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3243797\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -366.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.5e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1584      |\n",
      "|    time_elapsed         | 43539     |\n",
      "|    total_timesteps      | 3244032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0454    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.36e+08  |\n",
      "|    n_updates            | 15830     |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.87e+09  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.5e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1585         |\n",
      "|    time_elapsed         | 43559        |\n",
      "|    total_timesteps      | 3246080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0419       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.44e+08     |\n",
      "|    n_updates            | 15840        |\n",
      "|    policy_gradient_loss | -3.16e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.55e+09     |\n",
      "------------------------------------------\n",
      "Episode 1299  finished with cumulative reward: -2906000.0 and \n",
      "with an average reward of: -1161.935225909636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3246298\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1162.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.43e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1586      |\n",
      "|    time_elapsed         | 43586     |\n",
      "|    total_timesteps      | 3248128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0317    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.99e+09  |\n",
      "|    n_updates            | 15850     |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 1300  finished with cumulative reward: -4002500.0 and \n",
      "with an average reward of: -1600.359856057577\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3248799\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1601.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.44e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1587      |\n",
      "|    time_elapsed         | 43614     |\n",
      "|    total_timesteps      | 3250176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.037     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.03e+09  |\n",
      "|    n_updates            | 15860     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.46e+09  |\n",
      "---------------------------------------\n",
      "Episode 1301  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3251300\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1588      |\n",
      "|    time_elapsed         | 43642     |\n",
      "|    total_timesteps      | 3252224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0359    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.2e+09   |\n",
      "|    n_updates            | 15870     |\n",
      "|    policy_gradient_loss | -9.92e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 1302  finished with cumulative reward: -5660000.0 and \n",
      "with an average reward of: -2263.094762095162\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3253801\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2264.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1589      |\n",
      "|    time_elapsed         | 43671     |\n",
      "|    total_timesteps      | 3254272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0333    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.16e+09  |\n",
      "|    n_updates            | 15880     |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 1303  finished with cumulative reward: -5864000.0 and \n",
      "with an average reward of: -2344.6621351459416\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3256302\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2345.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.52e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1590          |\n",
      "|    time_elapsed         | 43700         |\n",
      "|    total_timesteps      | 3256320       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0377        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.09e+09      |\n",
      "|    n_updates            | 15890         |\n",
      "|    policy_gradient_loss | -6.4e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.33e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1591      |\n",
      "|    time_elapsed         | 43721     |\n",
      "|    total_timesteps      | 3258368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.038     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.34e+09  |\n",
      "|    n_updates            | 15900     |\n",
      "|    policy_gradient_loss | -1.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.15e+09  |\n",
      "---------------------------------------\n",
      "Episode 1304  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3258803\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1592      |\n",
      "|    time_elapsed         | 43750     |\n",
      "|    total_timesteps      | 3260416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0339    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.97e+09  |\n",
      "|    n_updates            | 15910     |\n",
      "|    policy_gradient_loss | -2.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.06e+09  |\n",
      "---------------------------------------\n",
      "Episode 1305  finished with cumulative reward: -7802000.0 and \n",
      "with an average reward of: -3119.5521791283486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3261304\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3120.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.54e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1593      |\n",
      "|    time_elapsed         | 43778     |\n",
      "|    total_timesteps      | 3262464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0301    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.44e+10  |\n",
      "|    n_updates            | 15920     |\n",
      "|    policy_gradient_loss | -1.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+10  |\n",
      "---------------------------------------\n",
      "Episode 1306  finished with cumulative reward: -8720000.0 and \n",
      "with an average reward of: -3486.605357856857\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3263805\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3488.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.55e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1594      |\n",
      "|    time_elapsed         | 43806     |\n",
      "|    total_timesteps      | 3264512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0318    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+10  |\n",
      "|    n_updates            | 15930     |\n",
      "|    policy_gradient_loss | -2.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 1307  finished with cumulative reward: -7751000.0 and \n",
      "with an average reward of: -3099.160335865654\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3266306\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3100.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1595      |\n",
      "|    time_elapsed         | 43834     |\n",
      "|    total_timesteps      | 3266560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0308    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.83e+09  |\n",
      "|    n_updates            | 15940     |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.52e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1596         |\n",
      "|    time_elapsed         | 43855        |\n",
      "|    total_timesteps      | 3268608      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0302       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.72e+09     |\n",
      "|    n_updates            | 15950        |\n",
      "|    policy_gradient_loss | -2.34e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.41e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1308  finished with cumulative reward: -7292000.0 and \n",
      "with an average reward of: -2915.6337465013994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3268807\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2916.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1597      |\n",
      "|    time_elapsed         | 43883     |\n",
      "|    total_timesteps      | 3270656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.17e+09  |\n",
      "|    n_updates            | 15960     |\n",
      "|    policy_gradient_loss | -3.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 1309  finished with cumulative reward: -2523500.0 and \n",
      "with an average reward of: -1008.9964014394242\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3271308\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1009.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1598      |\n",
      "|    time_elapsed         | 43912     |\n",
      "|    total_timesteps      | 3272704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0426    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.3e+09   |\n",
      "|    n_updates            | 15970     |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.97e+09  |\n",
      "---------------------------------------\n",
      "Episode 1310  finished with cumulative reward: -3543500.0 and \n",
      "with an average reward of: -1416.8332666933227\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3273809\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1417.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.47e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1599      |\n",
      "|    time_elapsed         | 43940     |\n",
      "|    total_timesteps      | 3274752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0361    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.68e+09  |\n",
      "|    n_updates            | 15980     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 1311  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3276310\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.48e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1600      |\n",
      "|    time_elapsed         | 43969     |\n",
      "|    total_timesteps      | 3276800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0349    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.55e+09  |\n",
      "|    n_updates            | 15990     |\n",
      "|    policy_gradient_loss | -9.64e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.81e+09  |\n",
      "---------------------------------------\n",
      "Episode 1312  finished with cumulative reward: -9383000.0 and \n",
      "with an average reward of: -3751.6993202718913\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3278811\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3753.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1601      |\n",
      "|    time_elapsed         | 43997     |\n",
      "|    total_timesteps      | 3278848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0321    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.02e+09  |\n",
      "|    n_updates            | 16000     |\n",
      "|    policy_gradient_loss | -4.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.81e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.52e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1602      |\n",
      "|    time_elapsed         | 44018     |\n",
      "|    total_timesteps      | 3280896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0286    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.04e+09  |\n",
      "|    n_updates            | 16010     |\n",
      "|    policy_gradient_loss | -3.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.97e+10  |\n",
      "---------------------------------------\n",
      "Episode 1313  finished with cumulative reward: -6858500.0 and \n",
      "with an average reward of: -2742.3030787684925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3281312\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2743.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1603      |\n",
      "|    time_elapsed         | 44046     |\n",
      "|    total_timesteps      | 3282944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0351    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.44e+09  |\n",
      "|    n_updates            | 16020     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 1314  finished with cumulative reward: -1146500.0 and \n",
      "with an average reward of: -458.41663334666134\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3283813\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -458.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.55e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1604         |\n",
      "|    time_elapsed         | 44074        |\n",
      "|    total_timesteps      | 3284992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0471       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.11e+09     |\n",
      "|    n_updates            | 16030        |\n",
      "|    policy_gradient_loss | -3.38e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.2e+09      |\n",
      "------------------------------------------\n",
      "Episode 1315  finished with cumulative reward: -5685500.0 and \n",
      "with an average reward of: -2273.2906837265095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3286314\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2274.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.57e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1605          |\n",
      "|    time_elapsed         | 44101         |\n",
      "|    total_timesteps      | 3287040       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 8.1490725e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.044         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.1e+09       |\n",
      "|    n_updates            | 16040         |\n",
      "|    policy_gradient_loss | -8.21e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.11e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1316  finished with cumulative reward: -8949500.0 and \n",
      "with an average reward of: -3578.3686525389844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3288815\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3579.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1606      |\n",
      "|    time_elapsed         | 44129     |\n",
      "|    total_timesteps      | 3289088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0322    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.86e+09  |\n",
      "|    n_updates            | 16050     |\n",
      "|    policy_gradient_loss | -3.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.9e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1607      |\n",
      "|    time_elapsed         | 44150     |\n",
      "|    total_timesteps      | 3291136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0342    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.1e+09   |\n",
      "|    n_updates            | 16060     |\n",
      "|    policy_gradient_loss | -3.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 1317  finished with cumulative reward: -6450500.0 and \n",
      "with an average reward of: -2579.168332666933\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3291316\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2580.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1608      |\n",
      "|    time_elapsed         | 44179     |\n",
      "|    total_timesteps      | 3293184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0335    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.09e+09  |\n",
      "|    n_updates            | 16070     |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 1318  finished with cumulative reward: -2880500.0 and \n",
      "with an average reward of: -1151.7393042782887\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3293817\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1152.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.59e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1609          |\n",
      "|    time_elapsed         | 44207         |\n",
      "|    total_timesteps      | 3295232       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0429        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.59e+09      |\n",
      "|    n_updates            | 16080         |\n",
      "|    policy_gradient_loss | -2.71e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.75e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1319  finished with cumulative reward: -3620000.0 and \n",
      "with an average reward of: -1447.421031587365\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3296318\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1448.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.53e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1610          |\n",
      "|    time_elapsed         | 44236         |\n",
      "|    total_timesteps      | 3297280       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4028427e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0347        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.28e+08      |\n",
      "|    n_updates            | 16090         |\n",
      "|    policy_gradient_loss | -3.72e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.02e+09      |\n",
      "-------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1320  finished with cumulative reward: -9102500.0 and \n",
      "with an average reward of: -3639.5441823270694\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3298819\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3641.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1611      |\n",
      "|    time_elapsed         | 44265     |\n",
      "|    total_timesteps      | 3299328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0366    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.28e+09  |\n",
      "|    n_updates            | 16100     |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.56e+09  |\n",
      "---------------------------------------\n",
      "Episode 1321  finished with cumulative reward: -8108000.0 and \n",
      "with an average reward of: -3241.903238704518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3301320\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3243.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1612      |\n",
      "|    time_elapsed         | 44292     |\n",
      "|    total_timesteps      | 3301376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.4e+10   |\n",
      "|    n_updates            | 16110     |\n",
      "|    policy_gradient_loss | -9.59e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.35e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1613      |\n",
      "|    time_elapsed         | 44313     |\n",
      "|    total_timesteps      | 3303424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0276    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.43e+09  |\n",
      "|    n_updates            | 16120     |\n",
      "|    policy_gradient_loss | -5.83e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.78e+10  |\n",
      "---------------------------------------\n",
      "Episode 1322  finished with cumulative reward: -8490500.0 and \n",
      "with an average reward of: -3394.84206317473\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3303821\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3396.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1614      |\n",
      "|    time_elapsed         | 44341     |\n",
      "|    total_timesteps      | 3305472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0212    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.87e+09  |\n",
      "|    n_updates            | 16130     |\n",
      "|    policy_gradient_loss | -1.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 1323  finished with cumulative reward: -10964000.0 and \n",
      "with an average reward of: -4383.846461415434\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3306322\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4385.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1615      |\n",
      "|    time_elapsed         | 44370     |\n",
      "|    total_timesteps      | 3307520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0271    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.1e+10   |\n",
      "|    n_updates            | 16140     |\n",
      "|    policy_gradient_loss | -1.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 1324  finished with cumulative reward: -3671000.0 and \n",
      "with an average reward of: -1467.8128748500599\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3308823\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1468.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1616      |\n",
      "|    time_elapsed         | 44399     |\n",
      "|    total_timesteps      | 3309568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0314    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.14e+10  |\n",
      "|    n_updates            | 16150     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n",
      "Episode 1325  finished with cumulative reward: -6272000.0 and \n",
      "with an average reward of: -2507.796881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3311324\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2508.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1617      |\n",
      "|    time_elapsed         | 44427     |\n",
      "|    total_timesteps      | 3311616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0308    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.8e+09   |\n",
      "|    n_updates            | 16160     |\n",
      "|    policy_gradient_loss | -1.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.42e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.61e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1618      |\n",
      "|    time_elapsed         | 44449     |\n",
      "|    total_timesteps      | 3313664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0367    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.13e+09  |\n",
      "|    n_updates            | 16170     |\n",
      "|    policy_gradient_loss | -1.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1326  finished with cumulative reward: -1044500.0 and \n",
      "with an average reward of: -417.6329468212715\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3313825\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -417.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.59e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1619          |\n",
      "|    time_elapsed         | 44478         |\n",
      "|    total_timesteps      | 3315712       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0399        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.3e+07       |\n",
      "|    n_updates            | 16180         |\n",
      "|    policy_gradient_loss | -3.13e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.4e+09       |\n",
      "-------------------------------------------\n",
      "Episode 1327  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3316326\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.6e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1620     |\n",
      "|    time_elapsed         | 44506    |\n",
      "|    total_timesteps      | 3317760  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0318   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 6.66e+09 |\n",
      "|    n_updates            | 16190    |\n",
      "|    policy_gradient_loss | -9.7e-07 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.25e+10 |\n",
      "--------------------------------------\n",
      "Episode 1328  finished with cumulative reward: -11270000.0 and \n",
      "with an average reward of: -4506.197520991604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3318827\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4508.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1621      |\n",
      "|    time_elapsed         | 44534     |\n",
      "|    total_timesteps      | 3319808   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.39e+09  |\n",
      "|    n_updates            | 16200     |\n",
      "|    policy_gradient_loss | -8.62e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.51e+10  |\n",
      "---------------------------------------\n",
      "Episode 1329  finished with cumulative reward: -9536000.0 and \n",
      "with an average reward of: -3812.874850059976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3321328\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3814.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1622      |\n",
      "|    time_elapsed         | 44562     |\n",
      "|    total_timesteps      | 3321856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.025     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.97e+09  |\n",
      "|    n_updates            | 16210     |\n",
      "|    policy_gradient_loss | -1.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 1330  finished with cumulative reward: -11091500.0 and \n",
      "with an average reward of: -4434.826069572171\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3323829\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4436.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1623      |\n",
      "|    time_elapsed         | 44590     |\n",
      "|    total_timesteps      | 3323904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0265    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.43e+09  |\n",
      "|    n_updates            | 16220     |\n",
      "|    policy_gradient_loss | -3.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.53e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1624      |\n",
      "|    time_elapsed         | 44611     |\n",
      "|    total_timesteps      | 3325952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0285    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.93e+09  |\n",
      "|    n_updates            | 16230     |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "Episode 1331  finished with cumulative reward: -2804000.0 and \n",
      "with an average reward of: -1121.1515393842462\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3326330\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1121.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.58e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1625         |\n",
      "|    time_elapsed         | 44640        |\n",
      "|    total_timesteps      | 3328000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0329       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.36e+09     |\n",
      "|    n_updates            | 16240        |\n",
      "|    policy_gradient_loss | -2.5e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.03e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1332  finished with cumulative reward: -6476000.0 and \n",
      "with an average reward of: -2589.3642542982807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3328831\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2590.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.6e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1626      |\n",
      "|    time_elapsed         | 44669     |\n",
      "|    total_timesteps      | 3330048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0357    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 16250     |\n",
      "|    policy_gradient_loss | -2.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "Episode 1333  finished with cumulative reward: -7011500.0 and \n",
      "with an average reward of: -2803.4786085565775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3331332\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2804.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1627      |\n",
      "|    time_elapsed         | 44697     |\n",
      "|    total_timesteps      | 3332096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0346    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.26e+09  |\n",
      "|    n_updates            | 16260     |\n",
      "|    policy_gradient_loss | -1.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 1334  finished with cumulative reward: -7394000.0 and \n",
      "with an average reward of: -2956.4174330267892\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3333833\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2957.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1628      |\n",
      "|    time_elapsed         | 44726     |\n",
      "|    total_timesteps      | 3334144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0315    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.01e+09  |\n",
      "|    n_updates            | 16270     |\n",
      "|    policy_gradient_loss | -1.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.2e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1629      |\n",
      "|    time_elapsed         | 44746     |\n",
      "|    total_timesteps      | 3336192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0278    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.39e+09  |\n",
      "|    n_updates            | 16280     |\n",
      "|    policy_gradient_loss | -1.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.04e+10  |\n",
      "---------------------------------------\n",
      "Episode 1335  finished with cumulative reward: -8414000.0 and \n",
      "with an average reward of: -3364.2542982806876\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3336334\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3365.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1630      |\n",
      "|    time_elapsed         | 44774     |\n",
      "|    total_timesteps      | 3338240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0323    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.13e+09  |\n",
      "|    n_updates            | 16290     |\n",
      "|    policy_gradient_loss | -2.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 1336  finished with cumulative reward: -2243000.0 and \n",
      "with an average reward of: -896.8412634946021\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3338835\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -897.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1631      |\n",
      "|    time_elapsed         | 44802     |\n",
      "|    total_timesteps      | 3340288   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0368    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.54e+09  |\n",
      "|    n_updates            | 16300     |\n",
      "|    policy_gradient_loss | -3.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.85e+09  |\n",
      "---------------------------------------\n",
      "Episode 1337  finished with cumulative reward: -5252000.0 and \n",
      "with an average reward of: -2099.9600159936026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3341336\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2100.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1632      |\n",
      "|    time_elapsed         | 44830     |\n",
      "|    total_timesteps      | 3342336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0359    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.1e+09   |\n",
      "|    n_updates            | 16310     |\n",
      "|    policy_gradient_loss | -1.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.14e+09  |\n",
      "---------------------------------------\n",
      "Episode 1338  finished with cumulative reward: -5430500.0 and \n",
      "with an average reward of: -2171.3314674130347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3343837\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2172.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1633      |\n",
      "|    time_elapsed         | 44859     |\n",
      "|    total_timesteps      | 3344384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0314    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.68e+09  |\n",
      "|    n_updates            | 16320     |\n",
      "|    policy_gradient_loss | -2.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.42e+10  |\n",
      "---------------------------------------\n",
      "Episode 1339  finished with cumulative reward: -8924000.0 and \n",
      "with an average reward of: -3568.172730907637\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3346338\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3569.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.7e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1634         |\n",
      "|    time_elapsed         | 44888        |\n",
      "|    total_timesteps      | 3346432      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0497       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.43e+09     |\n",
      "|    n_updates            | 16330        |\n",
      "|    policy_gradient_loss | -4.07e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.82e+09     |\n",
      "------------------------------------------\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.7e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1635     |\n",
      "|    time_elapsed         | 44909    |\n",
      "|    total_timesteps      | 3348480  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0296   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 6.83e+09 |\n",
      "|    n_updates            | 16340    |\n",
      "|    policy_gradient_loss | -2.7e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 2.55e+10 |\n",
      "--------------------------------------\n",
      "Episode 1340  finished with cumulative reward: -3849500.0 and \n",
      "with an average reward of: -1539.1843262694922\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3348839\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1539.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.65e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1636      |\n",
      "|    time_elapsed         | 44937     |\n",
      "|    total_timesteps      | 3350528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0319    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.44e+09  |\n",
      "|    n_updates            | 16350     |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.63e+09  |\n",
      "---------------------------------------\n",
      "Episode 1341  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3351340\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1637      |\n",
      "|    time_elapsed         | 44965     |\n",
      "|    total_timesteps      | 3352576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0274    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.7e+09   |\n",
      "|    n_updates            | 16360     |\n",
      "|    policy_gradient_loss | -1.5e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.63e+10  |\n",
      "---------------------------------------\n",
      "Episode 1342  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3353841\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1638      |\n",
      "|    time_elapsed         | 44993     |\n",
      "|    total_timesteps      | 3354624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0351    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.39e+09  |\n",
      "|    n_updates            | 16370     |\n",
      "|    policy_gradient_loss | -2.95e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.74e+09  |\n",
      "---------------------------------------\n",
      "Episode 1343  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3356342\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1639      |\n",
      "|    time_elapsed         | 45021     |\n",
      "|    total_timesteps      | 3356672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.026     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.87e+09  |\n",
      "|    n_updates            | 16380     |\n",
      "|    policy_gradient_loss | -3.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.83e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1640      |\n",
      "|    time_elapsed         | 45041     |\n",
      "|    total_timesteps      | 3358720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0406    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.86e+09  |\n",
      "|    n_updates            | 16390     |\n",
      "|    policy_gradient_loss | -1.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.02e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1344  finished with cumulative reward: -3441500.0 and \n",
      "with an average reward of: -1376.0495801679328\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3358843\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1376.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1641      |\n",
      "|    time_elapsed         | 45069     |\n",
      "|    total_timesteps      | 3360768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0324    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.48e+09  |\n",
      "|    n_updates            | 16400     |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.49e+09  |\n",
      "---------------------------------------\n",
      "Episode 1345  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3361344\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1642      |\n",
      "|    time_elapsed         | 45097     |\n",
      "|    total_timesteps      | 3362816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0279    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.67e+09  |\n",
      "|    n_updates            | 16410     |\n",
      "|    policy_gradient_loss | -7.52e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+10  |\n",
      "---------------------------------------\n",
      "Episode 1346  finished with cumulative reward: -6578000.0 and \n",
      "with an average reward of: -2630.1479408236705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3363845\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2631.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.77e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1643      |\n",
      "|    time_elapsed         | 45125     |\n",
      "|    total_timesteps      | 3364864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.038     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.19e+09  |\n",
      "|    n_updates            | 16420     |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 1347  finished with cumulative reward: -11958500.0 and \n",
      "with an average reward of: -4781.487405037985\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3366346\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4783.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1644      |\n",
      "|    time_elapsed         | 45153     |\n",
      "|    total_timesteps      | 3366912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.22e+10  |\n",
      "|    n_updates            | 16430     |\n",
      "|    policy_gradient_loss | -6.09e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 1348  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3368847\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1645      |\n",
      "|    time_elapsed         | 45181     |\n",
      "|    total_timesteps      | 3368960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0348    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.56e+09  |\n",
      "|    n_updates            | 16440     |\n",
      "|    policy_gradient_loss | -4.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.83e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1646         |\n",
      "|    time_elapsed         | 45203        |\n",
      "|    total_timesteps      | 3371008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0324       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.72e+09     |\n",
      "|    n_updates            | 16450        |\n",
      "|    policy_gradient_loss | -3.65e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.7e+10      |\n",
      "------------------------------------------\n",
      "Episode 1349  finished with cumulative reward: -7955000.0 and \n",
      "with an average reward of: -3180.7277089164336\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3371348\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3182.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1647      |\n",
      "|    time_elapsed         | 45231     |\n",
      "|    total_timesteps      | 3373056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0325    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.25e+09  |\n",
      "|    n_updates            | 16460     |\n",
      "|    policy_gradient_loss | -7.14e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1350  finished with cumulative reward: -6246500.0 and \n",
      "with an average reward of: -2497.6009596161534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3373849\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2498.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1648         |\n",
      "|    time_elapsed         | 45259        |\n",
      "|    total_timesteps      | 3375104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0357       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.47e+09     |\n",
      "|    n_updates            | 16470        |\n",
      "|    policy_gradient_loss | -2.68e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.51e+10     |\n",
      "------------------------------------------\n",
      "Episode 1351  finished with cumulative reward: -9587000.0 and \n",
      "with an average reward of: -3833.266693322671\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3376350\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3834.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1649      |\n",
      "|    time_elapsed         | 45287     |\n",
      "|    total_timesteps      | 3377152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0332    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.68e+09  |\n",
      "|    n_updates            | 16480     |\n",
      "|    policy_gradient_loss | -3.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "Episode 1352  finished with cumulative reward: -7164500.0 and \n",
      "with an average reward of: -2864.654138344662\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3378851\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2865.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1650      |\n",
      "|    time_elapsed         | 45314     |\n",
      "|    total_timesteps      | 3379200   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0305    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.68e+09  |\n",
      "|    n_updates            | 16490     |\n",
      "|    policy_gradient_loss | -2.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.96e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1651         |\n",
      "|    time_elapsed         | 45336        |\n",
      "|    total_timesteps      | 3381248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.895302e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0358       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.31e+09     |\n",
      "|    n_updates            | 16500        |\n",
      "|    policy_gradient_loss | -1.2e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.61e+10     |\n",
      "------------------------------------------\n",
      "Episode 1353  finished with cumulative reward: -5048000.0 and \n",
      "with an average reward of: -2018.392642942823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3381352\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2019.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1652      |\n",
      "|    time_elapsed         | 45365     |\n",
      "|    total_timesteps      | 3383296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0238    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+10  |\n",
      "|    n_updates            | 16510     |\n",
      "|    policy_gradient_loss | -1.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1354  finished with cumulative reward: -11397500.0 and \n",
      "with an average reward of: -4557.177129148341\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3383853\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4559.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1653      |\n",
      "|    time_elapsed         | 45393     |\n",
      "|    total_timesteps      | 3385344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0327    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.91e+09  |\n",
      "|    n_updates            | 16520     |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 1355  finished with cumulative reward: -6272000.0 and \n",
      "with an average reward of: -2507.796881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3386354\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2508.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1654      |\n",
      "|    time_elapsed         | 45421     |\n",
      "|    total_timesteps      | 3387392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0321    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.41e+09  |\n",
      "|    n_updates            | 16530     |\n",
      "|    policy_gradient_loss | -1.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.42e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1356  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3388855\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1655      |\n",
      "|    time_elapsed         | 45450     |\n",
      "|    total_timesteps      | 3389440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0272    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.86e+09  |\n",
      "|    n_updates            | 16540     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "Episode 1357  finished with cumulative reward: -7980500.0 and \n",
      "with an average reward of: -3190.9236305477807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3391356\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3192.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1656      |\n",
      "|    time_elapsed         | 45478     |\n",
      "|    total_timesteps      | 3391488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0401    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.34e+09  |\n",
      "|    n_updates            | 16550     |\n",
      "|    policy_gradient_loss | -2.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.3e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1657      |\n",
      "|    time_elapsed         | 45498     |\n",
      "|    total_timesteps      | 3393536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0268    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.47e+10  |\n",
      "|    n_updates            | 16560     |\n",
      "|    policy_gradient_loss | -2.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 1358  finished with cumulative reward: -1350500.0 and \n",
      "with an average reward of: -539.984006397441\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3393857\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -540.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1658      |\n",
      "|    time_elapsed         | 45527     |\n",
      "|    total_timesteps      | 3395584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0389    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.82e+08  |\n",
      "|    n_updates            | 16570     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.7e+09   |\n",
      "---------------------------------------\n",
      "Episode 1359  finished with cumulative reward: -9816500.0 and \n",
      "with an average reward of: -3925.0299880047983\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3396358\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3926.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1659      |\n",
      "|    time_elapsed         | 45554     |\n",
      "|    total_timesteps      | 3397632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0286    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.38e+09  |\n",
      "|    n_updates            | 16580     |\n",
      "|    policy_gradient_loss | -7.93e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.61e+10  |\n",
      "---------------------------------------\n",
      "Episode 1360  finished with cumulative reward: -5609000.0 and \n",
      "with an average reward of: -2242.702918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3398859\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2243.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.93e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1660          |\n",
      "|    time_elapsed         | 45583         |\n",
      "|    total_timesteps      | 3399680       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.4924597e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0359        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.86e+09      |\n",
      "|    n_updates            | 16590         |\n",
      "|    policy_gradient_loss | -6.04e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.52e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1361  finished with cumulative reward: -2549000.0 and \n",
      "with an average reward of: -1019.1923230707716\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3401360\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1019.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1661      |\n",
      "|    time_elapsed         | 45612     |\n",
      "|    total_timesteps      | 3401728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.29e+09  |\n",
      "|    n_updates            | 16600     |\n",
      "|    policy_gradient_loss | -1.13e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.77e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.89e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1662         |\n",
      "|    time_elapsed         | 45633        |\n",
      "|    total_timesteps      | 3403776      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0389       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.06e+09     |\n",
      "|    n_updates            | 16610        |\n",
      "|    policy_gradient_loss | -1.55e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 5.85e+09     |\n",
      "------------------------------------------\n",
      "Episode 1362  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3403861\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1663         |\n",
      "|    time_elapsed         | 45661        |\n",
      "|    total_timesteps      | 3405824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.313226e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.037        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.89e+09     |\n",
      "|    n_updates            | 16620        |\n",
      "|    policy_gradient_loss | -4.09e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.8e+09      |\n",
      "------------------------------------------\n",
      "Episode 1363  finished with cumulative reward: -12035000.0 and \n",
      "with an average reward of: -4812.075169932027\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3406362\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4814.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1664      |\n",
      "|    time_elapsed         | 45689     |\n",
      "|    total_timesteps      | 3407872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0251    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.79e+10  |\n",
      "|    n_updates            | 16630     |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 1364  finished with cumulative reward: -5277500.0 and \n",
      "with an average reward of: -2110.15593762495\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3408863\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2111.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1665      |\n",
      "|    time_elapsed         | 45717     |\n",
      "|    total_timesteps      | 3409920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0426    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.63e+09  |\n",
      "|    n_updates            | 16640     |\n",
      "|    policy_gradient_loss | -3.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.91e+09  |\n",
      "---------------------------------------\n",
      "Episode 1365  finished with cumulative reward: -9536000.0 and \n",
      "with an average reward of: -3812.874850059976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3411364\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3814.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.99e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1666          |\n",
      "|    time_elapsed         | 45744         |\n",
      "|    total_timesteps      | 3411968       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.03          |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.24e+10      |\n",
      "|    n_updates            | 16650         |\n",
      "|    policy_gradient_loss | -8.31e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.78e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1366  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3413865\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1667      |\n",
      "|    time_elapsed         | 45772     |\n",
      "|    total_timesteps      | 3414016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.09e+09  |\n",
      "|    n_updates            | 16660     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.45e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1668      |\n",
      "|    time_elapsed         | 45793     |\n",
      "|    total_timesteps      | 3416064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0381    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.09e+10  |\n",
      "|    n_updates            | 16670     |\n",
      "|    policy_gradient_loss | -1.27e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.33e+10  |\n",
      "---------------------------------------\n",
      "Episode 1367  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3416366\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1669      |\n",
      "|    time_elapsed         | 45821     |\n",
      "|    total_timesteps      | 3418112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.024     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.22e+09  |\n",
      "|    n_updates            | 16680     |\n",
      "|    policy_gradient_loss | -9.17e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.37e+09  |\n",
      "---------------------------------------\n",
      "Episode 1368  finished with cumulative reward: -2166500.0 and \n",
      "with an average reward of: -866.2534986005597\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3418867\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -866.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1670      |\n",
      "|    time_elapsed         | 45849     |\n",
      "|    total_timesteps      | 3420160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0408    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.75e+09  |\n",
      "|    n_updates            | 16690     |\n",
      "|    policy_gradient_loss | -3.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.05e+09  |\n",
      "---------------------------------------\n",
      "Episode 1369  finished with cumulative reward: -7572500.0 and \n",
      "with an average reward of: -3027.7888844462213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3421368\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3029.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1671      |\n",
      "|    time_elapsed         | 45878     |\n",
      "|    total_timesteps      | 3422208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0283    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.5e+09   |\n",
      "|    n_updates            | 16700     |\n",
      "|    policy_gradient_loss | -2.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.98e+10  |\n",
      "---------------------------------------\n",
      "Episode 1370  finished with cumulative reward: -12213500.0 and \n",
      "with an average reward of: -4883.44662135146\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3423869\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4885.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1672      |\n",
      "|    time_elapsed         | 45906     |\n",
      "|    total_timesteps      | 3424256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0281    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.41e+09  |\n",
      "|    n_updates            | 16710     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.14e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1673      |\n",
      "|    time_elapsed         | 45927     |\n",
      "|    total_timesteps      | 3426304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.032     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.17e+09  |\n",
      "|    n_updates            | 16720     |\n",
      "|    policy_gradient_loss | -2.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 1371  finished with cumulative reward: -5966000.0 and \n",
      "with an average reward of: -2385.4458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3426370\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2386.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1674      |\n",
      "|    time_elapsed         | 45955     |\n",
      "|    total_timesteps      | 3428352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.033     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.49e+09  |\n",
      "|    n_updates            | 16730     |\n",
      "|    policy_gradient_loss | -1.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.87e+09  |\n",
      "---------------------------------------\n",
      "Episode 1372  finished with cumulative reward: -3492500.0 and \n",
      "with an average reward of: -1396.4414234306278\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3428871\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1397.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1675      |\n",
      "|    time_elapsed         | 45983     |\n",
      "|    total_timesteps      | 3430400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.037     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.32e+09  |\n",
      "|    n_updates            | 16740     |\n",
      "|    policy_gradient_loss | -3.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.18e+09  |\n",
      "---------------------------------------\n",
      "Episode 1373  finished with cumulative reward: -7802000.0 and \n",
      "with an average reward of: -3119.5521791283486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3431372\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3120.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.07e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1676      |\n",
      "|    time_elapsed         | 46011     |\n",
      "|    total_timesteps      | 3432448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0333    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+09  |\n",
      "|    n_updates            | 16750     |\n",
      "|    policy_gradient_loss | -2.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.75e+09  |\n",
      "---------------------------------------\n",
      "Episode 1374  finished with cumulative reward: -8516000.0 and \n",
      "with an average reward of: -3405.0379848060775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3433873\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3406.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1677      |\n",
      "|    time_elapsed         | 46040     |\n",
      "|    total_timesteps      | 3434496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.031     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.49e+10  |\n",
      "|    n_updates            | 16760     |\n",
      "|    policy_gradient_loss | -8.11e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.75e+10  |\n",
      "---------------------------------------\n",
      "Episode 1375  finished with cumulative reward: -5150000.0 and \n",
      "with an average reward of: -2059.1763294682128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3436374\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2060.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1678         |\n",
      "|    time_elapsed         | 46069        |\n",
      "|    total_timesteps      | 3436544      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0329       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.95e+09     |\n",
      "|    n_updates            | 16770        |\n",
      "|    policy_gradient_loss | -4.45e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.28e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1679      |\n",
      "|    time_elapsed         | 46090     |\n",
      "|    total_timesteps      | 3438592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0283    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.67e+09  |\n",
      "|    n_updates            | 16780     |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 1376  finished with cumulative reward: -9077000.0 and \n",
      "with an average reward of: -3629.348260695722\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3438875\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3630.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.1e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1680          |\n",
      "|    time_elapsed         | 46119         |\n",
      "|    total_timesteps      | 3440640       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0352        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.13e+10      |\n",
      "|    n_updates            | 16790         |\n",
      "|    policy_gradient_loss | -5.5e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.27e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1377  finished with cumulative reward: -5252000.0 and \n",
      "with an average reward of: -2099.9600159936026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3441376\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2100.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1681      |\n",
      "|    time_elapsed         | 46147     |\n",
      "|    total_timesteps      | 3442688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0373    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.15e+09  |\n",
      "|    n_updates            | 16800     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 1378  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3443877\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1682      |\n",
      "|    time_elapsed         | 46175     |\n",
      "|    total_timesteps      | 3444736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0339    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.84e+09  |\n",
      "|    n_updates            | 16810     |\n",
      "|    policy_gradient_loss | -2.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.41e+10  |\n",
      "---------------------------------------\n",
      "Episode 1379  finished with cumulative reward: -9000500.0 and \n",
      "with an average reward of: -3598.7604958016796\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3446378\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3600.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1683      |\n",
      "|    time_elapsed         | 46203     |\n",
      "|    total_timesteps      | 3446784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.042     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.29e+09  |\n",
      "|    n_updates            | 16820     |\n",
      "|    policy_gradient_loss | -3.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.48e+09  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.1e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1684          |\n",
      "|    time_elapsed         | 46224         |\n",
      "|    total_timesteps      | 3448832       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9790605e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0333        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.2e+10       |\n",
      "|    n_updates            | 16830         |\n",
      "|    policy_gradient_loss | -9.52e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.91e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1380  finished with cumulative reward: -585500.0 and \n",
      "with an average reward of: -234.1063574570172\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3448879\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -234.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1685      |\n",
      "|    time_elapsed         | 46251     |\n",
      "|    total_timesteps      | 3450880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0531    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.05e+08  |\n",
      "|    n_updates            | 16840     |\n",
      "|    policy_gradient_loss | -2.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 1381  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3451380\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1686      |\n",
      "|    time_elapsed         | 46279     |\n",
      "|    total_timesteps      | 3452928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.037     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.2e+09   |\n",
      "|    n_updates            | 16850     |\n",
      "|    policy_gradient_loss | -1.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 1382  finished with cumulative reward: -4895000.0 and \n",
      "with an average reward of: -1957.2171131547382\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3453881\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1958.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1687      |\n",
      "|    time_elapsed         | 46308     |\n",
      "|    total_timesteps      | 3454976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0346    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.31e+09  |\n",
      "|    n_updates            | 16860     |\n",
      "|    policy_gradient_loss | -1.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 1383  finished with cumulative reward: -14916500.0 and \n",
      "with an average reward of: -5964.214314274291\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3456382\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5966.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.14e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1688         |\n",
      "|    time_elapsed         | 46336        |\n",
      "|    total_timesteps      | 3457024      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.905772e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0435       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.41e+09     |\n",
      "|    n_updates            | 16870        |\n",
      "|    policy_gradient_loss | -1.2e-05     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.76e+09     |\n",
      "------------------------------------------\n",
      "Episode 1384  finished with cumulative reward: -4028000.0 and \n",
      "with an average reward of: -1610.5557776889245\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3458883\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1611.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1689      |\n",
      "|    time_elapsed         | 46364     |\n",
      "|    total_timesteps      | 3459072   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.41e+10  |\n",
      "|    n_updates            | 16880     |\n",
      "|    policy_gradient_loss | -2.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.4e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1690      |\n",
      "|    time_elapsed         | 46385     |\n",
      "|    total_timesteps      | 3461120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0407    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.89e+09  |\n",
      "|    n_updates            | 16890     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.84e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1385  finished with cumulative reward: -8541500.0 and \n",
      "with an average reward of: -3415.233906437425\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3461384\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3416.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1691      |\n",
      "|    time_elapsed         | 46414     |\n",
      "|    total_timesteps      | 3463168   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0277    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+10  |\n",
      "|    n_updates            | 16900     |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 1386  finished with cumulative reward: -2778500.0 and \n",
      "with an average reward of: -1110.955617752899\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3463885\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1111.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.14e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1692          |\n",
      "|    time_elapsed         | 46442         |\n",
      "|    total_timesteps      | 3465216       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6775524e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0414        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.77e+09      |\n",
      "|    n_updates            | 16910         |\n",
      "|    policy_gradient_loss | -1.37e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.25e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1387  finished with cumulative reward: -1860500.0 and \n",
      "with an average reward of: -743.9024390243902\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3466386\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -744.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1693      |\n",
      "|    time_elapsed         | 46470     |\n",
      "|    total_timesteps      | 3467264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.034     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.56e+09  |\n",
      "|    n_updates            | 16920     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.66e+09  |\n",
      "---------------------------------------\n",
      "Episode 1388  finished with cumulative reward: -6425000.0 and \n",
      "with an average reward of: -2568.972411035586\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3468887\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2570.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1694      |\n",
      "|    time_elapsed         | 46498     |\n",
      "|    total_timesteps      | 3469312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0341    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.2e+09   |\n",
      "|    n_updates            | 16930     |\n",
      "|    policy_gradient_loss | -8.34e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.14e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1695         |\n",
      "|    time_elapsed         | 46519        |\n",
      "|    total_timesteps      | 3471360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0389       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.68e+09     |\n",
      "|    n_updates            | 16940        |\n",
      "|    policy_gradient_loss | -4.39e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.51e+09     |\n",
      "------------------------------------------\n",
      "Episode 1389  finished with cumulative reward: -6960500.0 and \n",
      "with an average reward of: -2783.0867652938823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3471388\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2784.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1696      |\n",
      "|    time_elapsed         | 46547     |\n",
      "|    total_timesteps      | 3473408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0345    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.32e+09  |\n",
      "|    n_updates            | 16950     |\n",
      "|    policy_gradient_loss | -1.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 1390  finished with cumulative reward: -3237500.0 and \n",
      "with an average reward of: -1294.4822071171532\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3473889\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1295.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1697      |\n",
      "|    time_elapsed         | 46576     |\n",
      "|    total_timesteps      | 3475456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0327    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.05e+09  |\n",
      "|    n_updates            | 16960     |\n",
      "|    policy_gradient_loss | -1.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.75e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1391  finished with cumulative reward: -3696500.0 and \n",
      "with an average reward of: -1478.0087964814074\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3476390\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1478.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1698      |\n",
      "|    time_elapsed         | 46604     |\n",
      "|    total_timesteps      | 3477504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0426    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.24e+09  |\n",
      "|    n_updates            | 16970     |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.62e+09  |\n",
      "---------------------------------------\n",
      "Episode 1392  finished with cumulative reward: -9944000.0 and \n",
      "with an average reward of: -3976.0095961615352\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3478891\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3977.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1699      |\n",
      "|    time_elapsed         | 46633     |\n",
      "|    total_timesteps      | 3479552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0357    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.99e+09  |\n",
      "|    n_updates            | 16980     |\n",
      "|    policy_gradient_loss | -8.42e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.42e+10  |\n",
      "---------------------------------------\n",
      "Episode 1393  finished with cumulative reward: -1299500.0 and \n",
      "with an average reward of: -519.5921631347461\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3481392\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -519.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.05e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1700          |\n",
      "|    time_elapsed         | 46661         |\n",
      "|    total_timesteps      | 3481600       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 3.3760443e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0263        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.9e+09       |\n",
      "|    n_updates            | 16990         |\n",
      "|    policy_gradient_loss | -3.56e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.52e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.05e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1701         |\n",
      "|    time_elapsed         | 46682        |\n",
      "|    total_timesteps      | 3483648      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.947651e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0368       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.1e+08      |\n",
      "|    n_updates            | 17000        |\n",
      "|    policy_gradient_loss | -6.1e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.35e+09     |\n",
      "------------------------------------------\n",
      "Episode 1394  finished with cumulative reward: -1376000.0 and \n",
      "with an average reward of: -550.1799280287885\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3483893\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -550.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1702      |\n",
      "|    time_elapsed         | 46710     |\n",
      "|    total_timesteps      | 3485696   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0381    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.56e+08  |\n",
      "|    n_updates            | 17010     |\n",
      "|    policy_gradient_loss | -2.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.95e+09  |\n",
      "---------------------------------------\n",
      "Episode 1395  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3486394\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1703      |\n",
      "|    time_elapsed         | 46738     |\n",
      "|    total_timesteps      | 3487744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0391    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.04e+09  |\n",
      "|    n_updates            | 17020     |\n",
      "|    policy_gradient_loss | -2.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7e+09     |\n",
      "---------------------------------------\n",
      "Episode 1396  finished with cumulative reward: -3008000.0 and \n",
      "with an average reward of: -1202.718912435026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3488895\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1203.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1704      |\n",
      "|    time_elapsed         | 46766     |\n",
      "|    total_timesteps      | 3489792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0359    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.7e+09   |\n",
      "|    n_updates            | 17030     |\n",
      "|    policy_gradient_loss | -2.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.22e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1397  finished with cumulative reward: -7266500.0 and \n",
      "with an average reward of: -2905.437824870052\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3491396\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2906.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1705      |\n",
      "|    time_elapsed         | 46795     |\n",
      "|    total_timesteps      | 3491840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0381    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.33e+09  |\n",
      "|    n_updates            | 17040     |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.86e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1706      |\n",
      "|    time_elapsed         | 46816     |\n",
      "|    total_timesteps      | 3493888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0358    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.46e+09  |\n",
      "|    n_updates            | 17050     |\n",
      "|    policy_gradient_loss | -1.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.34e+10  |\n",
      "---------------------------------------\n",
      "Episode 1398  finished with cumulative reward: -12009500.0 and \n",
      "with an average reward of: -4801.87924830068\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3493897\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4803.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1707      |\n",
      "|    time_elapsed         | 46844     |\n",
      "|    total_timesteps      | 3495936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0208    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.92e+09  |\n",
      "|    n_updates            | 17060     |\n",
      "|    policy_gradient_loss | -7.92e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 1399  finished with cumulative reward: -7674500.0 and \n",
      "with an average reward of: -3068.572570971611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3496398\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3069.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.22e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1708      |\n",
      "|    time_elapsed         | 46873     |\n",
      "|    total_timesteps      | 3497984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0367    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.62e+09  |\n",
      "|    n_updates            | 17070     |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.93e+10  |\n",
      "---------------------------------------\n",
      "Episode 1400  finished with cumulative reward: -11754500.0 and \n",
      "with an average reward of: -4699.920031987205\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3498899\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4701.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.3e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1709      |\n",
      "|    time_elapsed         | 46901     |\n",
      "|    total_timesteps      | 3500032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0303    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.89e+10  |\n",
      "|    n_updates            | 17080     |\n",
      "|    policy_gradient_loss | -2.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 1401  finished with cumulative reward: -9791000.0 and \n",
      "with an average reward of: -3914.8340663734507\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3501400\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3916.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.33e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1710      |\n",
      "|    time_elapsed         | 46929     |\n",
      "|    total_timesteps      | 3502080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0263    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1e+10     |\n",
      "|    n_updates            | 17090     |\n",
      "|    policy_gradient_loss | -1.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.81e+10  |\n",
      "---------------------------------------\n",
      "Episode 1402  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3503901\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.32e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1711      |\n",
      "|    time_elapsed         | 46958     |\n",
      "|    total_timesteps      | 3504128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0358    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.51e+09  |\n",
      "|    n_updates            | 17100     |\n",
      "|    policy_gradient_loss | -9.07e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.82e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.32e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1712      |\n",
      "|    time_elapsed         | 46978     |\n",
      "|    total_timesteps      | 3506176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0364    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.81e+09  |\n",
      "|    n_updates            | 17110     |\n",
      "|    policy_gradient_loss | -1.5e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.2e+09   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1403  finished with cumulative reward: -4359500.0 and \n",
      "with an average reward of: -1743.1027588964414\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3506402\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1743.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.3e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1713         |\n",
      "|    time_elapsed         | 47006        |\n",
      "|    total_timesteps      | 3508224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0327       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.52e+09     |\n",
      "|    n_updates            | 17120        |\n",
      "|    policy_gradient_loss | -2.19e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.15e+10     |\n",
      "------------------------------------------\n",
      "Episode 1404  finished with cumulative reward: -9332000.0 and \n",
      "with an average reward of: -3731.307477009196\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3508903\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3732.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.36e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1714      |\n",
      "|    time_elapsed         | 47034     |\n",
      "|    total_timesteps      | 3510272   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0353    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.83e+09  |\n",
      "|    n_updates            | 17130     |\n",
      "|    policy_gradient_loss | -2.52e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 1405  finished with cumulative reward: -10938500.0 and \n",
      "with an average reward of: -4373.6505397840865\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3511404\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4375.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.4e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1715      |\n",
      "|    time_elapsed         | 47062     |\n",
      "|    total_timesteps      | 3512320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0339    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.1e+09   |\n",
      "|    n_updates            | 17140     |\n",
      "|    policy_gradient_loss | -9.09e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.42e+10  |\n",
      "---------------------------------------\n",
      "Episode 1406  finished with cumulative reward: -1809500.0 and \n",
      "with an average reward of: -723.5105957616953\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3513905\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -723.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.33e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1716      |\n",
      "|    time_elapsed         | 47091     |\n",
      "|    total_timesteps      | 3514368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0321    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.99e+09  |\n",
      "|    n_updates            | 17150     |\n",
      "|    policy_gradient_loss | -1.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.63e+10  |\n",
      "---------------------------------------\n",
      "Episode 1407  finished with cumulative reward: -3747500.0 and \n",
      "with an average reward of: -1498.4006397441024\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3516406\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1499.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1717      |\n",
      "|    time_elapsed         | 47119     |\n",
      "|    total_timesteps      | 3516416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0861    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.15e+08  |\n",
      "|    n_updates            | 17160     |\n",
      "|    policy_gradient_loss | -3.57e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.15e+08  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.29e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1718      |\n",
      "|    time_elapsed         | 47141     |\n",
      "|    total_timesteps      | 3518464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0355    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.73e+09  |\n",
      "|    n_updates            | 17170     |\n",
      "|    policy_gradient_loss | -1.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 1408  finished with cumulative reward: -1758500.0 and \n",
      "with an average reward of: -703.1187524990004\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3518907\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -703.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1719      |\n",
      "|    time_elapsed         | 47168     |\n",
      "|    total_timesteps      | 3520512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0398    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.11e+09  |\n",
      "|    n_updates            | 17180     |\n",
      "|    policy_gradient_loss | -2.4e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.15e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1409  finished with cumulative reward: -9000500.0 and \n",
      "with an average reward of: -3598.7604958016796\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3521408\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3600.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.3e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1720      |\n",
      "|    time_elapsed         | 47196     |\n",
      "|    total_timesteps      | 3522560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0355    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.14e+09  |\n",
      "|    n_updates            | 17190     |\n",
      "|    policy_gradient_loss | -1.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1410  finished with cumulative reward: -8822000.0 and \n",
      "with an average reward of: -3527.389044382247\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3523909\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3528.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.35e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1721      |\n",
      "|    time_elapsed         | 47224     |\n",
      "|    total_timesteps      | 3524608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0335    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.05e+09  |\n",
      "|    n_updates            | 17200     |\n",
      "|    policy_gradient_loss | -2.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.42e+10  |\n",
      "---------------------------------------\n",
      "Episode 1411  finished with cumulative reward: -3212000.0 and \n",
      "with an average reward of: -1284.2862854858056\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3526410\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1284.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1722      |\n",
      "|    time_elapsed         | 47252     |\n",
      "|    total_timesteps      | 3526656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.039     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.04e+09  |\n",
      "|    n_updates            | 17210     |\n",
      "|    policy_gradient_loss | -2.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.34e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1723      |\n",
      "|    time_elapsed         | 47273     |\n",
      "|    total_timesteps      | 3528704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0331    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.16e+09  |\n",
      "|    n_updates            | 17220     |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 1412  finished with cumulative reward: -3390500.0 and \n",
      "with an average reward of: -1355.657736905238\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3528911\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1356.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.28e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1724      |\n",
      "|    time_elapsed         | 47302     |\n",
      "|    total_timesteps      | 3530752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0367    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.21e+09  |\n",
      "|    n_updates            | 17230     |\n",
      "|    policy_gradient_loss | -2.31e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 1413  finished with cumulative reward: -1121000.0 and \n",
      "with an average reward of: -448.2207117153139\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3531412\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -448.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1725      |\n",
      "|    time_elapsed         | 47331     |\n",
      "|    total_timesteps      | 3532800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0367    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.27e+09  |\n",
      "|    n_updates            | 17240     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.27e+09  |\n",
      "---------------------------------------\n",
      "Episode 1414  finished with cumulative reward: -3569000.0 and \n",
      "with an average reward of: -1427.02918832467\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3533913\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1427.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1726      |\n",
      "|    time_elapsed         | 47360     |\n",
      "|    total_timesteps      | 3534848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.58e+09  |\n",
      "|    n_updates            | 17250     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.55e+09  |\n",
      "---------------------------------------\n",
      "Episode 1415  finished with cumulative reward: -2064500.0 and \n",
      "with an average reward of: -825.4698120751699\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3536414\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -825.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.21e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1727      |\n",
      "|    time_elapsed         | 47388     |\n",
      "|    total_timesteps      | 3536896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0421    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.62e+08  |\n",
      "|    n_updates            | 17260     |\n",
      "|    policy_gradient_loss | -2.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.45e+09  |\n",
      "---------------------------------------\n",
      "Episode 1416  finished with cumulative reward: -4767500.0 and \n",
      "with an average reward of: -1906.2375049980008\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3538915\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1907.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1728      |\n",
      "|    time_elapsed         | 47417     |\n",
      "|    total_timesteps      | 3538944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0389    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.91e+09  |\n",
      "|    n_updates            | 17270     |\n",
      "|    policy_gradient_loss | -2.59e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.6e+09   |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.17e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1729         |\n",
      "|    time_elapsed         | 47438        |\n",
      "|    total_timesteps      | 3540992      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0457       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.15e+09     |\n",
      "|    n_updates            | 17280        |\n",
      "|    policy_gradient_loss | -2.12e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.79e+09     |\n",
      "------------------------------------------\n",
      "Episode 1417  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3541416\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1730      |\n",
      "|    time_elapsed         | 47466     |\n",
      "|    total_timesteps      | 3543040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0309    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.5e+09   |\n",
      "|    n_updates            | 17290     |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.74e+10  |\n",
      "---------------------------------------\n",
      "Episode 1418  finished with cumulative reward: -5762000.0 and \n",
      "with an average reward of: -2303.878448620552\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3543917\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2304.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1731      |\n",
      "|    time_elapsed         | 47494     |\n",
      "|    total_timesteps      | 3545088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0314    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.59e+09  |\n",
      "|    n_updates            | 17300     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 1419  finished with cumulative reward: -2778500.0 and \n",
      "with an average reward of: -1110.955617752899\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3546418\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1111.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1732      |\n",
      "|    time_elapsed         | 47523     |\n",
      "|    total_timesteps      | 3547136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0378    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.07e+09  |\n",
      "|    n_updates            | 17310     |\n",
      "|    policy_gradient_loss | -1.69e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.76e+09  |\n",
      "---------------------------------------\n",
      "Episode 1420  finished with cumulative reward: -8337500.0 and \n",
      "with an average reward of: -3333.6665333866454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3548919\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3335.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1733      |\n",
      "|    time_elapsed         | 47551     |\n",
      "|    total_timesteps      | 3549184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0405    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.53e+09  |\n",
      "|    n_updates            | 17320     |\n",
      "|    policy_gradient_loss | -4.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.52e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1734      |\n",
      "|    time_elapsed         | 47572     |\n",
      "|    total_timesteps      | 3551232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0342    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.04e+09  |\n",
      "|    n_updates            | 17330     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2e+10     |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1421  finished with cumulative reward: -6782000.0 and \n",
      "with an average reward of: -2711.71531387445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3551420\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2712.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.16e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1735      |\n",
      "|    time_elapsed         | 47600     |\n",
      "|    total_timesteps      | 3553280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0279    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.61e+09  |\n",
      "|    n_updates            | 17340     |\n",
      "|    policy_gradient_loss | -1.86e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 1422  finished with cumulative reward: -5940500.0 and \n",
      "with an average reward of: -2375.249900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3553921\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2376.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1736      |\n",
      "|    time_elapsed         | 47629     |\n",
      "|    total_timesteps      | 3555328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0392    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.79e+08  |\n",
      "|    n_updates            | 17350     |\n",
      "|    policy_gradient_loss | -9.08e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.68e+09  |\n",
      "---------------------------------------\n",
      "Episode 1423  finished with cumulative reward: -8592500.0 and \n",
      "with an average reward of: -3435.6257497001197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3556422\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3437.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1737      |\n",
      "|    time_elapsed         | 47657     |\n",
      "|    total_timesteps      | 3557376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0294    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 17360     |\n",
      "|    policy_gradient_loss | -1.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.91e+10  |\n",
      "---------------------------------------\n",
      "Episode 1424  finished with cumulative reward: -1274000.0 and \n",
      "with an average reward of: -509.39624150339864\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3558923\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -509.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1738      |\n",
      "|    time_elapsed         | 47685     |\n",
      "|    total_timesteps      | 3559424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0332    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.57e+09  |\n",
      "|    n_updates            | 17370     |\n",
      "|    policy_gradient_loss | -2.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 1425  finished with cumulative reward: -6552500.0 and \n",
      "with an average reward of: -2619.952019192323\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3561424\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2621.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1739      |\n",
      "|    time_elapsed         | 47713     |\n",
      "|    total_timesteps      | 3561472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0962    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.68e+07  |\n",
      "|    n_updates            | 17380     |\n",
      "|    policy_gradient_loss | -4.49e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.58e+08  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1740      |\n",
      "|    time_elapsed         | 47733     |\n",
      "|    total_timesteps      | 3563520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0304    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.3e+10   |\n",
      "|    n_updates            | 17390     |\n",
      "|    policy_gradient_loss | -2.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 1426  finished with cumulative reward: -8975000.0 and \n",
      "with an average reward of: -3588.564574170332\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3563925\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3590.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1741      |\n",
      "|    time_elapsed         | 47761     |\n",
      "|    total_timesteps      | 3565568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0338    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.97e+09  |\n",
      "|    n_updates            | 17400     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "Episode 1427  finished with cumulative reward: -7266500.0 and \n",
      "with an average reward of: -2905.437824870052\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3566426\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2906.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1742      |\n",
      "|    time_elapsed         | 47789     |\n",
      "|    total_timesteps      | 3567616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0379    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.4e+09   |\n",
      "|    n_updates            | 17410     |\n",
      "|    policy_gradient_loss | -1.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 1428  finished with cumulative reward: -4232000.0 and \n",
      "with an average reward of: -1692.1231507397042\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3568927\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1692.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1743      |\n",
      "|    time_elapsed         | 47817     |\n",
      "|    total_timesteps      | 3569664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0322    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.1e+09   |\n",
      "|    n_updates            | 17420     |\n",
      "|    policy_gradient_loss | -2.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 1429  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3571428\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.08e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1744         |\n",
      "|    time_elapsed         | 47845        |\n",
      "|    total_timesteps      | 3571712      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0303       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.75e+09     |\n",
      "|    n_updates            | 17430        |\n",
      "|    policy_gradient_loss | -4.16e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.04e+09     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1745      |\n",
      "|    time_elapsed         | 47866     |\n",
      "|    total_timesteps      | 3573760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0352    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.33e+09  |\n",
      "|    n_updates            | 17440     |\n",
      "|    policy_gradient_loss | -1.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 1430  finished with cumulative reward: -2829500.0 and \n",
      "with an average reward of: -1131.3474610155938\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3573929\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1131.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -6e+06   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1746     |\n",
      "|    time_elapsed         | 47894    |\n",
      "|    total_timesteps      | 3575808  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0368   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 3.6e+09  |\n",
      "|    n_updates            | 17450    |\n",
      "|    policy_gradient_loss | -1.5e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 7.02e+09 |\n",
      "--------------------------------------\n",
      "Episode 1431  finished with cumulative reward: -4487000.0 and \n",
      "with an average reward of: -1794.0823670531788\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3576430\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1794.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1747      |\n",
      "|    time_elapsed         | 47921     |\n",
      "|    total_timesteps      | 3577856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0324    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.23e+09  |\n",
      "|    n_updates            | 17460     |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.06e+10  |\n",
      "---------------------------------------\n",
      "Episode 1432  finished with cumulative reward: -9051500.0 and \n",
      "with an average reward of: -3619.1523390643742\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3578931\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3620.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1748      |\n",
      "|    time_elapsed         | 47949     |\n",
      "|    total_timesteps      | 3579904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0348    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.45e+10  |\n",
      "|    n_updates            | 17470     |\n",
      "|    policy_gradient_loss | -3.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.84e+10  |\n",
      "---------------------------------------\n",
      "Episode 1433  finished with cumulative reward: -6578000.0 and \n",
      "with an average reward of: -2630.1479408236705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3581432\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2631.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1749      |\n",
      "|    time_elapsed         | 47977     |\n",
      "|    total_timesteps      | 3581952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0215    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.91e+09  |\n",
      "|    n_updates            | 17480     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 1434  finished with cumulative reward: -3288500.0 and \n",
      "with an average reward of: -1314.874050379848\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3583933\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1315.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6e+06       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1750         |\n",
      "|    time_elapsed         | 48005        |\n",
      "|    total_timesteps      | 3584000      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0387       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.89e+09     |\n",
      "|    n_updates            | 17490        |\n",
      "|    policy_gradient_loss | -6.12e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.02e+10     |\n",
      "------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6e+06       |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1751         |\n",
      "|    time_elapsed         | 48026        |\n",
      "|    total_timesteps      | 3586048      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0346       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.79e+09     |\n",
      "|    n_updates            | 17500        |\n",
      "|    policy_gradient_loss | -3.9e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.75e+09     |\n",
      "------------------------------------------\n",
      "Episode 1435  finished with cumulative reward: -7904000.0 and \n",
      "with an average reward of: -3160.3358656537384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3586434\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3161.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1752      |\n",
      "|    time_elapsed         | 48054     |\n",
      "|    total_timesteps      | 3588096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0354    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.79e+09  |\n",
      "|    n_updates            | 17510     |\n",
      "|    policy_gradient_loss | -7.52e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.76e+10  |\n",
      "---------------------------------------\n",
      "Episode 1436  finished with cumulative reward: -330500.0 and \n",
      "with an average reward of: -132.14714114354257\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3588935\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -132.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.97e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1753         |\n",
      "|    time_elapsed         | 48082        |\n",
      "|    total_timesteps      | 3590144      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.566996e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.046        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.42e+09     |\n",
      "|    n_updates            | 17520        |\n",
      "|    policy_gradient_loss | -7.5e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.99e+09     |\n",
      "------------------------------------------\n",
      "Episode 1437  finished with cumulative reward: -9995000.0 and \n",
      "with an average reward of: -3996.4014394242304\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3591436\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3998.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1754      |\n",
      "|    time_elapsed         | 48111     |\n",
      "|    total_timesteps      | 3592192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0264    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.45e+10  |\n",
      "|    n_updates            | 17530     |\n",
      "|    policy_gradient_loss | -1.81e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1438  finished with cumulative reward: -6782000.0 and \n",
      "with an average reward of: -2711.71531387445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3593937\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2712.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1755      |\n",
      "|    time_elapsed         | 48139     |\n",
      "|    total_timesteps      | 3594240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0514    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.79e+08  |\n",
      "|    n_updates            | 17540     |\n",
      "|    policy_gradient_loss | -3.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.15e+09  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.04e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1756         |\n",
      "|    time_elapsed         | 48160        |\n",
      "|    total_timesteps      | 3596288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.566996e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0356       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.77e+09     |\n",
      "|    n_updates            | 17550        |\n",
      "|    policy_gradient_loss | -6.76e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.08e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1439  finished with cumulative reward: -7521500.0 and \n",
      "with an average reward of: -3007.3970411835267\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3596438\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3008.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1757      |\n",
      "|    time_elapsed         | 48187     |\n",
      "|    total_timesteps      | 3598336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0299    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.82e+09  |\n",
      "|    n_updates            | 17560     |\n",
      "|    policy_gradient_loss | -1.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.7e+10   |\n",
      "---------------------------------------\n",
      "Episode 1440  finished with cumulative reward: -10403000.0 and \n",
      "with an average reward of: -4159.53618552579\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3598939\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4161.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1758      |\n",
      "|    time_elapsed         | 48214     |\n",
      "|    total_timesteps      | 3600384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0302    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.8e+09   |\n",
      "|    n_updates            | 17570     |\n",
      "|    policy_gradient_loss | -1.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 1441  finished with cumulative reward: -5022500.0 and \n",
      "with an average reward of: -2008.1967213114754\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3601440\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2009.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1759      |\n",
      "|    time_elapsed         | 48243     |\n",
      "|    total_timesteps      | 3602432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0296    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.93e+09  |\n",
      "|    n_updates            | 17580     |\n",
      "|    policy_gradient_loss | -1.67e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.57e+10  |\n",
      "---------------------------------------\n",
      "Episode 1442  finished with cumulative reward: -5532500.0 and \n",
      "with an average reward of: -2212.1151539384246\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3603941\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2213.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1760      |\n",
      "|    time_elapsed         | 48271     |\n",
      "|    total_timesteps      | 3604480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0318    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.47e+09  |\n",
      "|    n_updates            | 17590     |\n",
      "|    policy_gradient_loss | -1.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1443  finished with cumulative reward: -10199000.0 and \n",
      "with an average reward of: -4077.96881247501\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3606442\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4079.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1761      |\n",
      "|    time_elapsed         | 48299     |\n",
      "|    total_timesteps      | 3606528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0365    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.85e+09  |\n",
      "|    n_updates            | 17600     |\n",
      "|    policy_gradient_loss | -9.55e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1762      |\n",
      "|    time_elapsed         | 48320     |\n",
      "|    total_timesteps      | 3608576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.03      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.35e+10  |\n",
      "|    n_updates            | 17610     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 1444  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3608943\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1763      |\n",
      "|    time_elapsed         | 48348     |\n",
      "|    total_timesteps      | 3610624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.65e+09  |\n",
      "|    n_updates            | 17620     |\n",
      "|    policy_gradient_loss | -3.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 1445  finished with cumulative reward: -1197500.0 and \n",
      "with an average reward of: -478.80847660935626\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3611444\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -479.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.04e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1764          |\n",
      "|    time_elapsed         | 48376         |\n",
      "|    total_timesteps      | 3612672       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.0186341e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0495        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.34e+08      |\n",
      "|    n_updates            | 17630         |\n",
      "|    policy_gradient_loss | -9.65e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.32e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1446  finished with cumulative reward: -4053500.0 and \n",
      "with an average reward of: -1620.7516993202719\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3613945\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1621.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.01e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1765          |\n",
      "|    time_elapsed         | 48403         |\n",
      "|    total_timesteps      | 3614720       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.0640665e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0397        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.8e+09       |\n",
      "|    n_updates            | 17640         |\n",
      "|    policy_gradient_loss | -7.45e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.16e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1447  finished with cumulative reward: -4206500.0 and \n",
      "with an average reward of: -1681.9272291083566\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3616446\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1682.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1766      |\n",
      "|    time_elapsed         | 48430     |\n",
      "|    total_timesteps      | 3616768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0407    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.5e+09   |\n",
      "|    n_updates            | 17650     |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.26e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1767      |\n",
      "|    time_elapsed         | 48450     |\n",
      "|    total_timesteps      | 3618816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.036     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.93e+09  |\n",
      "|    n_updates            | 17660     |\n",
      "|    policy_gradient_loss | -1.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.14e+09  |\n",
      "---------------------------------------\n",
      "Episode 1448  finished with cumulative reward: -5787500.0 and \n",
      "with an average reward of: -2314.0743702518994\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3618947\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2315.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1768      |\n",
      "|    time_elapsed         | 48478     |\n",
      "|    total_timesteps      | 3620864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.03      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.75e+09  |\n",
      "|    n_updates            | 17670     |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.17e+10  |\n",
      "---------------------------------------\n",
      "Episode 1449  finished with cumulative reward: -8592500.0 and \n",
      "with an average reward of: -3435.6257497001197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3621448\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3437.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.95e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1769      |\n",
      "|    time_elapsed         | 48506     |\n",
      "|    total_timesteps      | 3622912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0358    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.7e+09   |\n",
      "|    n_updates            | 17680     |\n",
      "|    policy_gradient_loss | -2.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.63e+10  |\n",
      "---------------------------------------\n",
      "Episode 1450  finished with cumulative reward: -2957000.0 and \n",
      "with an average reward of: -1182.327069172331\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3623949\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1182.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1770      |\n",
      "|    time_elapsed         | 48533     |\n",
      "|    total_timesteps      | 3624960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0361    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.83e+09  |\n",
      "|    n_updates            | 17690     |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.64e+09  |\n",
      "---------------------------------------\n",
      "Episode 1451  finished with cumulative reward: -6017000.0 and \n",
      "with an average reward of: -2405.837664934026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3626450\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2406.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1771      |\n",
      "|    time_elapsed         | 48562     |\n",
      "|    total_timesteps      | 3627008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0303    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.89e+09  |\n",
      "|    n_updates            | 17700     |\n",
      "|    policy_gradient_loss | -1.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 1452  finished with cumulative reward: -5940500.0 and \n",
      "with an average reward of: -2375.249900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3628951\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2376.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1772      |\n",
      "|    time_elapsed         | 48590     |\n",
      "|    total_timesteps      | 3629056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.95e+09  |\n",
      "|    n_updates            | 17710     |\n",
      "|    policy_gradient_loss | -2.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.9e+09   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.87e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1773      |\n",
      "|    time_elapsed         | 48610     |\n",
      "|    total_timesteps      | 3631104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0261    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.61e+09  |\n",
      "|    n_updates            | 17720     |\n",
      "|    policy_gradient_loss | -1.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.79e+10  |\n",
      "---------------------------------------\n",
      "Episode 1453  finished with cumulative reward: -2625500.0 and \n",
      "with an average reward of: -1049.7800879648141\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3631452\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1050.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.84e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1774         |\n",
      "|    time_elapsed         | 48638        |\n",
      "|    total_timesteps      | 3633152      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0419       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.96e+09     |\n",
      "|    n_updates            | 17730        |\n",
      "|    policy_gradient_loss | -4.33e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 6.87e+09     |\n",
      "------------------------------------------\n",
      "Episode 1454  finished with cumulative reward: -3365000.0 and \n",
      "with an average reward of: -1345.4618152738904\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3633953\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1346.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.76e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1775          |\n",
      "|    time_elapsed         | 48666         |\n",
      "|    total_timesteps      | 3635200       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1350494e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0497        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.17e+09      |\n",
      "|    n_updates            | 17740         |\n",
      "|    policy_gradient_loss | -9.12e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.67e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1455  finished with cumulative reward: -3416000.0 and \n",
      "with an average reward of: -1365.8536585365853\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3636454\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1366.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.73e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1776          |\n",
      "|    time_elapsed         | 48695         |\n",
      "|    total_timesteps      | 3637248       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.6193447e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0445        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.8e+09       |\n",
      "|    n_updates            | 17750         |\n",
      "|    policy_gradient_loss | -5.1e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.07e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1456  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3638955\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1777      |\n",
      "|    time_elapsed         | 48723     |\n",
      "|    total_timesteps      | 3639296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0421    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.84e+09  |\n",
      "|    n_updates            | 17760     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.1e+09   |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1778      |\n",
      "|    time_elapsed         | 48744     |\n",
      "|    total_timesteps      | 3641344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0385    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.9e+09   |\n",
      "|    n_updates            | 17770     |\n",
      "|    policy_gradient_loss | -2.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n",
      "Episode 1457  finished with cumulative reward: -8337500.0 and \n",
      "with an average reward of: -3333.6665333866454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3641456\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3335.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1779      |\n",
      "|    time_elapsed         | 48772     |\n",
      "|    total_timesteps      | 3643392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0288    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.1e+09   |\n",
      "|    n_updates            | 17780     |\n",
      "|    policy_gradient_loss | -1.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "Episode 1458  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3643957\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1780      |\n",
      "|    time_elapsed         | 48801     |\n",
      "|    total_timesteps      | 3645440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0502    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.98e+09  |\n",
      "|    n_updates            | 17790     |\n",
      "|    policy_gradient_loss | -5.97e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.47e+09  |\n",
      "---------------------------------------\n",
      "Episode 1459  finished with cumulative reward: -7368500.0 and \n",
      "with an average reward of: -2946.2215113954417\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3646458\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2947.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.73e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1781          |\n",
      "|    time_elapsed         | 48828         |\n",
      "|    total_timesteps      | 3647488       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1350494e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0384        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.37e+09      |\n",
      "|    n_updates            | 17800         |\n",
      "|    policy_gradient_loss | -9.8e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.09e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1460  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3648959\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.74e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1782      |\n",
      "|    time_elapsed         | 48856     |\n",
      "|    total_timesteps      | 3649536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.18e+10  |\n",
      "|    n_updates            | 17810     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.65e+10  |\n",
      "---------------------------------------\n",
      "Episode 1461  finished with cumulative reward: -6680000.0 and \n",
      "with an average reward of: -2670.9316273490604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3651460\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2672.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1783      |\n",
      "|    time_elapsed         | 48883     |\n",
      "|    total_timesteps      | 3651584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0335    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.8e+09   |\n",
      "|    n_updates            | 17820     |\n",
      "|    policy_gradient_loss | -1.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.26e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1784      |\n",
      "|    time_elapsed         | 48904     |\n",
      "|    total_timesteps      | 3653632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0327    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.73e+09  |\n",
      "|    n_updates            | 17830     |\n",
      "|    policy_gradient_loss | -2.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.75e+10  |\n",
      "---------------------------------------\n",
      "Episode 1462  finished with cumulative reward: -8669000.0 and \n",
      "with an average reward of: -3466.2135145941625\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3653961\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3467.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1785      |\n",
      "|    time_elapsed         | 48932     |\n",
      "|    total_timesteps      | 3655680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0392    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.71e+09  |\n",
      "|    n_updates            | 17840     |\n",
      "|    policy_gradient_loss | -2.53e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1463  finished with cumulative reward: -4079000.0 and \n",
      "with an average reward of: -1630.9476209516195\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3656462\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1631.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.75e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1786          |\n",
      "|    time_elapsed         | 48961         |\n",
      "|    total_timesteps      | 3657728       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0356        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.5e+10       |\n",
      "|    n_updates            | 17850         |\n",
      "|    policy_gradient_loss | -8.9e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.03e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1464  finished with cumulative reward: 332500.0 and \n",
      "with an average reward of: 132.94682127149142\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3658963\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: 133.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1787      |\n",
      "|    time_elapsed         | 48989     |\n",
      "|    total_timesteps      | 3659776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0944    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.18e+07  |\n",
      "|    n_updates            | 17860     |\n",
      "|    policy_gradient_loss | -3.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.32e+08  |\n",
      "---------------------------------------\n",
      "Episode 1465  finished with cumulative reward: -11499500.0 and \n",
      "with an average reward of: -4597.96081567373\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3661464\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4599.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1788      |\n",
      "|    time_elapsed         | 49017     |\n",
      "|    total_timesteps      | 3661824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.032     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.05e+09  |\n",
      "|    n_updates            | 17870     |\n",
      "|    policy_gradient_loss | -3.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.55e+09  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.71e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1789          |\n",
      "|    time_elapsed         | 49038         |\n",
      "|    total_timesteps      | 3663872       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0326        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.19e+10      |\n",
      "|    n_updates            | 17880         |\n",
      "|    policy_gradient_loss | -6.4e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.64e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1466  finished with cumulative reward: -7674500.0 and \n",
      "with an average reward of: -3068.572570971611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3663965\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3069.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1790      |\n",
      "|    time_elapsed         | 49067     |\n",
      "|    total_timesteps      | 3665920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0311    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.34e+10  |\n",
      "|    n_updates            | 17890     |\n",
      "|    policy_gradient_loss | -1.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.25e+10  |\n",
      "---------------------------------------\n",
      "Episode 1467  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3666466\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.77e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1791      |\n",
      "|    time_elapsed         | 49095     |\n",
      "|    total_timesteps      | 3667968   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0353    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.99e+09  |\n",
      "|    n_updates            | 17900     |\n",
      "|    policy_gradient_loss | -2.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 1468  finished with cumulative reward: -7241000.0 and \n",
      "with an average reward of: -2895.2419032387047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3668967\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2896.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.82e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1792      |\n",
      "|    time_elapsed         | 49123     |\n",
      "|    total_timesteps      | 3670016   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0355    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.87e+09  |\n",
      "|    n_updates            | 17910     |\n",
      "|    policy_gradient_loss | -1.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 1469  finished with cumulative reward: -9536000.0 and \n",
      "with an average reward of: -3812.874850059976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3671468\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3814.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1793      |\n",
      "|    time_elapsed         | 49150     |\n",
      "|    total_timesteps      | 3672064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0312    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.39e+09  |\n",
      "|    n_updates            | 17920     |\n",
      "|    policy_gradient_loss | -2.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.92e+10  |\n",
      "---------------------------------------\n",
      "Episode 1470  finished with cumulative reward: -8286500.0 and \n",
      "with an average reward of: -3313.27469012395\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3673969\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3314.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1794      |\n",
      "|    time_elapsed         | 49178     |\n",
      "|    total_timesteps      | 3674112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0363    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.2e+10   |\n",
      "|    n_updates            | 17930     |\n",
      "|    policy_gradient_loss | -2.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1795      |\n",
      "|    time_elapsed         | 49200     |\n",
      "|    total_timesteps      | 3676160   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0374    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.83e+09  |\n",
      "|    n_updates            | 17940     |\n",
      "|    policy_gradient_loss | -8.13e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.54e+10  |\n",
      "---------------------------------------\n",
      "Episode 1471  finished with cumulative reward: -7674500.0 and \n",
      "with an average reward of: -3068.572570971611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3676470\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3069.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.82e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1796      |\n",
      "|    time_elapsed         | 49229     |\n",
      "|    total_timesteps      | 3678208   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.035     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.31e+09  |\n",
      "|    n_updates            | 17950     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 1472  finished with cumulative reward: -6782000.0 and \n",
      "with an average reward of: -2711.71531387445\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3678971\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2712.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1797      |\n",
      "|    time_elapsed         | 49257     |\n",
      "|    total_timesteps      | 3680256   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0316    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8e+09     |\n",
      "|    n_updates            | 17960     |\n",
      "|    policy_gradient_loss | -1.21e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 1473  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3681472\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.81e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1798      |\n",
      "|    time_elapsed         | 49285     |\n",
      "|    total_timesteps      | 3682304   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0392    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.4e+09   |\n",
      "|    n_updates            | 17970     |\n",
      "|    policy_gradient_loss | -2.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 1474  finished with cumulative reward: -3186500.0 and \n",
      "with an average reward of: -1274.0903638544582\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3683973\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1274.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1799      |\n",
      "|    time_elapsed         | 49313     |\n",
      "|    total_timesteps      | 3684352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0426    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.52e+09  |\n",
      "|    n_updates            | 17980     |\n",
      "|    policy_gradient_loss | -2.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.22e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1800      |\n",
      "|    time_elapsed         | 49334     |\n",
      "|    total_timesteps      | 3686400   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0404    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.74e+09  |\n",
      "|    n_updates            | 17990     |\n",
      "|    policy_gradient_loss | -1.39e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 1475  finished with cumulative reward: -2498000.0 and \n",
      "with an average reward of: -998.8004798080767\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3686474\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -999.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1801      |\n",
      "|    time_elapsed         | 49362     |\n",
      "|    total_timesteps      | 3688448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0338    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+09  |\n",
      "|    n_updates            | 18000     |\n",
      "|    policy_gradient_loss | -2.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.1e+09   |\n",
      "---------------------------------------\n",
      "Episode 1476  finished with cumulative reward: -6042500.0 and \n",
      "with an average reward of: -2416.0335865653738\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3688975\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2417.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1802      |\n",
      "|    time_elapsed         | 49391     |\n",
      "|    total_timesteps      | 3690496   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0283    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.56e+09  |\n",
      "|    n_updates            | 18010     |\n",
      "|    policy_gradient_loss | -1.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n",
      "Episode 1477  finished with cumulative reward: -4487000.0 and \n",
      "with an average reward of: -1794.0823670531788\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3691476\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1794.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1803      |\n",
      "|    time_elapsed         | 49418     |\n",
      "|    total_timesteps      | 3692544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.16e+08  |\n",
      "|    n_updates            | 18020     |\n",
      "|    policy_gradient_loss | -3.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.98e+09  |\n",
      "---------------------------------------\n",
      "Episode 1478  finished with cumulative reward: -9306500.0 and \n",
      "with an average reward of: -3721.111555377849\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3693977\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3722.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1804      |\n",
      "|    time_elapsed         | 49446     |\n",
      "|    total_timesteps      | 3694592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0286    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.57e+09  |\n",
      "|    n_updates            | 18030     |\n",
      "|    policy_gradient_loss | -1.56e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 1479  finished with cumulative reward: -3135500.0 and \n",
      "with an average reward of: -1253.6985205917633\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3696478\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1254.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.69e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1805          |\n",
      "|    time_elapsed         | 49475         |\n",
      "|    total_timesteps      | 3696640       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.5133992e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0322        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.06e+10      |\n",
      "|    n_updates            | 18040         |\n",
      "|    policy_gradient_loss | -1.13e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.92e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1806      |\n",
      "|    time_elapsed         | 49496     |\n",
      "|    total_timesteps      | 3698688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0387    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.9e+09   |\n",
      "|    n_updates            | 18050     |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.53e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1480  finished with cumulative reward: -10836500.0 and \n",
      "with an average reward of: -4332.866853258696\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3698979\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4334.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.79e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1807      |\n",
      "|    time_elapsed         | 49524     |\n",
      "|    total_timesteps      | 3700736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0312    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.13e+10  |\n",
      "|    n_updates            | 18060     |\n",
      "|    policy_gradient_loss | -1.83e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 1481  finished with cumulative reward: -4334000.0 and \n",
      "with an average reward of: -1732.906837265094\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3701480\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1733.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.79e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1808         |\n",
      "|    time_elapsed         | 49552        |\n",
      "|    total_timesteps      | 3702784      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0403       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.26e+09     |\n",
      "|    n_updates            | 18070        |\n",
      "|    policy_gradient_loss | -4.19e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.13e+10     |\n",
      "------------------------------------------\n",
      "Episode 1482  finished with cumulative reward: -5940500.0 and \n",
      "with an average reward of: -2375.249900039984\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3703981\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2376.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.8e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1809     |\n",
      "|    time_elapsed         | 49580    |\n",
      "|    total_timesteps      | 3704832  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0275   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 7.68e+09 |\n",
      "|    n_updates            | 18080    |\n",
      "|    policy_gradient_loss | -1.6e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 1.26e+10 |\n",
      "--------------------------------------\n",
      "Episode 1483  finished with cumulative reward: -1962500.0 and \n",
      "with an average reward of: -784.6861255497801\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3706482\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -785.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1810      |\n",
      "|    time_elapsed         | 49608     |\n",
      "|    total_timesteps      | 3706880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0319    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.93e+09  |\n",
      "|    n_updates            | 18090     |\n",
      "|    policy_gradient_loss | -1.97e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.21e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.67e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1811         |\n",
      "|    time_elapsed         | 49629        |\n",
      "|    total_timesteps      | 3708928      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.529728e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.075        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.01e+08     |\n",
      "|    n_updates            | 18100        |\n",
      "|    policy_gradient_loss | -7.88e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.02e+08     |\n",
      "------------------------------------------\n",
      "Episode 1484  finished with cumulative reward: -2217500.0 and \n",
      "with an average reward of: -886.6453418632547\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3708983\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -887.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1812      |\n",
      "|    time_elapsed         | 49657     |\n",
      "|    total_timesteps      | 3710976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.038     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.52e+09  |\n",
      "|    n_updates            | 18110     |\n",
      "|    policy_gradient_loss | -1.48e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.88e+09  |\n",
      "---------------------------------------\n",
      "Episode 1485  finished with cumulative reward: -2931500.0 and \n",
      "with an average reward of: -1172.1311475409836\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3711484\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1172.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.6e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1813         |\n",
      "|    time_elapsed         | 49686        |\n",
      "|    total_timesteps      | 3713024      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0401       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.68e+09     |\n",
      "|    n_updates            | 18120        |\n",
      "|    policy_gradient_loss | -3.52e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.24e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1486  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3713985\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1814         |\n",
      "|    time_elapsed         | 49715        |\n",
      "|    total_timesteps      | 3715072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0365       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.79e+09     |\n",
      "|    n_updates            | 18130        |\n",
      "|    policy_gradient_loss | -5.75e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.34e+10     |\n",
      "------------------------------------------\n",
      "Episode 1487  finished with cumulative reward: -5303000.0 and \n",
      "with an average reward of: -2120.3518592562973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3716486\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2121.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1815      |\n",
      "|    time_elapsed         | 49743     |\n",
      "|    total_timesteps      | 3717120   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0354    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.4e+09   |\n",
      "|    n_updates            | 18140     |\n",
      "|    policy_gradient_loss | -4.17e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 1488  finished with cumulative reward: -2472500.0 and \n",
      "with an average reward of: -988.6045581767293\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3718987\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -989.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.62e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1816          |\n",
      "|    time_elapsed         | 49771         |\n",
      "|    total_timesteps      | 3719168       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 4.0745363e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.051         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.53e+09      |\n",
      "|    n_updates            | 18150         |\n",
      "|    policy_gradient_loss | -7.87e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.86e+09      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1817      |\n",
      "|    time_elapsed         | 49792     |\n",
      "|    total_timesteps      | 3721216   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0402    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.27e+09  |\n",
      "|    n_updates            | 18160     |\n",
      "|    policy_gradient_loss | -3.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.72e+09  |\n",
      "---------------------------------------\n",
      "Episode 1489  finished with cumulative reward: -4181000.0 and \n",
      "with an average reward of: -1671.7313074770093\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3721488\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1672.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.59e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1818      |\n",
      "|    time_elapsed         | 49820     |\n",
      "|    total_timesteps      | 3723264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0393    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.72e+09  |\n",
      "|    n_updates            | 18170     |\n",
      "|    policy_gradient_loss | -1.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.85e+09  |\n",
      "---------------------------------------\n",
      "Episode 1490  finished with cumulative reward: -9867500.0 and \n",
      "with an average reward of: -3945.421831267493\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3723989\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3947.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1819      |\n",
      "|    time_elapsed         | 49848     |\n",
      "|    total_timesteps      | 3725312   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0331    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.26e+10  |\n",
      "|    n_updates            | 18180     |\n",
      "|    policy_gradient_loss | -9.42e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 1491  finished with cumulative reward: -7139000.0 and \n",
      "with an average reward of: -2854.458216713315\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3726490\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2855.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1820         |\n",
      "|    time_elapsed         | 49876        |\n",
      "|    total_timesteps      | 3727360      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0357       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+10     |\n",
      "|    n_updates            | 18190        |\n",
      "|    policy_gradient_loss | -3.4e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.5e+10      |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1492  finished with cumulative reward: -4155500.0 and \n",
      "with an average reward of: -1661.5353858456617\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3728991\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1662.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1821      |\n",
      "|    time_elapsed         | 49904     |\n",
      "|    total_timesteps      | 3729408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0327    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.92e+09  |\n",
      "|    n_updates            | 18200     |\n",
      "|    policy_gradient_loss | -2.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.39e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1822      |\n",
      "|    time_elapsed         | 49925     |\n",
      "|    total_timesteps      | 3731456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0336    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.32e+09  |\n",
      "|    n_updates            | 18210     |\n",
      "|    policy_gradient_loss | -1.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.52e+09  |\n",
      "---------------------------------------\n",
      "Episode 1493  finished with cumulative reward: -8006000.0 and \n",
      "with an average reward of: -3201.1195521791283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3731492\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3202.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1823      |\n",
      "|    time_elapsed         | 49954     |\n",
      "|    total_timesteps      | 3733504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0335    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1e+10     |\n",
      "|    n_updates            | 18220     |\n",
      "|    policy_gradient_loss | -4.62e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.16e+10  |\n",
      "---------------------------------------\n",
      "Episode 1494  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3733993\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.75e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1824      |\n",
      "|    time_elapsed         | 49982     |\n",
      "|    total_timesteps      | 3735552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0333    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.48e+09  |\n",
      "|    n_updates            | 18230     |\n",
      "|    policy_gradient_loss | -2.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.64e+10  |\n",
      "---------------------------------------\n",
      "Episode 1495  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3736494\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.8e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1825      |\n",
      "|    time_elapsed         | 50010     |\n",
      "|    total_timesteps      | 3737600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.031     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.82e+09  |\n",
      "|    n_updates            | 18240     |\n",
      "|    policy_gradient_loss | -1.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.71e+10  |\n",
      "---------------------------------------\n",
      "Episode 1496  finished with cumulative reward: -6221000.0 and \n",
      "with an average reward of: -2487.405037984806\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3738995\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2488.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1826      |\n",
      "|    time_elapsed         | 50039     |\n",
      "|    total_timesteps      | 3739648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0371    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.42e+09  |\n",
      "|    n_updates            | 18250     |\n",
      "|    policy_gradient_loss | -4.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.35e+10  |\n",
      "---------------------------------------\n",
      "Episode 1497  finished with cumulative reward: -1350500.0 and \n",
      "with an average reward of: -539.984006397441\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3741496\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -540.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.77e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1827         |\n",
      "|    time_elapsed         | 50066        |\n",
      "|    total_timesteps      | 3741696      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0377       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.36e+09     |\n",
      "|    n_updates            | 18260        |\n",
      "|    policy_gradient_loss | -3.17e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 7.45e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.77e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1828          |\n",
      "|    time_elapsed         | 50087         |\n",
      "|    total_timesteps      | 3743744       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.6065695e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0552        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.05e+09      |\n",
      "|    n_updates            | 18270         |\n",
      "|    policy_gradient_loss | -6.78e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.74e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1498  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3743997\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1829      |\n",
      "|    time_elapsed         | 50115     |\n",
      "|    total_timesteps      | 3745792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0505    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.38e+09  |\n",
      "|    n_updates            | 18280     |\n",
      "|    policy_gradient_loss | -2.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.36e+09  |\n",
      "---------------------------------------\n",
      "Episode 1499  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3746498\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1830      |\n",
      "|    time_elapsed         | 50143     |\n",
      "|    total_timesteps      | 3747840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0383    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4e+09     |\n",
      "|    n_updates            | 18290     |\n",
      "|    policy_gradient_loss | -1.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.19e+09  |\n",
      "---------------------------------------\n",
      "Episode 1500  finished with cumulative reward: -7725500.0 and \n",
      "with an average reward of: -3088.9644142343063\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3748999\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3090.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1831      |\n",
      "|    time_elapsed         | 50171     |\n",
      "|    total_timesteps      | 3749888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0398    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.02e+10  |\n",
      "|    n_updates            | 18300     |\n",
      "|    policy_gradient_loss | -2.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n",
      "Episode 1501  finished with cumulative reward: -8108000.0 and \n",
      "with an average reward of: -3241.903238704518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3751500\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3243.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.62e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1832      |\n",
      "|    time_elapsed         | 50200     |\n",
      "|    total_timesteps      | 3751936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0267    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.05e+10  |\n",
      "|    n_updates            | 18310     |\n",
      "|    policy_gradient_loss | -8.76e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.07e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.62e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1833         |\n",
      "|    time_elapsed         | 50221        |\n",
      "|    total_timesteps      | 3753984      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0273       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.52e+09     |\n",
      "|    n_updates            | 18320        |\n",
      "|    policy_gradient_loss | -4.57e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.54e+10     |\n",
      "------------------------------------------\n",
      "Episode 1502  finished with cumulative reward: -866000.0 and \n",
      "with an average reward of: -346.2614954018393\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3754001\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -346.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.58e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1834          |\n",
      "|    time_elapsed         | 50250         |\n",
      "|    total_timesteps      | 3756032       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0397        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.1e+09       |\n",
      "|    n_updates            | 18330         |\n",
      "|    policy_gradient_loss | -4.11e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.18e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1503  finished with cumulative reward: -9663500.0 and \n",
      "with an average reward of: -3863.8544582167133\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3756502\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3865.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1835         |\n",
      "|    time_elapsed         | 50278        |\n",
      "|    total_timesteps      | 3758080      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 9.313226e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0388       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.21e+10     |\n",
      "|    n_updates            | 18340        |\n",
      "|    policy_gradient_loss | -6.75e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.18e+10     |\n",
      "------------------------------------------\n",
      "Episode 1504  finished with cumulative reward: -2753000.0 and \n",
      "with an average reward of: -1100.7596961215513\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3759003\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1101.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1836      |\n",
      "|    time_elapsed         | 50306     |\n",
      "|    total_timesteps      | 3760128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0402    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.06e+09  |\n",
      "|    n_updates            | 18350     |\n",
      "|    policy_gradient_loss | -1.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.3e+09   |\n",
      "---------------------------------------\n",
      "Episode 1505  finished with cumulative reward: -5991500.0 and \n",
      "with an average reward of: -2395.641743302679\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3761504\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2396.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.52e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1837          |\n",
      "|    time_elapsed         | 50333         |\n",
      "|    total_timesteps      | 3762176       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2514647e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0379        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.42e+09      |\n",
      "|    n_updates            | 18360         |\n",
      "|    policy_gradient_loss | -6.31e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 6.55e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1506  finished with cumulative reward: -7113500.0 and \n",
      "with an average reward of: -2844.2622950819673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3764005\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2845.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.57e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1838         |\n",
      "|    time_elapsed         | 50361        |\n",
      "|    total_timesteps      | 3764224      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0399       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.05e+10     |\n",
      "|    n_updates            | 18370        |\n",
      "|    policy_gradient_loss | -4e-06       |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.3e+10      |\n",
      "------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.57e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1839          |\n",
      "|    time_elapsed         | 50382         |\n",
      "|    total_timesteps      | 3766272       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0442        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.51e+09      |\n",
      "|    n_updates            | 18380         |\n",
      "|    policy_gradient_loss | -6.43e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.49e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1507  finished with cumulative reward: -3467000.0 and \n",
      "with an average reward of: -1386.2455017992802\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3766506\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1386.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.57e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1840      |\n",
      "|    time_elapsed         | 50411     |\n",
      "|    total_timesteps      | 3768320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0465    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.89e+09  |\n",
      "|    n_updates            | 18390     |\n",
      "|    policy_gradient_loss | -5.01e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.13e+09  |\n",
      "---------------------------------------\n",
      "Episode 1508  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3769007\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.58e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1841      |\n",
      "|    time_elapsed         | 50440     |\n",
      "|    total_timesteps      | 3770368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0388    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.81e+09  |\n",
      "|    n_updates            | 18400     |\n",
      "|    policy_gradient_loss | -2.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.43e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1509  finished with cumulative reward: -4410500.0 and \n",
      "with an average reward of: -1763.4946021591363\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3771508\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1764.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.53e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1842      |\n",
      "|    time_elapsed         | 50468     |\n",
      "|    total_timesteps      | 3772416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0441    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.73e+08  |\n",
      "|    n_updates            | 18410     |\n",
      "|    policy_gradient_loss | -2.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.9e+09   |\n",
      "---------------------------------------\n",
      "Episode 1510  finished with cumulative reward: -6246500.0 and \n",
      "with an average reward of: -2497.6009596161534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3774009\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2498.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.51e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1843          |\n",
      "|    time_elapsed         | 50495         |\n",
      "|    total_timesteps      | 3774464       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0386        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.87e+09      |\n",
      "|    n_updates            | 18420         |\n",
      "|    policy_gradient_loss | -6.8e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.14e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1511  finished with cumulative reward: -3849500.0 and \n",
      "with an average reward of: -1539.1843262694922\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3776510\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1539.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1844      |\n",
      "|    time_elapsed         | 50524     |\n",
      "|    total_timesteps      | 3776512   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.041     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.05e+09  |\n",
      "|    n_updates            | 18430     |\n",
      "|    policy_gradient_loss | -3.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.15e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.51e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1845      |\n",
      "|    time_elapsed         | 50545     |\n",
      "|    total_timesteps      | 3778560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0506    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.21e+09  |\n",
      "|    n_updates            | 18440     |\n",
      "|    policy_gradient_loss | -3.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.31e+09  |\n",
      "---------------------------------------\n",
      "Episode 1512  finished with cumulative reward: -8184500.0 and \n",
      "with an average reward of: -3272.4910035985604\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3779011\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3273.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.56e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1846      |\n",
      "|    time_elapsed         | 50572     |\n",
      "|    total_timesteps      | 3780608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0449    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.57e+09  |\n",
      "|    n_updates            | 18450     |\n",
      "|    policy_gradient_loss | -1.89e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.78e+09  |\n",
      "---------------------------------------\n",
      "Episode 1513  finished with cumulative reward: -4742000.0 and \n",
      "with an average reward of: -1896.0415833666534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3781512\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1896.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.6e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1847         |\n",
      "|    time_elapsed         | 50600        |\n",
      "|    total_timesteps      | 3782656      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.03         |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.79e+10     |\n",
      "|    n_updates            | 18460        |\n",
      "|    policy_gradient_loss | -5.27e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.46e+10     |\n",
      "------------------------------------------\n",
      "Episode 1514  finished with cumulative reward: -3849500.0 and \n",
      "with an average reward of: -1539.1843262694922\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3784013\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1539.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -5.6e+06 |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1848     |\n",
      "|    time_elapsed         | 50629    |\n",
      "|    total_timesteps      | 3784704  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0356   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 2.27e+09 |\n",
      "|    n_updates            | 18470    |\n",
      "|    policy_gradient_loss | -1.3e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 8.8e+09  |\n",
      "--------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1515  finished with cumulative reward: -8745500.0 and \n",
      "with an average reward of: -3496.8012794882047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3786514\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3498.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1849          |\n",
      "|    time_elapsed         | 50658         |\n",
      "|    total_timesteps      | 3786752       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0522        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.03e+09      |\n",
      "|    n_updates            | 18480         |\n",
      "|    policy_gradient_loss | -4.19e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 4.09e+09      |\n",
      "-------------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1850          |\n",
      "|    time_elapsed         | 50679         |\n",
      "|    total_timesteps      | 3788800       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.7462298e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0336        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.2e+09       |\n",
      "|    n_updates            | 18490         |\n",
      "|    policy_gradient_loss | -5.29e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.18e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1516  finished with cumulative reward: -5685500.0 and \n",
      "with an average reward of: -2273.2906837265095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3789015\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2274.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1851      |\n",
      "|    time_elapsed         | 50708     |\n",
      "|    total_timesteps      | 3790848   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.17e+09  |\n",
      "|    n_updates            | 18500     |\n",
      "|    policy_gradient_loss | -2.18e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 1517  finished with cumulative reward: -8618000.0 and \n",
      "with an average reward of: -3445.8216713314673\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3791516\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3447.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1852      |\n",
      "|    time_elapsed         | 50736     |\n",
      "|    total_timesteps      | 3792896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.3e+09   |\n",
      "|    n_updates            | 18510     |\n",
      "|    policy_gradient_loss | -1.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.97e+10  |\n",
      "---------------------------------------\n",
      "Episode 1518  finished with cumulative reward: -3492500.0 and \n",
      "with an average reward of: -1396.4414234306278\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3794017\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1397.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1853      |\n",
      "|    time_elapsed         | 50764     |\n",
      "|    total_timesteps      | 3794944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0333    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.48e+09  |\n",
      "|    n_updates            | 18520     |\n",
      "|    policy_gradient_loss | -1.62e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.91e+09  |\n",
      "---------------------------------------\n",
      "Episode 1519  finished with cumulative reward: -3977000.0 and \n",
      "with an average reward of: -1590.1639344262296\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3796518\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1590.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1854      |\n",
      "|    time_elapsed         | 50791     |\n",
      "|    total_timesteps      | 3796992   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0478    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.13e+09  |\n",
      "|    n_updates            | 18530     |\n",
      "|    policy_gradient_loss | -9.42e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.85e+09  |\n",
      "---------------------------------------\n",
      "Episode 1520  finished with cumulative reward: -1707500.0 and \n",
      "with an average reward of: -682.7269092363055\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3799019\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -683.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.63e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1855      |\n",
      "|    time_elapsed         | 50819     |\n",
      "|    total_timesteps      | 3799040   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0568    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.56e+09  |\n",
      "|    n_updates            | 18540     |\n",
      "|    policy_gradient_loss | -2.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.02e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.63e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1856         |\n",
      "|    time_elapsed         | 50840        |\n",
      "|    total_timesteps      | 3801088      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 4.656613e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0629       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.76e+08     |\n",
      "|    n_updates            | 18550        |\n",
      "|    policy_gradient_loss | -9.31e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 3.08e+09     |\n",
      "------------------------------------------\n",
      "Episode 1521  finished with cumulative reward: -7700000.0 and \n",
      "with an average reward of: -3078.7684926029588\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3801520\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3080.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1857      |\n",
      "|    time_elapsed         | 50868     |\n",
      "|    total_timesteps      | 3803136   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0362    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.17e+09  |\n",
      "|    n_updates            | 18560     |\n",
      "|    policy_gradient_loss | -2.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 1522  finished with cumulative reward: -10581500.0 and \n",
      "with an average reward of: -4230.907636945222\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3804021\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4232.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1858      |\n",
      "|    time_elapsed         | 50896     |\n",
      "|    total_timesteps      | 3805184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0336    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.54e+10  |\n",
      "|    n_updates            | 18570     |\n",
      "|    policy_gradient_loss | -7.78e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.6e+10   |\n",
      "---------------------------------------\n",
      "Episode 1523  finished with cumulative reward: -5048000.0 and \n",
      "with an average reward of: -2018.392642942823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3806522\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2019.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.65e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1859          |\n",
      "|    time_elapsed         | 50925         |\n",
      "|    total_timesteps      | 3807232       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.8230716e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0444        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.1e+09       |\n",
      "|    n_updates            | 18580         |\n",
      "|    policy_gradient_loss | -1.14e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 8.98e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1524  finished with cumulative reward: -8465000.0 and \n",
      "with an average reward of: -3384.646141543383\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3809023\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3386.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1860      |\n",
      "|    time_elapsed         | 50954     |\n",
      "|    total_timesteps      | 3809280   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0312    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.1e+10   |\n",
      "|    n_updates            | 18590     |\n",
      "|    policy_gradient_loss | -1.63e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.08e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1861      |\n",
      "|    time_elapsed         | 50975     |\n",
      "|    total_timesteps      | 3811328   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0407    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.02e+09  |\n",
      "|    n_updates            | 18600     |\n",
      "|    policy_gradient_loss | -1.46e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.59e+09  |\n",
      "---------------------------------------\n",
      "Episode 1525  finished with cumulative reward: -5481500.0 and \n",
      "with an average reward of: -2191.72331067573\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3811524\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2192.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.71e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1862      |\n",
      "|    time_elapsed         | 51003     |\n",
      "|    total_timesteps      | 3813376   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0391    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.66e+09  |\n",
      "|    n_updates            | 18610     |\n",
      "|    policy_gradient_loss | -1.9e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.37e+10  |\n",
      "---------------------------------------\n",
      "Episode 1526  finished with cumulative reward: -4130000.0 and \n",
      "with an average reward of: -1651.3394642143144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3814025\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1652.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.67e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1863         |\n",
      "|    time_elapsed         | 51031        |\n",
      "|    total_timesteps      | 3815424      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0427       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.86e+09     |\n",
      "|    n_updates            | 18620        |\n",
      "|    policy_gradient_loss | -5.38e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.76e+09     |\n",
      "------------------------------------------\n",
      "Episode 1527  finished with cumulative reward: -6603500.0 and \n",
      "with an average reward of: -2640.343862455018\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3816526\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2641.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1864      |\n",
      "|    time_elapsed         | 51059     |\n",
      "|    total_timesteps      | 3817472   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0457    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.23e+09  |\n",
      "|    n_updates            | 18630     |\n",
      "|    policy_gradient_loss | -1.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.07e+09  |\n",
      "---------------------------------------\n",
      "Episode 1528  finished with cumulative reward: -5150000.0 and \n",
      "with an average reward of: -2059.1763294682128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3819027\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2060.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1865      |\n",
      "|    time_elapsed         | 51087     |\n",
      "|    total_timesteps      | 3819520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0359    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.96e+09  |\n",
      "|    n_updates            | 18640     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "Episode 1529  finished with cumulative reward: -3033500.0 and \n",
      "with an average reward of: -1212.9148340663735\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3821528\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1213.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1866      |\n",
      "|    time_elapsed         | 51116     |\n",
      "|    total_timesteps      | 3821568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0474    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.53e+09  |\n",
      "|    n_updates            | 18650     |\n",
      "|    policy_gradient_loss | -1.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.77e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1867      |\n",
      "|    time_elapsed         | 51137     |\n",
      "|    total_timesteps      | 3823616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0419    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.14e+09  |\n",
      "|    n_updates            | 18660     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.38e+09  |\n",
      "---------------------------------------\n",
      "Episode 1530  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3824029\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.66e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1868         |\n",
      "|    time_elapsed         | 51167        |\n",
      "|    total_timesteps      | 3825664      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0391       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.39e+09     |\n",
      "|    n_updates            | 18670        |\n",
      "|    policy_gradient_loss | -2.3e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.94e+09     |\n",
      "------------------------------------------\n",
      "Episode 1531  finished with cumulative reward: -11346500.0 and \n",
      "with an average reward of: -4536.785285885646\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3826530\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4538.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1869      |\n",
      "|    time_elapsed         | 51195     |\n",
      "|    total_timesteps      | 3827712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0311    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.24e+10  |\n",
      "|    n_updates            | 18680     |\n",
      "|    policy_gradient_loss | -1.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.44e+10  |\n",
      "---------------------------------------\n",
      "Episode 1532  finished with cumulative reward: -8057000.0 and \n",
      "with an average reward of: -3221.5113954418234\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3829031\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3222.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1870      |\n",
      "|    time_elapsed         | 51223     |\n",
      "|    total_timesteps      | 3829760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.17e+09  |\n",
      "|    n_updates            | 18690     |\n",
      "|    policy_gradient_loss | -9.81e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.77e+10  |\n",
      "---------------------------------------\n",
      "Episode 1533  finished with cumulative reward: -2804000.0 and \n",
      "with an average reward of: -1121.1515393842462\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3831532\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1121.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.68e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1871          |\n",
      "|    time_elapsed         | 51251         |\n",
      "|    total_timesteps      | 3831808       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.037         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.89e+09      |\n",
      "|    n_updates            | 18700         |\n",
      "|    policy_gradient_loss | -4.3e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.21e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.68e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1872      |\n",
      "|    time_elapsed         | 51272     |\n",
      "|    total_timesteps      | 3833856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0308    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.53e+09  |\n",
      "|    n_updates            | 18710     |\n",
      "|    policy_gradient_loss | -1.93e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.07e+09  |\n",
      "---------------------------------------\n",
      "Episode 1534  finished with cumulative reward: -14483000.0 and \n",
      "with an average reward of: -5790.883646541383\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3834033\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5793.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.79e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1873      |\n",
      "|    time_elapsed         | 51300     |\n",
      "|    total_timesteps      | 3835904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0253    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.45e+10  |\n",
      "|    n_updates            | 18720     |\n",
      "|    policy_gradient_loss | -3.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.68e+10  |\n",
      "---------------------------------------\n",
      "Episode 1535  finished with cumulative reward: -4716500.0 and \n",
      "with an average reward of: -1885.8456617353058\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3836534\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1886.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.76e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1874      |\n",
      "|    time_elapsed         | 51328     |\n",
      "|    total_timesteps      | 3837952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0379    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.67e+09  |\n",
      "|    n_updates            | 18730     |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.2e+09   |\n",
      "---------------------------------------\n",
      "Episode 1536  finished with cumulative reward: -13361000.0 and \n",
      "with an average reward of: -5342.2630947620955\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3839035\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -5344.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1875      |\n",
      "|    time_elapsed         | 51356     |\n",
      "|    total_timesteps      | 3840000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0305    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.54e+10  |\n",
      "|    n_updates            | 18740     |\n",
      "|    policy_gradient_loss | -2.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.43e+10  |\n",
      "---------------------------------------\n",
      "Episode 1537  finished with cumulative reward: -3773000.0 and \n",
      "with an average reward of: -1508.5965613754497\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3841536\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1509.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1876      |\n",
      "|    time_elapsed         | 51384     |\n",
      "|    total_timesteps      | 3842048   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0414    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.37e+09  |\n",
      "|    n_updates            | 18750     |\n",
      "|    policy_gradient_loss | -2.51e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.46e+10  |\n",
      "---------------------------------------\n",
      "Episode 1538  finished with cumulative reward: -5634500.0 and \n",
      "with an average reward of: -2252.8988404638144\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3844037\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2253.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.82e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1877      |\n",
      "|    time_elapsed         | 51412     |\n",
      "|    total_timesteps      | 3844096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0325    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.42e+09  |\n",
      "|    n_updates            | 18760     |\n",
      "|    policy_gradient_loss | -2.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.21e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.82e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1878      |\n",
      "|    time_elapsed         | 51434     |\n",
      "|    total_timesteps      | 3846144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.034     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.76e+09  |\n",
      "|    n_updates            | 18770     |\n",
      "|    policy_gradient_loss | -1.55e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.31e+10  |\n",
      "---------------------------------------\n",
      "Episode 1539  finished with cumulative reward: -3849500.0 and \n",
      "with an average reward of: -1539.1843262694922\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3846538\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1539.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.78e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1879      |\n",
      "|    time_elapsed         | 51461     |\n",
      "|    total_timesteps      | 3848192   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0372    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+09  |\n",
      "|    n_updates            | 18780     |\n",
      "|    policy_gradient_loss | -2.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 1540  finished with cumulative reward: -6654500.0 and \n",
      "with an average reward of: -2660.735705717713\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3849039\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2661.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.74e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1880          |\n",
      "|    time_elapsed         | 51490         |\n",
      "|    total_timesteps      | 3850240       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3387762e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0321        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 7.39e+09      |\n",
      "|    n_updates            | 18790         |\n",
      "|    policy_gradient_loss | -6.05e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.49e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1541  finished with cumulative reward: -993500.0 and \n",
      "with an average reward of: -397.2411035585766\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3851540\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -397.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.7e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1881          |\n",
      "|    time_elapsed         | 51518         |\n",
      "|    total_timesteps      | 3852288       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 6.4028427e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0373        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 5.77e+09      |\n",
      "|    n_updates            | 18800         |\n",
      "|    policy_gradient_loss | -9.21e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.41e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1542  finished with cumulative reward: -8388500.0 and \n",
      "with an average reward of: -3354.05837664934\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3854041\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3355.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.73e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1882         |\n",
      "|    time_elapsed         | 51546        |\n",
      "|    total_timesteps      | 3854336      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0368       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.95e+09     |\n",
      "|    n_updates            | 18810        |\n",
      "|    policy_gradient_loss | -4.83e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.02e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1883      |\n",
      "|    time_elapsed         | 51567     |\n",
      "|    total_timesteps      | 3856384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0286    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.19e+09  |\n",
      "|    n_updates            | 18820     |\n",
      "|    policy_gradient_loss | -6.25e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 1543  finished with cumulative reward: -4155500.0 and \n",
      "with an average reward of: -1661.5353858456617\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3856542\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1662.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1884      |\n",
      "|    time_elapsed         | 51595     |\n",
      "|    total_timesteps      | 3858432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0351    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.83e+09  |\n",
      "|    n_updates            | 18830     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.65e+09  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1544  finished with cumulative reward: -3237500.0 and \n",
      "with an average reward of: -1294.4822071171532\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3859043\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1295.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1885      |\n",
      "|    time_elapsed         | 51623     |\n",
      "|    total_timesteps      | 3860480   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0438    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.14e+09  |\n",
      "|    n_updates            | 18840     |\n",
      "|    policy_gradient_loss | -1.32e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.89e+09  |\n",
      "---------------------------------------\n",
      "Episode 1545  finished with cumulative reward: -4334000.0 and \n",
      "with an average reward of: -1732.906837265094\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3861544\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1733.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1886      |\n",
      "|    time_elapsed         | 51651     |\n",
      "|    total_timesteps      | 3862528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0388    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.16e+09  |\n",
      "|    n_updates            | 18850     |\n",
      "|    policy_gradient_loss | -1.47e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.13e+09  |\n",
      "---------------------------------------\n",
      "Episode 1546  finished with cumulative reward: -1350500.0 and \n",
      "with an average reward of: -539.984006397441\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3864045\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -540.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.67e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1887         |\n",
      "|    time_elapsed         | 51679        |\n",
      "|    total_timesteps      | 3864576      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 7.014023e-09 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0328       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.26e+09     |\n",
      "|    n_updates            | 18860        |\n",
      "|    policy_gradient_loss | -1.99e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9e+09        |\n",
      "------------------------------------------\n",
      "Episode 1547  finished with cumulative reward: -4512500.0 and \n",
      "with an average reward of: -1804.2782886845262\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3866546\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1805.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1888      |\n",
      "|    time_elapsed         | 51707     |\n",
      "|    total_timesteps      | 3866624   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0355    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.02e+09  |\n",
      "|    n_updates            | 18870     |\n",
      "|    policy_gradient_loss | -1.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.42e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1889      |\n",
      "|    time_elapsed         | 51729     |\n",
      "|    total_timesteps      | 3868672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0258    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.76e+09  |\n",
      "|    n_updates            | 18880     |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.48e+09  |\n",
      "---------------------------------------\n",
      "Episode 1548  finished with cumulative reward: -4512500.0 and \n",
      "with an average reward of: -1804.2782886845262\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3869047\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1805.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.66e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1890      |\n",
      "|    time_elapsed         | 51757     |\n",
      "|    total_timesteps      | 3870720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0289    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.36e+09  |\n",
      "|    n_updates            | 18890     |\n",
      "|    policy_gradient_loss | -1.54e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.15e+10  |\n",
      "---------------------------------------\n",
      "Episode 1549  finished with cumulative reward: -6323000.0 and \n",
      "with an average reward of: -2528.1887245101957\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3871548\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2529.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1891      |\n",
      "|    time_elapsed         | 51785     |\n",
      "|    total_timesteps      | 3872768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0299    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.31e+09  |\n",
      "|    n_updates            | 18900     |\n",
      "|    policy_gradient_loss | -1.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1550  finished with cumulative reward: -8745500.0 and \n",
      "with an average reward of: -3496.8012794882047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3874049\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3498.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.7e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1892         |\n",
      "|    time_elapsed         | 51813        |\n",
      "|    total_timesteps      | 3874816      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0351       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.28e+10     |\n",
      "|    n_updates            | 18910        |\n",
      "|    policy_gradient_loss | -1.02e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.76e+10     |\n",
      "------------------------------------------\n",
      "Episode 1551  finished with cumulative reward: -6297500.0 and \n",
      "with an average reward of: -2517.9928028788486\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3876550\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2519.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1893      |\n",
      "|    time_elapsed         | 51842     |\n",
      "|    total_timesteps      | 3876864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.031     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.59e+09  |\n",
      "|    n_updates            | 18920     |\n",
      "|    policy_gradient_loss | -1.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.9e+10   |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.7e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1894      |\n",
      "|    time_elapsed         | 51863     |\n",
      "|    total_timesteps      | 3878912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0371    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.85e+09  |\n",
      "|    n_updates            | 18930     |\n",
      "|    policy_gradient_loss | -1.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.53e+09  |\n",
      "---------------------------------------\n",
      "Episode 1552  finished with cumulative reward: -3135500.0 and \n",
      "with an average reward of: -1253.6985205917633\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3879051\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1254.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.67e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1895          |\n",
      "|    time_elapsed         | 51892         |\n",
      "|    total_timesteps      | 3880960       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.3283064e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0317        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.17e+09      |\n",
      "|    n_updates            | 18940         |\n",
      "|    policy_gradient_loss | -2.95e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.22e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1553  finished with cumulative reward: -2345000.0 and \n",
      "with an average reward of: -937.624950019992\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3881552\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -938.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1896      |\n",
      "|    time_elapsed         | 51920     |\n",
      "|    total_timesteps      | 3883008   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0377    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.56e+09  |\n",
      "|    n_updates            | 18950     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.1e+09   |\n",
      "---------------------------------------\n",
      "Episode 1554  finished with cumulative reward: -9918500.0 and \n",
      "with an average reward of: -3965.813674530188\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3884053\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3967.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1897      |\n",
      "|    time_elapsed         | 51949     |\n",
      "|    total_timesteps      | 3885056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.033     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.85e+09  |\n",
      "|    n_updates            | 18960     |\n",
      "|    policy_gradient_loss | -2.71e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.48e+10  |\n",
      "---------------------------------------\n",
      "Episode 1555  finished with cumulative reward: -1784000.0 and \n",
      "with an average reward of: -713.3146741303478\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3886554\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -713.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.72e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1898         |\n",
      "|    time_elapsed         | 51977        |\n",
      "|    total_timesteps      | 3887104      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0315       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.96e+09     |\n",
      "|    n_updates            | 18970        |\n",
      "|    policy_gradient_loss | -3.29e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.44e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1556  finished with cumulative reward: -4359500.0 and \n",
      "with an average reward of: -1743.1027588964414\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3889055\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1743.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1899      |\n",
      "|    time_elapsed         | 52005     |\n",
      "|    total_timesteps      | 3889152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0354    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.14e+09  |\n",
      "|    n_updates            | 18980     |\n",
      "|    policy_gradient_loss | -1.61e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.72e+09  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.73e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1900         |\n",
      "|    time_elapsed         | 52026        |\n",
      "|    total_timesteps      | 3891200      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0345       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.04e+09     |\n",
      "|    n_updates            | 18990        |\n",
      "|    policy_gradient_loss | -6.44e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.7e+09      |\n",
      "------------------------------------------\n",
      "Episode 1557  finished with cumulative reward: -228500.0 and \n",
      "with an average reward of: -91.36345461815274\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3891556\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -91.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.65e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1901         |\n",
      "|    time_elapsed         | 52054        |\n",
      "|    total_timesteps      | 3893248      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0502       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.33e+08     |\n",
      "|    n_updates            | 19000        |\n",
      "|    policy_gradient_loss | -3.73e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.1e+09      |\n",
      "------------------------------------------\n",
      "Episode 1558  finished with cumulative reward: -8133500.0 and \n",
      "with an average reward of: -3252.0991603358657\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3894057\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3253.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1902      |\n",
      "|    time_elapsed         | 52082     |\n",
      "|    total_timesteps      | 3895296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0291    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.67e+09  |\n",
      "|    n_updates            | 19010     |\n",
      "|    policy_gradient_loss | -5.83e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "Episode 1559  finished with cumulative reward: -6068000.0 and \n",
      "with an average reward of: -2426.2295081967213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3896558\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2427.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1903      |\n",
      "|    time_elapsed         | 52110     |\n",
      "|    total_timesteps      | 3897344   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0412    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.03e+09  |\n",
      "|    n_updates            | 19020     |\n",
      "|    policy_gradient_loss | -5.87e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.54e+09  |\n",
      "---------------------------------------\n",
      "Episode 1560  finished with cumulative reward: -10811000.0 and \n",
      "with an average reward of: -4322.670931627349\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3899059\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4324.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1904      |\n",
      "|    time_elapsed         | 52138     |\n",
      "|    total_timesteps      | 3899392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.02e+09  |\n",
      "|    n_updates            | 19030     |\n",
      "|    policy_gradient_loss | -1.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.64e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.72e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1905      |\n",
      "|    time_elapsed         | 52159     |\n",
      "|    total_timesteps      | 3901440   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0372    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.67e+10  |\n",
      "|    n_updates            | 19040     |\n",
      "|    policy_gradient_loss | -7.83e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.1e+10   |\n",
      "---------------------------------------\n",
      "Episode 1561  finished with cumulative reward: -3926000.0 and \n",
      "with an average reward of: -1569.7720911635347\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3901560\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1570.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1906      |\n",
      "|    time_elapsed         | 52188     |\n",
      "|    total_timesteps      | 3903488   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0329    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.51e+09  |\n",
      "|    n_updates            | 19050     |\n",
      "|    policy_gradient_loss | -9.5e-07  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.13e+09  |\n",
      "---------------------------------------\n",
      "Episode 1562  finished with cumulative reward: -6986000.0 and \n",
      "with an average reward of: -2793.28268692523\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3904061\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2794.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1907      |\n",
      "|    time_elapsed         | 52216     |\n",
      "|    total_timesteps      | 3905536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0356    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.51e+10  |\n",
      "|    n_updates            | 19060     |\n",
      "|    policy_gradient_loss | -2.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.12e+10  |\n",
      "---------------------------------------\n",
      "Episode 1563  finished with cumulative reward: -5583500.0 and \n",
      "with an average reward of: -2232.5069972011197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3906562\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2233.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.69e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1908      |\n",
      "|    time_elapsed         | 52245     |\n",
      "|    total_timesteps      | 3907584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0292    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.98e+09  |\n",
      "|    n_updates            | 19070     |\n",
      "|    policy_gradient_loss | -1.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 1564  finished with cumulative reward: -3900500.0 and \n",
      "with an average reward of: -1559.5761695321871\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3909063\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1560.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1909      |\n",
      "|    time_elapsed         | 52273     |\n",
      "|    total_timesteps      | 3909632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0339    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.06e+10  |\n",
      "|    n_updates            | 19080     |\n",
      "|    policy_gradient_loss | -1.88e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.36e+10  |\n",
      "---------------------------------------\n",
      "Episode 1565  finished with cumulative reward: -11346500.0 and \n",
      "with an average reward of: -4536.785285885646\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3911564\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4538.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1910      |\n",
      "|    time_elapsed         | 52301     |\n",
      "|    total_timesteps      | 3911680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0346    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.82e+09  |\n",
      "|    n_updates            | 19090     |\n",
      "|    policy_gradient_loss | -1.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.85e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1911      |\n",
      "|    time_elapsed         | 52322     |\n",
      "|    total_timesteps      | 3913728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.041     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.85e+09  |\n",
      "|    n_updates            | 19100     |\n",
      "|    policy_gradient_loss | -1.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.44e+10  |\n",
      "---------------------------------------\n",
      "Episode 1566  finished with cumulative reward: -8541500.0 and \n",
      "with an average reward of: -3415.233906437425\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3914065\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3416.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.74e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1912      |\n",
      "|    time_elapsed         | 52349     |\n",
      "|    total_timesteps      | 3915776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0365    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.11e+10  |\n",
      "|    n_updates            | 19110     |\n",
      "|    policy_gradient_loss | -2.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.06e+10  |\n",
      "---------------------------------------\n",
      "Episode 1567  finished with cumulative reward: -3365000.0 and \n",
      "with an average reward of: -1345.4618152738904\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3916566\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1346.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1913         |\n",
      "|    time_elapsed         | 52378        |\n",
      "|    total_timesteps      | 3917824      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.825982e-08 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.045        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.45e+09     |\n",
      "|    n_updates            | 19120        |\n",
      "|    policy_gradient_loss | -2.69e-05    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 8.19e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1568  finished with cumulative reward: -8975000.0 and \n",
      "with an average reward of: -3588.564574170332\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3919067\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3590.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.71e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1914         |\n",
      "|    time_elapsed         | 52406        |\n",
      "|    total_timesteps      | 3919872      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.036        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.32e+09     |\n",
      "|    n_updates            | 19130        |\n",
      "|    policy_gradient_loss | -2.51e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.12e+10     |\n",
      "------------------------------------------\n",
      "Episode 1569  finished with cumulative reward: -11117000.0 and \n",
      "with an average reward of: -4445.021991203518\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3921568\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4446.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.73e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1915      |\n",
      "|    time_elapsed         | 52434     |\n",
      "|    total_timesteps      | 3921920   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0386    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.05e+09  |\n",
      "|    n_updates            | 19140     |\n",
      "|    policy_gradient_loss | -3.35e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.38e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.73e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1916          |\n",
      "|    time_elapsed         | 52455         |\n",
      "|    total_timesteps      | 3923968       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.9208528e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0347        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.31e+10      |\n",
      "|    n_updates            | 19150         |\n",
      "|    policy_gradient_loss | -9.88e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.84e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1570  finished with cumulative reward: -4971500.0 and \n",
      "with an average reward of: -1987.8048780487804\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3924069\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1988.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.69e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1917         |\n",
      "|    time_elapsed         | 52483        |\n",
      "|    total_timesteps      | 3926016      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0344       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.99e+09     |\n",
      "|    n_updates            | 19160        |\n",
      "|    policy_gradient_loss | -3.68e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.19e+10     |\n",
      "------------------------------------------\n",
      "Episode 1571  finished with cumulative reward: -5048000.0 and \n",
      "with an average reward of: -2018.392642942823\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3926570\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2019.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.67e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1918      |\n",
      "|    time_elapsed         | 52511     |\n",
      "|    total_timesteps      | 3928064   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0321    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+10  |\n",
      "|    n_updates            | 19170     |\n",
      "|    policy_gradient_loss | -1.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.99e+09  |\n",
      "---------------------------------------\n",
      "Episode 1572  finished with cumulative reward: -4028000.0 and \n",
      "with an average reward of: -1610.5557776889245\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3929071\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1611.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.64e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1919      |\n",
      "|    time_elapsed         | 52539     |\n",
      "|    total_timesteps      | 3930112   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0418    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.99e+09  |\n",
      "|    n_updates            | 19180     |\n",
      "|    policy_gradient_loss | -1.5e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.58e+09  |\n",
      "---------------------------------------\n",
      "Episode 1573  finished with cumulative reward: -16268000.0 and \n",
      "with an average reward of: -6504.598160735705\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3931572\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -6507.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.77e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1920         |\n",
      "|    time_elapsed         | 52567        |\n",
      "|    total_timesteps      | 3932160      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0368       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.61e+09     |\n",
      "|    n_updates            | 19190        |\n",
      "|    policy_gradient_loss | -5.31e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.36e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1574  finished with cumulative reward: -6017000.0 and \n",
      "with an average reward of: -2405.837664934026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3934073\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2406.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.8e+06      |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1921          |\n",
      "|    time_elapsed         | 52595         |\n",
      "|    total_timesteps      | 3934208       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0299        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.68e+10      |\n",
      "|    n_updates            | 19200         |\n",
      "|    policy_gradient_loss | -6.48e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 3.83e+10      |\n",
      "-------------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.8e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1922         |\n",
      "|    time_elapsed         | 52616        |\n",
      "|    total_timesteps      | 3936256      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.032        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.07e+09     |\n",
      "|    n_updates            | 19210        |\n",
      "|    policy_gradient_loss | -3.52e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.77e+10     |\n",
      "------------------------------------------\n",
      "Episode 1575  finished with cumulative reward: -5226500.0 and \n",
      "with an average reward of: -2089.764094362255\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3936574\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2090.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.83e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1923          |\n",
      "|    time_elapsed         | 52644         |\n",
      "|    total_timesteps      | 3938304       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.7043508e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0492        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 6.41e+09      |\n",
      "|    n_updates            | 19220         |\n",
      "|    policy_gradient_loss | -1.93e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 9.68e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1576  finished with cumulative reward: -8439500.0 and \n",
      "with an average reward of: -3374.450219912035\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3939075\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3375.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1924      |\n",
      "|    time_elapsed         | 52672     |\n",
      "|    total_timesteps      | 3940352   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.034     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.48e+09  |\n",
      "|    n_updates            | 19230     |\n",
      "|    policy_gradient_loss | -3.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.66e+10  |\n",
      "---------------------------------------\n",
      "Episode 1577  finished with cumulative reward: -7368500.0 and \n",
      "with an average reward of: -2946.2215113954417\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3941576\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2947.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1925         |\n",
      "|    time_elapsed         | 52700        |\n",
      "|    total_timesteps      | 3942400      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0433       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.76e+09     |\n",
      "|    n_updates            | 19240        |\n",
      "|    policy_gradient_loss | -5.19e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.42e+10     |\n",
      "------------------------------------------\n",
      "Episode 1578  finished with cumulative reward: -9306500.0 and \n",
      "with an average reward of: -3721.111555377849\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3944077\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3722.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1926      |\n",
      "|    time_elapsed         | 52729     |\n",
      "|    total_timesteps      | 3944448   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0414    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.44e+09  |\n",
      "|    n_updates            | 19250     |\n",
      "|    policy_gradient_loss | -8.53e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1927         |\n",
      "|    time_elapsed         | 52750        |\n",
      "|    total_timesteps      | 3946496      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0245       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.13e+10     |\n",
      "|    n_updates            | 19260        |\n",
      "|    policy_gradient_loss | -4.86e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.43e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1579  finished with cumulative reward: -6501500.0 and \n",
      "with an average reward of: -2599.5601759296283\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3946578\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2600.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1928      |\n",
      "|    time_elapsed         | 52778     |\n",
      "|    total_timesteps      | 3948544   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.04      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.38e+09  |\n",
      "|    n_updates            | 19270     |\n",
      "|    policy_gradient_loss | -2.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1580  finished with cumulative reward: -4385000.0 and \n",
      "with an average reward of: -1753.298680527789\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3949079\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1754.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.85e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1929      |\n",
      "|    time_elapsed         | 52807     |\n",
      "|    total_timesteps      | 3950592   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0359    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.97e+09  |\n",
      "|    n_updates            | 19280     |\n",
      "|    policy_gradient_loss | -1.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 1581  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3951580\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1930      |\n",
      "|    time_elapsed         | 52834     |\n",
      "|    total_timesteps      | 3952640   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0345    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.46e+09  |\n",
      "|    n_updates            | 19290     |\n",
      "|    policy_gradient_loss | -1.2e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 1582  finished with cumulative reward: -6909500.0 and \n",
      "with an average reward of: -2762.6949220311876\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3954081\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2763.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1931      |\n",
      "|    time_elapsed         | 52863     |\n",
      "|    total_timesteps      | 3954688   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0539    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.51e+09  |\n",
      "|    n_updates            | 19300     |\n",
      "|    policy_gradient_loss | -3.19e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.32e+09  |\n",
      "---------------------------------------\n",
      "Episode 1583  finished with cumulative reward: -993500.0 and \n",
      "with an average reward of: -397.2411035585766\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3956582\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -397.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1932      |\n",
      "|    time_elapsed         | 52891     |\n",
      "|    total_timesteps      | 3956736   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.031     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.97e+09  |\n",
      "|    n_updates            | 19310     |\n",
      "|    policy_gradient_loss | -2.41e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.14e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.83e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1933      |\n",
      "|    time_elapsed         | 52912     |\n",
      "|    total_timesteps      | 3958784   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0239    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.57e+09  |\n",
      "|    n_updates            | 19320     |\n",
      "|    policy_gradient_loss | -1.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.08e+09  |\n",
      "---------------------------------------\n",
      "Episode 1584  finished with cumulative reward: -2855000.0 and \n",
      "with an average reward of: -1141.5433826469412\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3959083\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1142.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.84e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1934      |\n",
      "|    time_elapsed         | 52940     |\n",
      "|    total_timesteps      | 3960832   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0437    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2e+09     |\n",
      "|    n_updates            | 19330     |\n",
      "|    policy_gradient_loss | -3.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.34e+09  |\n",
      "---------------------------------------\n",
      "Episode 1585  finished with cumulative reward: -5201000.0 and \n",
      "with an average reward of: -2079.5681727309075\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3961584\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2080.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.86e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1935      |\n",
      "|    time_elapsed         | 52968     |\n",
      "|    total_timesteps      | 3962880   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0364    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.9e+09   |\n",
      "|    n_updates            | 19340     |\n",
      "|    policy_gradient_loss | -1.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.6e+09   |\n",
      "---------------------------------------\n",
      "Episode 1586  finished with cumulative reward: -6935000.0 and \n",
      "with an average reward of: -2772.890843662535\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3964085\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2774.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1936      |\n",
      "|    time_elapsed         | 52997     |\n",
      "|    total_timesteps      | 3964928   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0381    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.53e+09  |\n",
      "|    n_updates            | 19350     |\n",
      "|    policy_gradient_loss | -1.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.24e+10  |\n",
      "---------------------------------------\n",
      "Episode 1587  finished with cumulative reward: -11219000.0 and \n",
      "with an average reward of: -4485.805677728908\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3966586\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4487.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1937      |\n",
      "|    time_elapsed         | 53025     |\n",
      "|    total_timesteps      | 3966976   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0353    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.14e+10  |\n",
      "|    n_updates            | 19360     |\n",
      "|    policy_gradient_loss | -3.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.32e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1938      |\n",
      "|    time_elapsed         | 53045     |\n",
      "|    total_timesteps      | 3969024   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0273    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.38e+10  |\n",
      "|    n_updates            | 19370     |\n",
      "|    policy_gradient_loss | -6.35e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1588  finished with cumulative reward: -4206500.0 and \n",
      "with an average reward of: -1681.9272291083566\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3969087\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1682.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.95e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1939         |\n",
      "|    time_elapsed         | 53073        |\n",
      "|    total_timesteps      | 3971072      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0374       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 2.77e+09     |\n",
      "|    n_updates            | 19380        |\n",
      "|    policy_gradient_loss | -3.82e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 9.79e+09     |\n",
      "------------------------------------------\n",
      "Episode 1589  finished with cumulative reward: -7445000.0 and \n",
      "with an average reward of: -2976.8092762894844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3971588\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2978.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.99e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1940         |\n",
      "|    time_elapsed         | 53102        |\n",
      "|    total_timesteps      | 3973120      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-10 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0376       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.98e+09     |\n",
      "|    n_updates            | 19390        |\n",
      "|    policy_gradient_loss | -8.21e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.89e+10     |\n",
      "------------------------------------------\n",
      "Episode 1590  finished with cumulative reward: -11321000.0 and \n",
      "with an average reward of: -4526.589364254298\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3974089\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4528.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "--------------------------------------\n",
      "| rollout/                |          |\n",
      "|    ep_len_mean          | 2.5e+03  |\n",
      "|    ep_rew_mean          | -6e+06   |\n",
      "| time/                   |          |\n",
      "|    fps                  | 74       |\n",
      "|    iterations           | 1941     |\n",
      "|    time_elapsed         | 53130    |\n",
      "|    total_timesteps      | 3975168  |\n",
      "| train/                  |          |\n",
      "|    approx_kl            | 0.0      |\n",
      "|    clip_fraction        | 0        |\n",
      "|    clip_range           | 0.2      |\n",
      "|    entropy_loss         | -17      |\n",
      "|    explained_variance   | 0.0365   |\n",
      "|    learning_rate        | 0.0003   |\n",
      "|    loss                 | 1.36e+10 |\n",
      "|    n_updates            | 19400    |\n",
      "|    policy_gradient_loss | -3.5e-06 |\n",
      "|    std                  | 1        |\n",
      "|    value_loss           | 2e+10    |\n",
      "--------------------------------------\n",
      "Episode 1591  finished with cumulative reward: -6017000.0 and \n",
      "with an average reward of: -2405.837664934026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3976590\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2406.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.99e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1942          |\n",
      "|    time_elapsed         | 53158         |\n",
      "|    total_timesteps      | 3977216       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4260877e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0343        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.58e+10      |\n",
      "|    n_updates            | 19410         |\n",
      "|    policy_gradient_loss | -6.04e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.12e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1592  finished with cumulative reward: -7419500.0 and \n",
      "with an average reward of: -2966.613354658137\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3979091\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2967.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1943      |\n",
      "|    time_elapsed         | 53186     |\n",
      "|    total_timesteps      | 3979264   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0285    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.69e+09  |\n",
      "|    n_updates            | 19420     |\n",
      "|    policy_gradient_loss | -2.08e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.87e+10  |\n",
      "---------------------------------------\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.02e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1944          |\n",
      "|    time_elapsed         | 53207         |\n",
      "|    total_timesteps      | 3981312       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.2631062e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0536        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.02e+09      |\n",
      "|    n_updates            | 19430         |\n",
      "|    policy_gradient_loss | -6.45e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 5.73e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1593  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3981592\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1945      |\n",
      "|    time_elapsed         | 53236     |\n",
      "|    total_timesteps      | 3983360   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0457    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.01e+09  |\n",
      "|    n_updates            | 19440     |\n",
      "|    policy_gradient_loss | -1.96e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1594  finished with cumulative reward: -2345000.0 and \n",
      "with an average reward of: -937.624950019992\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3984093\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -938.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1946      |\n",
      "|    time_elapsed         | 53264     |\n",
      "|    total_timesteps      | 3985408   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0388    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.57e+09  |\n",
      "|    n_updates            | 19450     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.54e+09  |\n",
      "---------------------------------------\n",
      "Episode 1595  finished with cumulative reward: -3339500.0 and \n",
      "with an average reward of: -1335.265893642543\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3986594\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1335.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.92e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1947      |\n",
      "|    time_elapsed         | 53292     |\n",
      "|    total_timesteps      | 3987456   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0405    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+08  |\n",
      "|    n_updates            | 19460     |\n",
      "|    policy_gradient_loss | -1.7e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.87e+09  |\n",
      "---------------------------------------\n",
      "Episode 1596  finished with cumulative reward: -4793000.0 and \n",
      "with an average reward of: -1916.4334266293483\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3989095\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1917.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1948      |\n",
      "|    time_elapsed         | 53320     |\n",
      "|    total_timesteps      | 3989504   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0295    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.07e+09  |\n",
      "|    n_updates            | 19470     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.55e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1949      |\n",
      "|    time_elapsed         | 53341     |\n",
      "|    total_timesteps      | 3991552   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0359    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.49e+09  |\n",
      "|    n_updates            | 19480     |\n",
      "|    policy_gradient_loss | -1.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.04e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1597  finished with cumulative reward: -9842000.0 and \n",
      "with an average reward of: -3935.2259096361454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3991596\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3936.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.99e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1950      |\n",
      "|    time_elapsed         | 53370     |\n",
      "|    total_timesteps      | 3993600   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0278    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.61e+10  |\n",
      "|    n_updates            | 19490     |\n",
      "|    policy_gradient_loss | -4.07e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.39e+10  |\n",
      "---------------------------------------\n",
      "Episode 1598  finished with cumulative reward: -2957000.0 and \n",
      "with an average reward of: -1182.327069172331\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3994097\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1182.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1951      |\n",
      "|    time_elapsed         | 53398     |\n",
      "|    total_timesteps      | 3995648   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0354    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.47e+09  |\n",
      "|    n_updates            | 19500     |\n",
      "|    policy_gradient_loss | -1.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.65e+09  |\n",
      "---------------------------------------\n",
      "Episode 1599  finished with cumulative reward: -8388500.0 and \n",
      "with an average reward of: -3354.05837664934\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3996598\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3355.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6e+06        |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1952          |\n",
      "|    time_elapsed         | 53425         |\n",
      "|    total_timesteps      | 3997696       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0326        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.36e+09      |\n",
      "|    n_updates            | 19510         |\n",
      "|    policy_gradient_loss | -6.44e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.4e+10       |\n",
      "-------------------------------------------\n",
      "Episode 1600  finished with cumulative reward: -7037000.0 and \n",
      "with an average reward of: -2813.674530187925\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 3999099\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2814.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1953      |\n",
      "|    time_elapsed         | 53453     |\n",
      "|    total_timesteps      | 3999744   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0362    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.44e+09  |\n",
      "|    n_updates            | 19520     |\n",
      "|    policy_gradient_loss | -2.02e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.18e+10  |\n",
      "---------------------------------------\n",
      "Episode 1601  finished with cumulative reward: -1758500.0 and \n",
      "with an average reward of: -703.1187524990004\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4001600\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -703.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1954      |\n",
      "|    time_elapsed         | 53481     |\n",
      "|    total_timesteps      | 4001792   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0333    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.01e+10  |\n",
      "|    n_updates            | 19530     |\n",
      "|    policy_gradient_loss | -2.38e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.69e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1955      |\n",
      "|    time_elapsed         | 53502     |\n",
      "|    total_timesteps      | 4003840   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0432    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.41e+09  |\n",
      "|    n_updates            | 19540     |\n",
      "|    policy_gradient_loss | -1.34e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.99e+09  |\n",
      "---------------------------------------\n",
      "Episode 1602  finished with cumulative reward: -8669000.0 and \n",
      "with an average reward of: -3466.2135145941625\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4004101\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3467.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1956      |\n",
      "|    time_elapsed         | 53531     |\n",
      "|    total_timesteps      | 4005888   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.039     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.98e+09  |\n",
      "|    n_updates            | 19550     |\n",
      "|    policy_gradient_loss | -1.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.56e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1603  finished with cumulative reward: -5660000.0 and \n",
      "with an average reward of: -2263.094762095162\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4006602\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2264.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1957      |\n",
      "|    time_elapsed         | 53559     |\n",
      "|    total_timesteps      | 4007936   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0409    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.89e+09  |\n",
      "|    n_updates            | 19560     |\n",
      "|    policy_gradient_loss | -1.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.15e+09  |\n",
      "---------------------------------------\n",
      "Episode 1604  finished with cumulative reward: -3518000.0 and \n",
      "with an average reward of: -1406.637345061975\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4009103\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1407.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.98e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1958      |\n",
      "|    time_elapsed         | 53587     |\n",
      "|    total_timesteps      | 4009984   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0377    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.33e+09  |\n",
      "|    n_updates            | 19570     |\n",
      "|    policy_gradient_loss | -1.36e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.62e+10  |\n",
      "---------------------------------------\n",
      "Episode 1605  finished with cumulative reward: -11066000.0 and \n",
      "with an average reward of: -4424.630147940824\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4011604\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4426.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1959      |\n",
      "|    time_elapsed         | 53616     |\n",
      "|    total_timesteps      | 4012032   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0362    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.06e+09  |\n",
      "|    n_updates            | 19580     |\n",
      "|    policy_gradient_loss | -1.33e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.89e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1960      |\n",
      "|    time_elapsed         | 53636     |\n",
      "|    total_timesteps      | 4014080   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.043     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.42e+09  |\n",
      "|    n_updates            | 19590     |\n",
      "|    policy_gradient_loss | -2.8e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.26e+10  |\n",
      "---------------------------------------\n",
      "Episode 1606  finished with cumulative reward: -7572500.0 and \n",
      "with an average reward of: -3027.7888844462213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4014105\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3029.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1961      |\n",
      "|    time_elapsed         | 53664     |\n",
      "|    total_timesteps      | 4016128   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0346    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.72e+09  |\n",
      "|    n_updates            | 19600     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.5e+10   |\n",
      "---------------------------------------\n",
      "Episode 1607  finished with cumulative reward: -4665500.0 and \n",
      "with an average reward of: -1865.453818472611\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4016606\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1866.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1962      |\n",
      "|    time_elapsed         | 53693     |\n",
      "|    total_timesteps      | 4018176   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0392    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.11e+09  |\n",
      "|    n_updates            | 19610     |\n",
      "|    policy_gradient_loss | -2.65e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 1608  finished with cumulative reward: -10377500.0 and \n",
      "with an average reward of: -4149.340263894443\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4019107\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4151.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1963      |\n",
      "|    time_elapsed         | 53721     |\n",
      "|    total_timesteps      | 4020224   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.029     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.16e+09  |\n",
      "|    n_updates            | 19620     |\n",
      "|    policy_gradient_loss | -1.82e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.63e+10  |\n",
      "---------------------------------------\n",
      "Episode 1609  finished with cumulative reward: -7011500.0 and \n",
      "with an average reward of: -2803.4786085565775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4021608\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2804.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.15e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1964         |\n",
      "|    time_elapsed         | 53749        |\n",
      "|    total_timesteps      | 4022272      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.031        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 7.7e+09      |\n",
      "|    n_updates            | 19630        |\n",
      "|    policy_gradient_loss | -2.19e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.97e+10     |\n",
      "------------------------------------------\n",
      "Episode 1610  finished with cumulative reward: -9026000.0 and \n",
      "with an average reward of: -3608.9564174330267\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4024109\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3610.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1965      |\n",
      "|    time_elapsed         | 53777     |\n",
      "|    total_timesteps      | 4024320   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0356    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.66e+09  |\n",
      "|    n_updates            | 19640     |\n",
      "|    policy_gradient_loss | -1.87e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.27e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.18e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1966      |\n",
      "|    time_elapsed         | 53798     |\n",
      "|    total_timesteps      | 4026368   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0351    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.22e+09  |\n",
      "|    n_updates            | 19650     |\n",
      "|    policy_gradient_loss | -1.44e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 1611  finished with cumulative reward: -5481500.0 and \n",
      "with an average reward of: -2191.72331067573\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4026610\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2192.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1967      |\n",
      "|    time_elapsed         | 53826     |\n",
      "|    total_timesteps      | 4028416   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0387    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.09e+09  |\n",
      "|    n_updates            | 19660     |\n",
      "|    policy_gradient_loss | -2.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.23e+10  |\n",
      "---------------------------------------\n",
      "Episode 1612  finished with cumulative reward: -7598000.0 and \n",
      "with an average reward of: -3037.984806077569\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4029111\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3039.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1968      |\n",
      "|    time_elapsed         | 53854     |\n",
      "|    total_timesteps      | 4030464   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0306    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.65e+08  |\n",
      "|    n_updates            | 19670     |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.99e+09  |\n",
      "---------------------------------------\n",
      "Episode 1613  finished with cumulative reward: -6527000.0 and \n",
      "with an average reward of: -2609.756097560976\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4031612\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2610.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.2e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1969         |\n",
      "|    time_elapsed         | 53882        |\n",
      "|    total_timesteps      | 4032512      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0439       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 9.05e+09     |\n",
      "|    n_updates            | 19680        |\n",
      "|    policy_gradient_loss | -4.39e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.42e+10     |\n",
      "------------------------------------------\n",
      "Episode 1614  finished with cumulative reward: -7649000.0 and \n",
      "with an average reward of: -3058.376649340264\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4034113\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3059.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1970      |\n",
      "|    time_elapsed         | 53910     |\n",
      "|    total_timesteps      | 4034560   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.04      |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.16e+10  |\n",
      "|    n_updates            | 19690     |\n",
      "|    policy_gradient_loss | -8.06e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.07e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1971      |\n",
      "|    time_elapsed         | 53931     |\n",
      "|    total_timesteps      | 4036608   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.25e+10  |\n",
      "|    n_updates            | 19700     |\n",
      "|    policy_gradient_loss | -1.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.17e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1615  finished with cumulative reward: -3722000.0 and \n",
      "with an average reward of: -1488.2047181127548\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4036614\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1488.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1972      |\n",
      "|    time_elapsed         | 53960     |\n",
      "|    total_timesteps      | 4038656   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0471    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.43e+08  |\n",
      "|    n_updates            | 19710     |\n",
      "|    policy_gradient_loss | -1.77e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.63e+09  |\n",
      "---------------------------------------\n",
      "Episode 1616  finished with cumulative reward: -5532500.0 and \n",
      "with an average reward of: -2212.1151539384246\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4039115\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2213.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1973      |\n",
      "|    time_elapsed         | 53988     |\n",
      "|    total_timesteps      | 4040704   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0445    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.43e+09  |\n",
      "|    n_updates            | 19720     |\n",
      "|    policy_gradient_loss | -4.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.47e+09  |\n",
      "---------------------------------------\n",
      "Episode 1617  finished with cumulative reward: -6807500.0 and \n",
      "with an average reward of: -2721.911235505798\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4041616\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2723.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1974      |\n",
      "|    time_elapsed         | 54018     |\n",
      "|    total_timesteps      | 4042752   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0406    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.02e+09  |\n",
      "|    n_updates            | 19730     |\n",
      "|    policy_gradient_loss | -1.73e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.63e+09  |\n",
      "---------------------------------------\n",
      "Episode 1618  finished with cumulative reward: -5405000.0 and \n",
      "with an average reward of: -2161.135545781687\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4044117\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2162.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1975      |\n",
      "|    time_elapsed         | 54046     |\n",
      "|    total_timesteps      | 4044800   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0366    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.1e+09   |\n",
      "|    n_updates            | 19740     |\n",
      "|    policy_gradient_loss | -1.22e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 1619  finished with cumulative reward: -9102500.0 and \n",
      "with an average reward of: -3639.5441823270694\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4046618\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3641.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.24e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1976          |\n",
      "|    time_elapsed         | 54074         |\n",
      "|    total_timesteps      | 4046848       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.4551915e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0309        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.41e+10      |\n",
      "|    n_updates            | 19750         |\n",
      "|    policy_gradient_loss | -4.67e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.32e+10      |\n",
      "-------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1977      |\n",
      "|    time_elapsed         | 54095     |\n",
      "|    total_timesteps      | 4048896   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0334    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.05e+09  |\n",
      "|    n_updates            | 19760     |\n",
      "|    policy_gradient_loss | -2.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.75e+10  |\n",
      "---------------------------------------\n",
      "Episode 1620  finished with cumulative reward: -5685500.0 and \n",
      "with an average reward of: -2273.2906837265095\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4049119\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2274.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.28e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1978      |\n",
      "|    time_elapsed         | 54123     |\n",
      "|    total_timesteps      | 4050944   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0443    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.97e+09  |\n",
      "|    n_updates            | 19770     |\n",
      "|    policy_gradient_loss | -1.24e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.06e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1621  finished with cumulative reward: -8592500.0 and \n",
      "with an average reward of: -3435.6257497001197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4051620\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3437.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.29e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1979          |\n",
      "|    time_elapsed         | 54152         |\n",
      "|    total_timesteps      | 4052992       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0314        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.58e+09      |\n",
      "|    n_updates            | 19780         |\n",
      "|    policy_gradient_loss | -4.47e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.17e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1622  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4054121\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.25e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1980          |\n",
      "|    time_elapsed         | 54180         |\n",
      "|    total_timesteps      | 4055040       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 5.5588316e-09 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0334        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 1.54e+10      |\n",
      "|    n_updates            | 19790         |\n",
      "|    policy_gradient_loss | -1.63e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.91e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1623  finished with cumulative reward: -5226500.0 and \n",
      "with an average reward of: -2089.764094362255\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4056622\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2090.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.25e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1981      |\n",
      "|    time_elapsed         | 54208     |\n",
      "|    total_timesteps      | 4057088   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0413    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.52e+09  |\n",
      "|    n_updates            | 19800     |\n",
      "|    policy_gradient_loss | -3.45e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.93e+09  |\n",
      "---------------------------------------\n",
      "Episode 1624  finished with cumulative reward: -2370500.0 and \n",
      "with an average reward of: -947.8208716513394\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4059123\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -948.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.19e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1982         |\n",
      "|    time_elapsed         | 54237        |\n",
      "|    total_timesteps      | 4059136      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0434       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.57e+08     |\n",
      "|    n_updates            | 19810        |\n",
      "|    policy_gradient_loss | -3.79e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.05e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.19e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1983      |\n",
      "|    time_elapsed         | 54258     |\n",
      "|    total_timesteps      | 4061184   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0316    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.25e+09  |\n",
      "|    n_updates            | 19820     |\n",
      "|    policy_gradient_loss | -1.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.03e+09  |\n",
      "---------------------------------------\n",
      "Episode 1625  finished with cumulative reward: -9485000.0 and \n",
      "with an average reward of: -3792.483006797281\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4061624\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3794.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.23e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1984      |\n",
      "|    time_elapsed         | 54286     |\n",
      "|    total_timesteps      | 4063232   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0375    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.22e+10  |\n",
      "|    n_updates            | 19830     |\n",
      "|    policy_gradient_loss | -3.23e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.11e+10  |\n",
      "---------------------------------------\n",
      "Episode 1626  finished with cumulative reward: -2166500.0 and \n",
      "with an average reward of: -866.2534986005597\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4064125\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -866.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.21e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1985         |\n",
      "|    time_elapsed         | 54314        |\n",
      "|    total_timesteps      | 4065280      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0318       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.41e+09     |\n",
      "|    n_updates            | 19840        |\n",
      "|    policy_gradient_loss | -5.29e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.27e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1627  finished with cumulative reward: -8516000.0 and \n",
      "with an average reward of: -3405.0379848060775\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4066626\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3406.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.23e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1986         |\n",
      "|    time_elapsed         | 54342        |\n",
      "|    total_timesteps      | 4067328      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0384       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.98e+09     |\n",
      "|    n_updates            | 19850        |\n",
      "|    policy_gradient_loss | -6.51e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.11e+10     |\n",
      "------------------------------------------\n",
      "Episode 1628  finished with cumulative reward: -2345000.0 and \n",
      "with an average reward of: -937.624950019992\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4069127\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -938.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.2e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 1987         |\n",
      "|    time_elapsed         | 54369        |\n",
      "|    total_timesteps      | 4069376      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.036        |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 4.64e+09     |\n",
      "|    n_updates            | 19860        |\n",
      "|    policy_gradient_loss | -3.8e-06     |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.27e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.2e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1988      |\n",
      "|    time_elapsed         | 54390     |\n",
      "|    total_timesteps      | 4071424   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.044     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.93e+09  |\n",
      "|    n_updates            | 19870     |\n",
      "|    policy_gradient_loss | -1.79e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.47e+09  |\n",
      "---------------------------------------\n",
      "Episode 1629  finished with cumulative reward: -1427000.0 and \n",
      "with an average reward of: -570.5717712914834\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4071628\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -570.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.18e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 1989          |\n",
      "|    time_elapsed         | 54418         |\n",
      "|    total_timesteps      | 4073472       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0514        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 2.42e+09      |\n",
      "|    n_updates            | 19880         |\n",
      "|    policy_gradient_loss | -3.49e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 2.83e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1630  finished with cumulative reward: -10862000.0 and \n",
      "with an average reward of: -4343.062774890044\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4074129\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4344.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.24e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1990      |\n",
      "|    time_elapsed         | 54446     |\n",
      "|    total_timesteps      | 4075520   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0399    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 8.09e+09  |\n",
      "|    n_updates            | 19890     |\n",
      "|    policy_gradient_loss | -1.87e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.77e+10  |\n",
      "---------------------------------------\n",
      "Episode 1631  finished with cumulative reward: -4640000.0 and \n",
      "with an average reward of: -1855.2578968412636\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4076630\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1856.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1991      |\n",
      "|    time_elapsed         | 54475     |\n",
      "|    total_timesteps      | 4077568   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0447    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.34e+09  |\n",
      "|    n_updates            | 19900     |\n",
      "|    policy_gradient_loss | -1.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.09e+10  |\n",
      "---------------------------------------\n",
      "Episode 1632  finished with cumulative reward: -4971500.0 and \n",
      "with an average reward of: -1987.8048780487804\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4079131\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1988.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1992      |\n",
      "|    time_elapsed         | 54504     |\n",
      "|    total_timesteps      | 4079616   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0328    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.54e+09  |\n",
      "|    n_updates            | 19910     |\n",
      "|    policy_gradient_loss | -1.43e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.17e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1633  finished with cumulative reward: -2804000.0 and \n",
      "with an average reward of: -1121.1515393842462\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4081632\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1121.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1993      |\n",
      "|    time_elapsed         | 54532     |\n",
      "|    total_timesteps      | 4081664   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0353    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.54e+09  |\n",
      "|    n_updates            | 19920     |\n",
      "|    policy_gradient_loss | -2.12e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.05e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1994      |\n",
      "|    time_elapsed         | 54552     |\n",
      "|    total_timesteps      | 4083712   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0469    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.74e+09  |\n",
      "|    n_updates            | 19930     |\n",
      "|    policy_gradient_loss | -2.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.16e+09  |\n",
      "---------------------------------------\n",
      "Episode 1634  finished with cumulative reward: -3594500.0 and \n",
      "with an average reward of: -1437.2251099560176\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4084133\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1437.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1995      |\n",
      "|    time_elapsed         | 54580     |\n",
      "|    total_timesteps      | 4085760   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0374    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.53e+09  |\n",
      "|    n_updates            | 19940     |\n",
      "|    policy_gradient_loss | -1.29e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 9.41e+09  |\n",
      "---------------------------------------\n",
      "Episode 1635  finished with cumulative reward: -1860500.0 and \n",
      "with an average reward of: -743.9024390243902\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4086634\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -744.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 2.5e+03     |\n",
      "|    ep_rew_mean          | -6e+06      |\n",
      "| time/                   |             |\n",
      "|    fps                  | 74          |\n",
      "|    iterations           | 1996        |\n",
      "|    time_elapsed         | 54609       |\n",
      "|    total_timesteps      | 4087808     |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 1.51922e-08 |\n",
      "|    clip_fraction        | 0           |\n",
      "|    clip_range           | 0.2         |\n",
      "|    entropy_loss         | -17         |\n",
      "|    explained_variance   | 0.0434      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 1.88e+09    |\n",
      "|    n_updates            | 19950       |\n",
      "|    policy_gradient_loss | -1.31e-05   |\n",
      "|    std                  | 1           |\n",
      "|    value_loss           | 3.82e+09    |\n",
      "-----------------------------------------\n",
      "Episode 1636  finished with cumulative reward: -3543500.0 and \n",
      "with an average reward of: -1416.8332666933227\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4089135\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1417.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1997      |\n",
      "|    time_elapsed         | 54637     |\n",
      "|    total_timesteps      | 4089856   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0398    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.91e+09  |\n",
      "|    n_updates            | 19960     |\n",
      "|    policy_gradient_loss | -2.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.84e+09  |\n",
      "---------------------------------------\n",
      "Episode 1637  finished with cumulative reward: -6833000.0 and \n",
      "with an average reward of: -2732.1071571371454\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4091636\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2733.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1998      |\n",
      "|    time_elapsed         | 54664     |\n",
      "|    total_timesteps      | 4091904   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.041     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.3e+09   |\n",
      "|    n_updates            | 19970     |\n",
      "|    policy_gradient_loss | -1.92e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.24e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 1999      |\n",
      "|    time_elapsed         | 54685     |\n",
      "|    total_timesteps      | 4093952   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0349    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.66e+09  |\n",
      "|    n_updates            | 19980     |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.3e+10   |\n",
      "---------------------------------------\n",
      "Episode 1638  finished with cumulative reward: -1376000.0 and \n",
      "with an average reward of: -550.1799280287885\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4094137\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -550.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.89e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2000      |\n",
      "|    time_elapsed         | 54713     |\n",
      "|    total_timesteps      | 4096000   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.049     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.34e+09  |\n",
      "|    n_updates            | 19990     |\n",
      "|    policy_gradient_loss | -3.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.54e+09  |\n",
      "---------------------------------------\n",
      "Episode 1639  finished with cumulative reward: -3365000.0 and \n",
      "with an average reward of: -1345.4618152738904\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4096638\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1346.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -5.89e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 2001          |\n",
      "|    time_elapsed         | 54743         |\n",
      "|    total_timesteps      | 4098048       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 2.0372681e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0384        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 4.07e+09      |\n",
      "|    n_updates            | 20000         |\n",
      "|    policy_gradient_loss | -7.6e-06      |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 7.51e+09      |\n",
      "-------------------------------------------\n",
      "Episode 1640  finished with cumulative reward: -6093500.0 and \n",
      "with an average reward of: -2436.425429828069\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4099139\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2437.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2002      |\n",
      "|    time_elapsed         | 54771     |\n",
      "|    total_timesteps      | 4100096   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0367    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.46e+09  |\n",
      "|    n_updates            | 20010     |\n",
      "|    policy_gradient_loss | -8.43e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 1641  finished with cumulative reward: -5558000.0 and \n",
      "with an average reward of: -2222.311075569772\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4101640\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2223.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.93e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2003      |\n",
      "|    time_elapsed         | 54799     |\n",
      "|    total_timesteps      | 4102144   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0366    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.76e+09  |\n",
      "|    n_updates            | 20020     |\n",
      "|    policy_gradient_loss | -9.13e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.05e+10  |\n",
      "---------------------------------------\n",
      "Episode 1642  finished with cumulative reward: -5889500.0 and \n",
      "with an average reward of: -2354.858056777289\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4104141\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2355.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.9e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 2004         |\n",
      "|    time_elapsed         | 54827        |\n",
      "|    total_timesteps      | 4104192      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0492       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 1.77e+09     |\n",
      "|    n_updates            | 20030        |\n",
      "|    policy_gradient_loss | -4.75e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.5e+09      |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.9e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2005      |\n",
      "|    time_elapsed         | 54848     |\n",
      "|    total_timesteps      | 4106240   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0343    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.51e+09  |\n",
      "|    n_updates            | 20040     |\n",
      "|    policy_gradient_loss | -4.72e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 1643  finished with cumulative reward: -1707500.0 and \n",
      "with an average reward of: -682.7269092363055\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4106642\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -683.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -5.88e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 2006         |\n",
      "|    time_elapsed         | 54876        |\n",
      "|    total_timesteps      | 4108288      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0491       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.5e+08      |\n",
      "|    n_updates            | 20050        |\n",
      "|    policy_gradient_loss | -3.26e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 2.2e+09      |\n",
      "------------------------------------------\n",
      "Episode 1644  finished with cumulative reward: -8745500.0 and \n",
      "with an average reward of: -3496.8012794882047\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4109143\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3498.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.94e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2007      |\n",
      "|    time_elapsed         | 54904     |\n",
      "|    total_timesteps      | 4110336   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0326    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.68e+09  |\n",
      "|    n_updates            | 20060     |\n",
      "|    policy_gradient_loss | -2.72e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.28e+10  |\n",
      "---------------------------------------\n",
      "Episode 1645  finished with cumulative reward: -6399500.0 and \n",
      "with an average reward of: -2558.7764894042384\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4111644\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2559.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.96e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2008      |\n",
      "|    time_elapsed         | 54931     |\n",
      "|    total_timesteps      | 4112384   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0373    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.29e+10  |\n",
      "|    n_updates            | 20070     |\n",
      "|    policy_gradient_loss | -2.91e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.06e+10  |\n",
      "---------------------------------------\n",
      "Episode 1646  finished with cumulative reward: -6756500.0 and \n",
      "with an average reward of: -2701.5193922431026\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4114145\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2702.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.01e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2009      |\n",
      "|    time_elapsed         | 54959     |\n",
      "|    total_timesteps      | 4114432   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0338    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.69e+09  |\n",
      "|    n_updates            | 20080     |\n",
      "|    policy_gradient_loss | -3.3e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 3.31e+09  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.01e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 2010         |\n",
      "|    time_elapsed         | 54980        |\n",
      "|    total_timesteps      | 4116480      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0349       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.8e+09      |\n",
      "|    n_updates            | 20090        |\n",
      "|    policy_gradient_loss | -3.32e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.61e+10     |\n",
      "------------------------------------------\n",
      "Episode 1647  finished with cumulative reward: -8949500.0 and \n",
      "with an average reward of: -3578.3686525389844\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4116646\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3579.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2011      |\n",
      "|    time_elapsed         | 55009     |\n",
      "|    total_timesteps      | 4118528   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0327    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1e+10     |\n",
      "|    n_updates            | 20100     |\n",
      "|    policy_gradient_loss | -1.11e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.22e+10  |\n",
      "---------------------------------------\n",
      "Episode 1648  finished with cumulative reward: -3084500.0 and \n",
      "with an average reward of: -1233.3066773290684\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4119147\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1233.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.04e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2012      |\n",
      "|    time_elapsed         | 55037     |\n",
      "|    total_timesteps      | 4120576   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0507    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.93e+09  |\n",
      "|    n_updates            | 20110     |\n",
      "|    policy_gradient_loss | -1.84e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.69e+09  |\n",
      "---------------------------------------\n",
      "Episode 1649  finished with cumulative reward: -11168000.0 and \n",
      "with an average reward of: -4465.413834466213\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4121648\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -4467.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.09e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 2013         |\n",
      "|    time_elapsed         | 55065        |\n",
      "|    total_timesteps      | 4122624      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 5.820766e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0436       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 5.84e+09     |\n",
      "|    n_updates            | 20120        |\n",
      "|    policy_gradient_loss | -2.75e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.21e+10     |\n",
      "------------------------------------------\n",
      "Episode 1650  finished with cumulative reward: -8312000.0 and \n",
      "with an average reward of: -3323.470611755298\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4124149\n",
      "..........................................\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3324.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2014      |\n",
      "|    time_elapsed         | 55093     |\n",
      "|    total_timesteps      | 4124672   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0337    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.1e+10   |\n",
      "|    n_updates            | 20130     |\n",
      "|    policy_gradient_loss | -1.64e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.96e+10  |\n",
      "---------------------------------------\n",
      "Episode 1651  finished with cumulative reward: -3798500.0 and \n",
      "with an average reward of: -1518.7924830067973\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4126650\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1519.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2015      |\n",
      "|    time_elapsed         | 55120     |\n",
      "|    total_timesteps      | 4126720   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0322    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.46e+10  |\n",
      "|    n_updates            | 20140     |\n",
      "|    policy_gradient_loss | -1.26e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.06e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.06e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2016      |\n",
      "|    time_elapsed         | 55141     |\n",
      "|    total_timesteps      | 4128768   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0442    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.26e+09  |\n",
      "|    n_updates            | 20150     |\n",
      "|    policy_gradient_loss | -1.09e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.7e+09   |\n",
      "---------------------------------------\n",
      "Episode 1652  finished with cumulative reward: -7623500.0 and \n",
      "with an average reward of: -3048.1807277089165\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4129151\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3049.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2017      |\n",
      "|    time_elapsed         | 55169     |\n",
      "|    total_timesteps      | 4130816   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0342    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 9.76e+09  |\n",
      "|    n_updates            | 20160     |\n",
      "|    policy_gradient_loss | -2.76e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.25e+10  |\n",
      "---------------------------------------\n",
      "Episode 1653  finished with cumulative reward: -4997000.0 and \n",
      "with an average reward of: -1998.000799680128\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4131652\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1998.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.13e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2018      |\n",
      "|    time_elapsed         | 55198     |\n",
      "|    total_timesteps      | 4132864   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0423    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.58e+09  |\n",
      "|    n_updates            | 20170     |\n",
      "|    policy_gradient_loss | -1.15e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.27e+09  |\n",
      "---------------------------------------\n",
      "Episode 1654  finished with cumulative reward: -2115500.0 and \n",
      "with an average reward of: -845.8616553378648\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4134153\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -846.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.05e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2019      |\n",
      "|    time_elapsed         | 55226     |\n",
      "|    total_timesteps      | 4134912   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0407    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.25e+09  |\n",
      "|    n_updates            | 20180     |\n",
      "|    policy_gradient_loss | -1.42e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.59e+09  |\n",
      "---------------------------------------\n",
      "Episode 1655  finished with cumulative reward: -6476000.0 and \n",
      "with an average reward of: -2589.3642542982807\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4136654\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2590.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.1e+06  |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2020      |\n",
      "|    time_elapsed         | 55254     |\n",
      "|    total_timesteps      | 4136960   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0363    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.09e+09  |\n",
      "|    n_updates            | 20190     |\n",
      "|    policy_gradient_loss | -1.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.25e+10  |\n",
      "---------------------------------------\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.1e+06     |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 2021         |\n",
      "|    time_elapsed         | 55276        |\n",
      "|    total_timesteps      | 4139008      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0437       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 8.62e+09     |\n",
      "|    n_updates            | 20200        |\n",
      "|    policy_gradient_loss | -2.13e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.16e+10     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1656  finished with cumulative reward: -5405000.0 and \n",
      "with an average reward of: -2161.135545781687\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4139155\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2162.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.11e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2022      |\n",
      "|    time_elapsed         | 55304     |\n",
      "|    total_timesteps      | 4141056   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0395    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.3e+09   |\n",
      "|    n_updates            | 20210     |\n",
      "|    policy_gradient_loss | -1.16e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 6.24e+09  |\n",
      "---------------------------------------\n",
      "Episode 1657  finished with cumulative reward: -6246500.0 and \n",
      "with an average reward of: -2497.6009596161534\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4141656\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2498.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.17e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2023      |\n",
      "|    time_elapsed         | 55333     |\n",
      "|    total_timesteps      | 4143104   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0347    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.52e+10  |\n",
      "|    n_updates            | 20220     |\n",
      "|    policy_gradient_loss | -2.6e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.21e+10  |\n",
      "---------------------------------------\n",
      "Episode 1658  finished with cumulative reward: -5379500.0 and \n",
      "with an average reward of: -2150.93962415034\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4144157\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2151.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.14e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2024      |\n",
      "|    time_elapsed         | 55361     |\n",
      "|    total_timesteps      | 4145152   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0472    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.89e+09  |\n",
      "|    n_updates            | 20230     |\n",
      "|    policy_gradient_loss | -1.37e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 5.61e+09  |\n",
      "---------------------------------------\n",
      "Episode 1659  finished with cumulative reward: -5175500.0 and \n",
      "with an average reward of: -2069.3722510995603\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4146658\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2070.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.13e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 2025          |\n",
      "|    time_elapsed         | 55389         |\n",
      "|    total_timesteps      | 4147200       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.1641532e-10 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.039         |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 3.8e+09       |\n",
      "|    n_updates            | 20240         |\n",
      "|    policy_gradient_loss | -3.59e-06     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.19e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1660  finished with cumulative reward: -5762000.0 and \n",
      "with an average reward of: -2303.878448620552\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4149159\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2304.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2026      |\n",
      "|    time_elapsed         | 55417     |\n",
      "|    total_timesteps      | 4149248   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0357    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.09e+10  |\n",
      "|    n_updates            | 20250     |\n",
      "|    policy_gradient_loss | -1.25e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.58e+10  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.08e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2027      |\n",
      "|    time_elapsed         | 55438     |\n",
      "|    total_timesteps      | 4151296   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0307    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 7.35e+09  |\n",
      "|    n_updates            | 20260     |\n",
      "|    policy_gradient_loss | -1.66e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.01e+10  |\n",
      "---------------------------------------\n",
      "Episode 1661  finished with cumulative reward: -2141000.0 and \n",
      "with an average reward of: -856.0575769692123\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4151660\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -856.4 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.07e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 2028         |\n",
      "|    time_elapsed         | 55467        |\n",
      "|    total_timesteps      | 4153344      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 8.731149e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0531       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 3.07e+09     |\n",
      "|    n_updates            | 20270        |\n",
      "|    policy_gradient_loss | -5.36e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 4.49e+09     |\n",
      "------------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1662  finished with cumulative reward: -9179000.0 and \n",
      "with an average reward of: -3670.1319472211117\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4154161\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3671.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.09e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2029      |\n",
      "|    time_elapsed         | 55495     |\n",
      "|    total_timesteps      | 4155392   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0358    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 2.7e+09   |\n",
      "|    n_updates            | 20280     |\n",
      "|    policy_gradient_loss | -1.01e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.02e+10  |\n",
      "---------------------------------------\n",
      "Episode 1663  finished with cumulative reward: -2702000.0 and \n",
      "with an average reward of: -1080.3678528588564\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4156662\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1080.8 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "-------------------------------------------\n",
      "| rollout/                |               |\n",
      "|    ep_len_mean          | 2.5e+03       |\n",
      "|    ep_rew_mean          | -6.06e+06     |\n",
      "| time/                   |               |\n",
      "|    fps                  | 74            |\n",
      "|    iterations           | 2030          |\n",
      "|    time_elapsed         | 55524         |\n",
      "|    total_timesteps      | 4157440       |\n",
      "| train/                  |               |\n",
      "|    approx_kl            | 1.3591489e-08 |\n",
      "|    clip_fraction        | 0             |\n",
      "|    clip_range           | 0.2           |\n",
      "|    entropy_loss         | -17           |\n",
      "|    explained_variance   | 0.0405        |\n",
      "|    learning_rate        | 0.0003        |\n",
      "|    loss                 | 8.64e+09      |\n",
      "|    n_updates            | 20290         |\n",
      "|    policy_gradient_loss | -2.07e-05     |\n",
      "|    std                  | 1             |\n",
      "|    value_loss           | 1.52e+10      |\n",
      "-------------------------------------------\n",
      "Episode 1664  finished with cumulative reward: -9561500.0 and \n",
      "with an average reward of: -3823.0707716913234\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4159163\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3824.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "------------------------------------------\n",
      "| rollout/                |              |\n",
      "|    ep_len_mean          | 2.5e+03      |\n",
      "|    ep_rew_mean          | -6.12e+06    |\n",
      "| time/                   |              |\n",
      "|    fps                  | 74           |\n",
      "|    iterations           | 2031         |\n",
      "|    time_elapsed         | 55551        |\n",
      "|    total_timesteps      | 4159488      |\n",
      "| train/                  |              |\n",
      "|    approx_kl            | 2.910383e-11 |\n",
      "|    clip_fraction        | 0            |\n",
      "|    clip_range           | 0.2          |\n",
      "|    entropy_loss         | -17          |\n",
      "|    explained_variance   | 0.0426       |\n",
      "|    learning_rate        | 0.0003       |\n",
      "|    loss                 | 6.3e+09      |\n",
      "|    n_updates            | 20300        |\n",
      "|    policy_gradient_loss | -2.28e-06    |\n",
      "|    std                  | 1            |\n",
      "|    value_loss           | 1.23e+10     |\n",
      "------------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.12e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2032      |\n",
      "|    time_elapsed         | 55572     |\n",
      "|    total_timesteps      | 4161536   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0376    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.65e+09  |\n",
      "|    n_updates            | 20310     |\n",
      "|    policy_gradient_loss | -1.05e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.19e+10  |\n",
      "---------------------------------------\n",
      "Episode 1665  finished with cumulative reward: -2294000.0 and \n",
      "with an average reward of: -917.233106757297\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4161664\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -917.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.02e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2033      |\n",
      "|    time_elapsed         | 55600     |\n",
      "|    total_timesteps      | 4163584   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0338    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.18e+09  |\n",
      "|    n_updates            | 20320     |\n",
      "|    policy_gradient_loss | -1.1e-06  |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.88e+09  |\n",
      "---------------------------------------\n",
      "Episode 1666  finished with cumulative reward: -5660000.0 and \n",
      "with an average reward of: -2263.094762095162\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4164165\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2264.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6e+06    |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2034      |\n",
      "|    time_elapsed         | 55627     |\n",
      "|    total_timesteps      | 4165632   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0344    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 3.29e+09  |\n",
      "|    n_updates            | 20330     |\n",
      "|    policy_gradient_loss | -9.88e-07 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.2e+10   |\n",
      "---------------------------------------\n",
      "Episode 1667  finished with cumulative reward: -7088000.0 and \n",
      "with an average reward of: -2834.0663734506197\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4166666\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2835.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -6.03e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2035      |\n",
      "|    time_elapsed         | 55655     |\n",
      "|    total_timesteps      | 4167680   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0346    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 5.94e+09  |\n",
      "|    n_updates            | 20340     |\n",
      "|    policy_gradient_loss | -1.78e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 1.52e+10  |\n",
      "---------------------------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 1668  finished with cumulative reward: -2600000.0 and \n",
      "with an average reward of: -1039.5841663334666\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4169167\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -1040.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.97e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2036      |\n",
      "|    time_elapsed         | 55684     |\n",
      "|    total_timesteps      | 4169728   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.049     |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 6.61e+09  |\n",
      "|    n_updates            | 20350     |\n",
      "|    policy_gradient_loss | -3.06e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 8.43e+09  |\n",
      "---------------------------------------\n",
      "Episode 1669  finished with cumulative reward: -2472500.0 and \n",
      "with an average reward of: -988.6045581767293\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4171668\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -989.0 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2037      |\n",
      "|    time_elapsed         | 55713     |\n",
      "|    total_timesteps      | 4171776   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0417    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.89e+09  |\n",
      "|    n_updates            | 20360     |\n",
      "|    policy_gradient_loss | -1.85e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 4.95e+09  |\n",
      "---------------------------------------\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.88e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2038      |\n",
      "|    time_elapsed         | 55734     |\n",
      "|    total_timesteps      | 4173824   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0371    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 4.86e+09  |\n",
      "|    n_updates            | 20370     |\n",
      "|    policy_gradient_loss | -2.74e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 7.97e+09  |\n",
      "---------------------------------------\n",
      "Episode 1670  finished with cumulative reward: -7521500.0 and \n",
      "with an average reward of: -3007.3970411835267\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4174169\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -3008.6 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n",
      "---------------------------------------\n",
      "| rollout/                |           |\n",
      "|    ep_len_mean          | 2.5e+03   |\n",
      "|    ep_rew_mean          | -5.91e+06 |\n",
      "| time/                   |           |\n",
      "|    fps                  | 74        |\n",
      "|    iterations           | 2039      |\n",
      "|    time_elapsed         | 55761     |\n",
      "|    total_timesteps      | 4175872   |\n",
      "| train/                  |           |\n",
      "|    approx_kl            | 0.0       |\n",
      "|    clip_fraction        | 0         |\n",
      "|    clip_range           | 0.2       |\n",
      "|    entropy_loss         | -17       |\n",
      "|    explained_variance   | 0.0278    |\n",
      "|    learning_rate        | 0.0003    |\n",
      "|    loss                 | 1.07e+10  |\n",
      "|    n_updates            | 20380     |\n",
      "|    policy_gradient_loss | -1.04e-06 |\n",
      "|    std                  | 1         |\n",
      "|    value_loss           | 2.03e+10  |\n",
      "---------------------------------------\n",
      "Episode 1671  finished with cumulative reward: -6195500.0 and \n",
      "with an average reward of: -2477.2091163534587\n",
      "number of steps in this episode: 2501\n",
      "total steps till now: 4176670\n",
      "..........................................\n",
      "forward : 0.0 \n",
      " velocity penalty:0.0 \n",
      " stability : 0.0\n",
      "energy : 0.0 \n",
      "  fall: -2478.2 \n",
      " smooth: 0.0 \n",
      " symmetry: 0.0\n"
     ]
    },
    {
     "ename": "error",
     "evalue": "Not connected to physics server.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31merror\u001b[0m                                     Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Train the agent\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m5000000\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\ppo\\ppo.py:315\u001b[0m, in \u001b[0;36mPPO.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlearn\u001b[39m(\n\u001b[0;32m    307\u001b[0m     \u001b[38;5;28mself\u001b[39m: SelfPPO,\n\u001b[0;32m    308\u001b[0m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    313\u001b[0m     progress_bar: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m    314\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m SelfPPO:\n\u001b[1;32m--> 315\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    316\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    317\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    318\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    319\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    320\u001b[0m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    321\u001b[0m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    322\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:300\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.learn\u001b[1;34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[0m\n\u001b[0;32m    297\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39menv \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    299\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m<\u001b[39m total_timesteps:\n\u001b[1;32m--> 300\u001b[0m     continue_training \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollect_rollouts\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrollout_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_rollout_steps\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_steps\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    302\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m continue_training:\n\u001b[0;32m    303\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:195\u001b[0m, in \u001b[0;36mOnPolicyAlgorithm.collect_rollouts\u001b[1;34m(self, env, callback, rollout_buffer, n_rollout_steps)\u001b[0m\n\u001b[0;32m    190\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    191\u001b[0m         \u001b[38;5;66;03m# Otherwise, clip the actions to avoid out of bound error\u001b[39;00m\n\u001b[0;32m    192\u001b[0m         \u001b[38;5;66;03m# as we are sampling from an unbounded Gaussian distribution\u001b[39;00m\n\u001b[0;32m    193\u001b[0m         clipped_actions \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mclip(actions, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mlow, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maction_space\u001b[38;5;241m.\u001b[39mhigh)\n\u001b[1;32m--> 195\u001b[0m new_obs, rewards, dones, infos \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclipped_actions\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_timesteps \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mnum_envs\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# Give access to local variables\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[1;34m(self, actions)\u001b[0m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[0;32m    201\u001b[0m \n\u001b[0;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[0;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[0;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[1;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\vec_env\\dummy_vec_env.py:58\u001b[0m, in \u001b[0;36mDummyVecEnv.step_wait\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mstep_wait\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m VecEnvStepReturn:\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;66;03m# Avoid circular imports\u001b[39;00m\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m env_idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_envs):\n\u001b[1;32m---> 58\u001b[0m         obs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_rews[env_idx], terminated, truncated, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_infos[env_idx] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menvs\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m     59\u001b[0m \u001b[43m            \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mactions\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv_idx\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m         \u001b[38;5;66;03m# convert to SB3 VecEnv api\u001b[39;00m\n\u001b[0;32m     62\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbuf_dones[env_idx] \u001b[38;5;241m=\u001b[39m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\stable_baselines3\\common\\monitor.py:94\u001b[0m, in \u001b[0;36mMonitor.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mneeds_reset:\n\u001b[0;32m     93\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTried to step environment that needs reset\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 94\u001b[0m observation, reward, terminated, truncated, info \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     95\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrewards\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mfloat\u001b[39m(reward))\n\u001b[0;32m     96\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m terminated \u001b[38;5;129;01mor\u001b[39;00m truncated:\n",
      "Cell \u001b[1;32mIn[3], line 180\u001b[0m, in \u001b[0;36mRoboticDogEnv.step\u001b[1;34m(self, action)\u001b[0m\n\u001b[0;32m    177\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(\u001b[38;5;241m1.\u001b[39m \u001b[38;5;241m/\u001b[39m \u001b[38;5;241m240.\u001b[39m)  \u001b[38;5;66;03m# Adjust sleep time to control simulation speed\u001b[39;00m\n\u001b[0;32m    179\u001b[0m \u001b[38;5;66;03m# Get the next state, reward, and check if done\u001b[39;00m\n\u001b[1;32m--> 180\u001b[0m state \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_state\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    181\u001b[0m reward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcompute_reward()\n\u001b[0;32m    182\u001b[0m terminated \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_done(state)\n",
      "Cell \u001b[1;32mIn[3], line 192\u001b[0m, in \u001b[0;36mRoboticDogEnv.get_state\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_state\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    189\u001b[0m     \u001b[38;5;66;03m# Debugging: Check joint indices\u001b[39;00m\n\u001b[0;32m    190\u001b[0m    \u001b[38;5;66;03m# print(\"Joint indices:\", self.joint_indices)\u001b[39;00m\n\u001b[1;32m--> 192\u001b[0m     joint_states \u001b[38;5;241m=\u001b[39m \u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetJointStates\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrobot_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoint_indices\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    193\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m joint_states \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(joint_states) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    194\u001b[0m        \u001b[38;5;66;03m# print(\"Error: p.getJointStates returned None or an empty list\")  # Debugging statement\u001b[39;00m\n\u001b[0;32m    195\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31merror\u001b[0m: Not connected to physics server."
     ]
    }
   ],
   "source": [
    "# Train the agent\n",
    "model.learn(total_timesteps=5000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134fae88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b5ca4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201dfa58",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5965470e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ffaf7f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with a specific name\n",
    "model.save(\"ppo_spot_trial_basic2\")\n",
    "\n",
    "print(\"Model saved successfully as trial_one.zip.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e4bdf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
