{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03ec437f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.callbacks import BaseCallback\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import pybullet as p\n",
    "import pybullet_data\n",
    "import numpy as np\n",
    "import time\n",
    "from stable_baselines3 import PPO, SAC\n",
    "from stable_baselines3.common.env_checker import check_env\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db02de8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class RoboticDogEnv(gym.Env):\n",
    "    def __init__(self):\n",
    "        super(RoboticDogEnv, self).__init__()\n",
    "        \n",
    "        self.current_target_positions = np.zeros(12)  # Action dimensions\n",
    "        self.physicsClient = p.connect(p.GUI)  # Connect to PyBullet\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())  # Set the simulation environment\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")  # Load the plane and URDF\n",
    "        self.urdf_path = \"C:/Users/youss/OneDrive - aucegypt.edu/Desktop/bolt/spot.urdf\"\n",
    "        self.robot_id = p.loadURDF(self.urdf_path, [0, 0, 0.15], useFixedBase=False)\n",
    "        self.num_joints = p.getNumJoints(self.robot_id)\n",
    "        \n",
    "        # Ensure the robot is correctly initialized\n",
    "        if self.robot_id < 0:\n",
    "            raise ValueError(\"Failed to initialize the robot.\")\n",
    "        if self.num_joints == 0:\n",
    "            raise ValueError(\"No joints found in the robot.\")\n",
    "        \n",
    "        print(f\"Robot ID: {self.robot_id}\")\n",
    "        \n",
    "        # Set gravity\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "        \n",
    "        # Get joint limits and ensure joints are valid\n",
    "        self.joint_indices = []\n",
    "        self.joint_limits = []\n",
    "        \n",
    "        for joint_index in range(self.num_joints):\n",
    "            joint_info = p.getJointInfo(self.robot_id, joint_index)\n",
    "            joint_name = joint_info[1].decode('utf-8')\n",
    "            \n",
    "            # Set the appropriate limits based on joint type\n",
    "            if \"coxa\" in joint_name:\n",
    "                joint_lower_limit, joint_upper_limit = -0.25, 0.25\n",
    "            elif \"femur\" in joint_name:\n",
    "                joint_lower_limit, joint_upper_limit = -0.5, 0.5\n",
    "            elif \"tibia\" in joint_name:\n",
    "                joint_lower_limit, joint_upper_limit = -0.5, 0.5\n",
    "            else:\n",
    "                continue\n",
    "            \n",
    "            self.joint_limits.append((joint_lower_limit, joint_upper_limit))\n",
    "            self.joint_indices.append(joint_index)\n",
    "        \n",
    "        # Define the action and observation space\n",
    "        self.action_space = spaces.Box(\n",
    "            low=np.array([limit[0] for limit in self.joint_limits]),\n",
    "            high=np.array([limit[1] for limit in self.joint_limits]),\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf,\n",
    "            high=np.inf,\n",
    "            shape=(len(self.joint_indices) * 3 + 14,),  # Observation space\n",
    "            dtype=np.float32\n",
    "        )\n",
    "        \n",
    "        # Data for plots\n",
    "        self.avg_rewards_per_episode = []\n",
    "        self.min_rewards_per_episode = []\n",
    "        self.max_rewards_per_episode = []\n",
    "        \n",
    "        self.std_rewards_per_episode = []\n",
    "        self.rewards_per_step = []\n",
    "        self.actions_per_episode = []\n",
    "        \n",
    "        # Reward components\n",
    "        self.forward_rewards = []   # Energy rewards tracking\n",
    "        self.stability_rewards = []# Stability rewards tracking\n",
    "        self.energy_rewards=[]\n",
    "        \n",
    "        self.fall_start_time = None  # Initialize the fall start time\n",
    "        self.leg_indices = self.joint_indices\n",
    "        self.noise_counter = 0\n",
    "        \n",
    "        # Rewards\n",
    "        self.forward = 0\n",
    "        self.stable = 0\n",
    "        self.energy = 0\n",
    "        self.velocity_pen = 0\n",
    "        self.fall = 0\n",
    "        self.smooth = 0\n",
    "        \n",
    "        self.previous_joint_velocities = None\n",
    "        self.sym = 0\n",
    "        \n",
    "        self.total_reward = 0\n",
    "        self.counter = 0\n",
    "        self.steps = 1\n",
    "        self.total_steps = 0\n",
    "        \n",
    "        # Store the link indices for the feet\n",
    "        self.front_left_foot = self.get_link_index('foot_FL')\n",
    "        self.front_right_foot = self.get_link_index('foot_FR')\n",
    "        self.back_left_foot = self.get_link_index('foot_BL')\n",
    "        self.back_right_foot = self.get_link_index('foot_BR')\n",
    "        \n",
    "    \n",
    "        \n",
    "        \n",
    "    def get_link_index(self, link_name):\n",
    "        for joint_index in range(self.num_joints):\n",
    "            joint_info = p.getJointInfo(self.robot_id, joint_index)\n",
    "            joint_name = joint_info[12].decode('utf-8')\n",
    "            if link_name in joint_name:\n",
    "                return joint_info[0]\n",
    "        raise ValueError(f\"Link {link_name} not found\")\n",
    "\n",
    "        \n",
    "        \n",
    "    \n",
    "    def reset(self, **kwargs):\n",
    "        # Increment episode counter and total steps\n",
    "        self.counter += 1\n",
    "        self.total_steps += self.steps\n",
    "\n",
    "        # Print episode statistics if applicable\n",
    "        if self.steps != 0:\n",
    "            episode_rewards = self.rewards_per_step\n",
    "            if len(episode_rewards) > 0:\n",
    "                average = np.mean(episode_rewards)\n",
    "                self.avg_rewards_per_episode.append(average)\n",
    "                self.min_rewards_per_episode.append(np.min(episode_rewards))\n",
    "                self.max_rewards_per_episode.append(np.max(episode_rewards))\n",
    "                self.std_rewards_per_episode.append(np.std(episode_rewards))\n",
    "\n",
    "        if self.total_reward != 0:\n",
    "            print(f\"Episode {self.counter} finished with cumulative reward: {self.total_reward}\")\n",
    "            print(f\"Average reward of: {average}\")\n",
    "            print(f\"Number of steps in this episode: {self.steps}\")\n",
    "            print(f\"Total steps till now: {self.total_steps}\")\n",
    "            print(f\"..........................................\")\n",
    "\n",
    "        # Reset the simulation environment\n",
    "        p.resetSimulation()\n",
    "        p.setAdditionalSearchPath(pybullet_data.getDataPath())\n",
    "        self.plane_id = p.loadURDF(\"plane.urdf\")\n",
    "\n",
    "       \n",
    "        upside_down_orientation = [0, 0, 0, 1]  # Default upright orientation\n",
    "\n",
    "        self.robot_id = p.loadURDF(self.urdf_path, [0, 0, 0.15], upside_down_orientation, useFixedBase=False)\n",
    "\n",
    "        # Set gravity\n",
    "        p.setGravity(0, 0, -9.8)\n",
    "\n",
    "        # Wait for the simulation to stabilize\n",
    "        time.sleep(1)\n",
    "\n",
    "        # Ensure the robot is correctly initialized\n",
    "        if self.robot_id < 0:\n",
    "            print(\"Error: Robot failed to load.\")\n",
    "            return None, {}\n",
    "\n",
    "        if p.getNumJoints(self.robot_id) == 0:\n",
    "            print(\"Error: No joints found in the robot.\")\n",
    "            return None, {}\n",
    "\n",
    "        # Reset fall start time\n",
    "        self.fall_start_time = None\n",
    "\n",
    "        # Print reward statistics\n",
    "        print(f\"Forward: {self.forward / self.steps}\")\n",
    "        print(f\"Velocity penalty: {self.velocity_pen / self.steps}\")\n",
    "        print(f\"Stability: {self.stable / self.steps}\")\n",
    "        \n",
    "        self.stability_rewards.append(self.stable / self.steps)\n",
    "        print(f\"Energy: {self.energy / self.steps}\")\n",
    "        self.forward_rewards.append(self.forward / self.steps)\n",
    "        self.energy_rewards.append(self.energy / self.steps)\n",
    "        print(f\"Fall: {self.fall / self.steps}\")\n",
    "        print(f\"Smooth: {self.smooth / self.steps}\")\n",
    "        print(f\"Symmetry: {self.sym / self.steps}\")\n",
    "\n",
    "        # Plot metrics if available\n",
    "        if len(self.avg_rewards_per_episode) > 1:\n",
    "            self.plot_average_rewards()\n",
    "            self.plot_std_rewards()\n",
    "            self.plot_action_distribution()\n",
    "            self.plot_stability_rewards()  # Plot energy rewards\n",
    "            self.plot_forward_rewards()  # Plot stability rewards\n",
    "            self.plot_energy_rewards()\n",
    "        # Reset statistics and reward tracking\n",
    "        self.actions_per_episode = []\n",
    "        self.forward = 0\n",
    "        self.stable = 0\n",
    "        self.energy = 0\n",
    "        self.velocity_pen = 0\n",
    "        self.fall = 0\n",
    "        self.smooth = 0\n",
    "        self.sym = 0\n",
    "        self.rewards_per_step = []\n",
    "       \n",
    "\n",
    "        self.total_reward = 0\n",
    "        self.steps = 1\n",
    "\n",
    "        # Return the initial state\n",
    "        initial_state = self.get_state()\n",
    "        return initial_state, {}\n",
    "    \n",
    "    \n",
    "    def step(self, action):\n",
    "        # Update the target positions based on the action\n",
    "        self.current_target_positions = action\n",
    "        self.actions_per_episode.append(action)\n",
    "\n",
    "        # Apply the action to the robot's joints\n",
    "        for i, joint_index in enumerate(self.joint_indices):\n",
    "            p.setJointMotorControl2(self.robot_id, joint_index, p.POSITION_CONTROL, targetPosition=action[i])\n",
    "\n",
    "        # Step the simulation\n",
    "        p.stepSimulation()\n",
    "        time.sleep(1. / 240)  # Ensure the simulation steps are processed\n",
    "\n",
    "        # Verify that joint positions have updated correctly\n",
    "        joint_positions = [p.getJointState(self.robot_id, joint_index)[0] for joint_index in self.joint_indices]\n",
    "\n",
    "        # Get the next state, reward, and check if done\n",
    "        state = self.get_state()\n",
    "        reward = self.compute_reward()\n",
    "        self.rewards_per_step.append(reward)\n",
    "        terminated = self.is_done(state)\n",
    "        truncated = False  # This can be set to True based on some logic, e.g., max steps reached\n",
    "\n",
    "        # Update total reward and step count\n",
    "        self.total_reward += reward\n",
    "        self.steps += 1\n",
    "       # time.sleep(1 / 20)  # Slow down the simulation\n",
    "\n",
    "\n",
    "        return state, reward, terminated, truncated, {}\n",
    "    \n",
    "    def get_state(self):\n",
    "        # Get joint states: positions and velocities\n",
    "        joint_states = p.getJointStates(self.robot_id, self.joint_indices)\n",
    "        joint_positions = [state[0] for state in joint_states]\n",
    "        joint_velocities = [state[1] for state in joint_states]\n",
    "\n",
    "        # Get base position, orientation, velocity, and angular velocity\n",
    "        base_position, base_orientation = p.getBasePositionAndOrientation(self.robot_id)\n",
    "        base_velocity, base_angular_velocity = p.getBaseVelocity(self.robot_id)\n",
    "\n",
    "        # Convert orientation from quaternion to Euler angles\n",
    "        base_orientation_euler = p.getEulerFromQuaternion(base_orientation)\n",
    "\n",
    "        # Contact information for the robot's feet\n",
    "        front_left_contact = p.getContactPoints(bodyA=self.robot_id, linkIndexA=self.front_left_foot, bodyB=self.plane_id)\n",
    "        front_right_contact = p.getContactPoints(bodyA=self.robot_id, linkIndexA=self.front_right_foot, bodyB=self.plane_id)\n",
    "        back_left_contact = p.getContactPoints(bodyA=self.robot_id, linkIndexA=self.back_left_foot, bodyB=self.plane_id)\n",
    "        back_right_contact = p.getContactPoints(bodyA=self.robot_id, linkIndexA=self.back_right_foot, bodyB=self.plane_id)\n",
    "\n",
    "        # Convert contact points to a binary indicator\n",
    "        foot_contacts = [\n",
    "            1.0 if len(front_left_contact) > 0 else 0.0,\n",
    "            1.0 if len(front_right_contact) > 0 else 0.0,\n",
    "            1.0 if len(back_left_contact) > 0 else 0.0,\n",
    "            1.0 if len(back_right_contact) > 0 else 0.0\n",
    "        ]\n",
    "\n",
    "        # Get the height of the base from its position\n",
    "        base_height = np.array([base_position[2]])\n",
    "\n",
    "        # Concatenate all the state information into a single array\n",
    "        state = np.concatenate([\n",
    "            joint_positions,\n",
    "            joint_velocities,\n",
    "            base_orientation_euler,  # Using Euler angles instead of Quaternion\n",
    "            base_angular_velocity,\n",
    "            base_velocity,\n",
    "            base_height,\n",
    "            foot_contacts,\n",
    "            self.current_target_positions  # Include current target positions\n",
    "        ])\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def compute_reward(self):\n",
    "        # Get base position, orientation, velocity, and angular velocity\n",
    "        base_position, base_orientation = p.getBasePositionAndOrientation(self.robot_id)\n",
    "        base_velocity, base_angular_velocity = p.getBaseVelocity(self.robot_id)\n",
    "\n",
    "        \n",
    "        # Calculate the forward vector from the orientation\n",
    "        forward_vector = p.getMatrixFromQuaternion(base_orientation)[0:3:2]\n",
    "        forward_vector = np.array(forward_vector)  # Forward direction in the world frame\n",
    "        forward_velocity = np.dot(forward_vector, base_velocity[:2])\n",
    "        forward_reward = forward_velocity\n",
    "\n",
    "            \n",
    "            \n",
    "        # Get base orientation in Euler angles\n",
    "        base_orientation_euler = p.getEulerFromQuaternion(base_orientation)\n",
    "        pitch, roll = base_orientation_euler[1], base_orientation_euler[0]\n",
    "        # Set acceptable thresholds for instability\n",
    "        acceptable_pitch_threshold = 0.03  # 0.03 and 0.04 worked good \n",
    "        acceptable_roll_threshold = 0.03 # 0.03 and 0.04 worked good \n",
    "        stability_penalty = 0.0\n",
    "        \n",
    "        if abs(pitch) < acceptable_pitch_threshold: \n",
    "            stability_penalty += -0.05\n",
    "        else: stability_penalty= (abs(pitch)-acceptable_pitch_threshold)**2\n",
    "        \n",
    "        if abs(roll) < acceptable_roll_threshold: \n",
    "            stability_penalty += -0.05\n",
    "        else: stability_penalty= (abs(roll)-acceptable_roll_threshold)**2\n",
    "           \n",
    "    \n",
    "        \n",
    "        \n",
    "        # Penalize high velocities\n",
    "        max_velocity_threshold = 5.0  # Set the maximum desired velocity\n",
    "        if forward_velocity > max_velocity_threshold:\n",
    "            velocity_penalty = (forward_velocity - max_velocity_threshold) ** 2\n",
    "        else:\n",
    "            velocity_penalty = -1\n",
    "            \n",
    "    \n",
    "        # Apply dynamic energy penalty\n",
    "        joint_states = p.getJointStates(self.robot_id, self.joint_indices)\n",
    "        joint_velocities = [state[1] for state in joint_states]\n",
    "        energy_penalty = 0.0\n",
    "        joint_velocity_threshold = 10  # Threshold for penalizing high joint velocities\n",
    "        for velocity in joint_velocities:\n",
    "            if abs(velocity) > joint_velocity_threshold:\n",
    "                energy_penalty += (abs(velocity) - joint_velocity_threshold) ** 2\n",
    "            else:\n",
    "                energy_penalty += -2\n",
    "\n",
    "       \n",
    "                \n",
    "                \n",
    "        # Penalize if the robot has fallen (based on height threshold)\n",
    "        fall_penalty = 0.0\n",
    "        if base_position[2] < 0.12:\n",
    "            fall_penalty = 50.0\n",
    "        else:\n",
    "            fall_penalty = -1\n",
    "\n",
    "        # Calculate smoothness and symmetry penalties\n",
    "        smoothness_penalty = self.compute_smoothness_reward(joint_velocities)\n",
    "        symmetry_penalty = self.compute_symmetry_reward()\n",
    "\n",
    "        # Scaling factors\n",
    "        forward_scale = 10  # Adjust as needed\n",
    "        stability_penalty_scale = -200  # Adjust as needed\n",
    "        energy_penalty_scale = 0\n",
    "        fall_penalty_scale = 0  # Adjust as needed\n",
    "        smoothness_scale = 0  # Adjust as needed\n",
    "        symmetry_penalty_scale = 0  # Adjust as needed\n",
    "        velocity_penalty_scale = 0  # Adjust as needed\n",
    "        \n",
    "        stability= np.clip( stability_penalty * stability_penalty_scale, -50, 50)\n",
    "         \n",
    "        # Compute the total reward\n",
    "        reward = (forward_reward * forward_scale) + \\\n",
    "                 (velocity_penalty * velocity_penalty_scale) + \\\n",
    "                 (energy_penalty * energy_penalty_scale) + \\\n",
    "                 (stability) + \\\n",
    "                 (fall_penalty * fall_penalty_scale) + \\\n",
    "                 (smoothness_penalty * smoothness_scale) + \\\n",
    "                 (symmetry_penalty * symmetry_penalty_scale)\n",
    "\n",
    "        # Update reward tracking\n",
    "        self.forward += (forward_reward * forward_scale)\n",
    "        self.velocity_pen += (velocity_penalty * velocity_penalty_scale)\n",
    "        self.stable += (stability)\n",
    "        self.energy += (energy_penalty * energy_penalty_scale)\n",
    "        self.fall += (fall_penalty * fall_penalty_scale)\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def compute_symmetry_reward(self):\n",
    "             # Get the height of each foot joint\n",
    "        front_left_height = p.getLinkState(self.robot_id, self.front_left_foot)[0][2]\n",
    "        front_right_height = p.getLinkState(self.robot_id, self.front_right_foot)[0][2]\n",
    "        back_left_height = p.getLinkState(self.robot_id, self.back_left_foot)[0][2]\n",
    "        back_right_height = p.getLinkState(self.robot_id, self.back_right_foot)[0][2]\n",
    "\n",
    "        # Reward for diagonal leg coordination\n",
    "        symmetry_reward = 0.0\n",
    "        if abs(front_left_height - back_right_height) < 0.015:  # Adjust threshold as needed\n",
    "            symmetry_reward += 1.0\n",
    "        else: symmetry_reward -= 1.0\n",
    "        if abs(front_right_height - back_left_height) < 0.015:  # Adjust threshold as needed\n",
    "            symmetry_reward += 1.0\n",
    "        else: symmetry_reward -= 1.0\n",
    "        return symmetry_reward\n",
    "\n",
    "    def compute_smoothness_reward(self, joint_velocities):\n",
    "        velocity_change_threshold = 2\n",
    "        smoothness_penalty = 0\n",
    "\n",
    "        if self.previous_joint_velocities is None:\n",
    "            smoothness_reward = 0.0\n",
    "        else:\n",
    "            velocity_differences = np.abs(np.array(joint_velocities) - np.array(self.previous_joint_velocities))\n",
    "            velocity_differences = np.clip(velocity_differences - velocity_change_threshold, 0, None)\n",
    "            smoothness_penalty = np.sum(velocity_differences)\n",
    "\n",
    "        self.previous_joint_velocities = joint_velocities\n",
    "        return smoothness_penalty\n",
    "\n",
    "    def plot_average_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.avg_rewards_per_episode, label='Avg Reward')\n",
    "        plt.fill_between(range(len(self.avg_rewards_per_episode)),\n",
    "                         self.min_rewards_per_episode, self.max_rewards_per_episode, alpha=0.3)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Average Reward')\n",
    "        plt.title('Average Reward with Min/Max')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_forward_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.forward_rewards, label='forward Reward')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('forward Reward')\n",
    "        plt.title('forward Reward per Step')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_stability_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.stability_rewards, label='Stability Reward')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('Stability Reward')\n",
    "        plt.title('Stability Reward per Step')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "    def plot_energy_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.energy_rewards, label='energy Reward')\n",
    "        plt.xlabel('Step')\n",
    "        plt.ylabel('energy Reward')\n",
    "        plt.title('energy Reward per Step')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_std_rewards(self):\n",
    "        plt.figure(figsize=(10, 5))\n",
    "        plt.plot(self.std_rewards_per_episode)\n",
    "        plt.xlabel('Episode')\n",
    "        plt.ylabel('Standard Deviation')\n",
    "        plt.title('Standard Deviation of Total Reward')\n",
    "        plt.show()\n",
    "\n",
    "    def plot_action_distribution(self):\n",
    "        if len(self.actions_per_episode) == 0:\n",
    "            print(\"No actions to plot.\")\n",
    "            return\n",
    "\n",
    "        actions_array = np.array(self.actions_per_episode)\n",
    "\n",
    "        if actions_array.shape[1] != 12:\n",
    "            print(f\"Expected 12 actions per step, but got {actions_array.shape[1]}.\")\n",
    "            return\n",
    "\n",
    "        plt.figure(figsize=(15, 10))\n",
    "        for i in range(12):  # We know there are 12 joints\n",
    "            plt.subplot(3, 4, i + 1)\n",
    "            plt.hist(actions_array[:, i], bins=20, alpha=0.7, label=f'Joint {i}',\n",
    "                     range=self.joint_limits[i])  # Respect the action limits\n",
    "            plt.xlabel('Action Value')\n",
    "            plt.ylabel('Frequency')\n",
    "            plt.title(f'Action Distribution for Joint {i}')\n",
    "            plt.xlim(self.joint_limits[i])  # Set the x-axis limits to respect the joint limits\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "    def is_done(self, state):\n",
    "        base_position, base_orientation = p.getBasePositionAndOrientation(self.robot_id)\n",
    "\n",
    "        # Check if the robot has fallen\n",
    "        if base_position[2] < 0.12:\n",
    "            if self.fall_start_time is None:\n",
    "                self.fall_start_time = time.time()  # Start the timer\n",
    "                # print(\"Robot has started falling\")\n",
    "            elif time.time() - self.fall_start_time > 2:  # Wait for 2 seconds before deciding to reset\n",
    "                # print(\"Robot has fallen for 2 seconds, resetting...\")\n",
    "                return True\n",
    "        else:\n",
    "            self.fall_start_time = None  # Reset the timer if the robot is not fallen\n",
    "\n",
    "        # Check if the maximum number of steps is reached\n",
    "        if self.steps == 2500:\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        pass\n",
    "\n",
    "    def close(self):\n",
    "        p.disconnect()\n",
    "\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b85709f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0eaa0492",
   "metadata": {},
   "outputs": [],
   "source": [
    "gym.envs.registration.register(id='RoboticDog-v0', entry_point='__main__:RoboticDogEnv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13457148",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Robot ID: 1\n"
     ]
    }
   ],
   "source": [
    "env = RoboticDogEnv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8df418d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n"
     ]
    }
   ],
   "source": [
    "# Instantiate the agent\n",
    "# For more exploration, consider decreasing the learning rate to 1e-4 and increasing entropy to 0.1\n",
    "\n",
    "#model = PPO('MlpPolicy', env, verbose=1)\n",
    "# Load the pre-trained model\n",
    "model_path = \"C://Users//youss//OneDrive - aucegypt.edu//Desktop//bolt//bolt_final.zip\"\n",
    "model = PPO.load(model_path, env=env)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0dc93b4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e86fbae1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model.learn(total_timesteps=1000000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24e812db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b49b0b6d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a396ab55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " good boy bolt\n"
     ]
    }
   ],
   "source": [
    "print(\" good boy bolt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b02b3d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab00ef41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model with a specific name\n",
    "model.save(\"ppo_spot_trial_basic2\")\n",
    "\n",
    "print(\"Model saved successfully as trial_one.zip.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
